{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigbird.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNwAB7DnPB/1JWGf0saYges",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb18f0f793cb48ef80244c57aeba516e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c7d198a2b904408fa06e0b9db3d99a6a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9975a618b0ae402d8dc382ed58c341cd",
              "IPY_MODEL_8c43ccff5fdc4af5808310f33f38466e",
              "IPY_MODEL_dbcad235943c41cd924ca702836f9375"
            ]
          }
        },
        "c7d198a2b904408fa06e0b9db3d99a6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9975a618b0ae402d8dc382ed58c341cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_352825bfd87b479dbd38069376f8015f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Dl Completed...: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f36bdb5f47af42579dc3381783462361"
          }
        },
        "8c43ccff5fdc4af5808310f33f38466e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2bccaa8511d64beb8a6e8dcfec96359f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78399e4d35294549a05c2ef5f9386dbb"
          }
        },
        "dbcad235943c41cd924ca702836f9375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e7160e7c6f1b452fa8f722e9298ece50",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:02&lt;00:00,  2.84s/ url]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c767eb18ed36405ba65600843d0a3aef"
          }
        },
        "352825bfd87b479dbd38069376f8015f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f36bdb5f47af42579dc3381783462361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bccaa8511d64beb8a6e8dcfec96359f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78399e4d35294549a05c2ef5f9386dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7160e7c6f1b452fa8f722e9298ece50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c767eb18ed36405ba65600843d0a3aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0126b2e655744f3e896d5f3c74111d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_543121ce3b5144cab95da109c7c4100a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_777ece97dc7c497da6b7acd954862d32",
              "IPY_MODEL_89bf6ea7fd5f4803a52465e72339cd0d",
              "IPY_MODEL_7b8dc3dc9a5f47d8a4686f3d5d307f4f"
            ]
          }
        },
        "543121ce3b5144cab95da109c7c4100a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "777ece97dc7c497da6b7acd954862d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b2c8ffd70794322a1360c22198c1c8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Dl Size...: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93f1d9503031416d85dfd07a223c3d6c"
          }
        },
        "89bf6ea7fd5f4803a52465e72339cd0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0142a3dab2d48db810215dfa7c4aa3a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b1f8019ae2b4f84ad962df7639c6370"
          }
        },
        "7b8dc3dc9a5f47d8a4686f3d5d307f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0f30c06513174cc1884a715cc639b0a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 80/80 [00:02&lt;00:00, 32.26 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de8c603c35e54059b2a775e9a3b8513f"
          }
        },
        "9b2c8ffd70794322a1360c22198c1c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93f1d9503031416d85dfd07a223c3d6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0142a3dab2d48db810215dfa7c4aa3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b1f8019ae2b4f84ad962df7639c6370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f30c06513174cc1884a715cc639b0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de8c603c35e54059b2a775e9a3b8513f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "009eda280c57461684fc3452d17b534b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_280d673e69874a3eb0cbb94127506f04",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cb8dfade1a354771ab29f3b4a8b53741",
              "IPY_MODEL_754dfe171d2e4cb592787d46b585263c",
              "IPY_MODEL_2458e3e25eec464887c3accb846e5535"
            ]
          }
        },
        "280d673e69874a3eb0cbb94127506f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb8dfade1a354771ab29f3b4a8b53741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71a9f4b8aeba4bdbba4a272d8164e214",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Generating splits...: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c89bce775f44f52bfd1cb1db74ffd5a"
          }
        },
        "754dfe171d2e4cb592787d46b585263c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_742284d04bd64e9bba44f3c1532528cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acea966f6ed34aed979aed4fa3dd92d3"
          }
        },
        "2458e3e25eec464887c3accb846e5535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e1eaff7f37fa477c83500d7731b7e299",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [01:02&lt;00:00, 21.70s/ splits]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_32bf82c905db47f398ba7570ce7109c1"
          }
        },
        "71a9f4b8aeba4bdbba4a272d8164e214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c89bce775f44f52bfd1cb1db74ffd5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "742284d04bd64e9bba44f3c1532528cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acea966f6ed34aed979aed4fa3dd92d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1eaff7f37fa477c83500d7731b7e299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "32bf82c905db47f398ba7570ce7109c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad47a3f79dc84691b9b993197dffabfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9d1b66d5821a4e8fac5a4c8447308e33",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f75fcba350c94a5ca049c5e3fd39da6d",
              "IPY_MODEL_f4540de7e32b450080253e0d3646f2fe",
              "IPY_MODEL_41cff3038828493da46f1295d0d56fa3"
            ]
          }
        },
        "9d1b66d5821a4e8fac5a4c8447308e33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f75fcba350c94a5ca049c5e3fd39da6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fca031a120874eca82dc21f650cc2759",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Generating train examples...:  99%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80d61c034c9148d597d3c8b80c6d9a10"
          }
        },
        "f4540de7e32b450080253e0d3646f2fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b330b2a21bf44df5a8f06bb0c8112a88",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 25000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a48b986efe34fedb5734210be3ce239"
          }
        },
        "41cff3038828493da46f1295d0d56fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b9007640337545b38db808f457b5f87b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 24829/25000 [00:13&lt;00:00, 2388.65 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78f0eb31cde1494690644c7013b8bed9"
          }
        },
        "fca031a120874eca82dc21f650cc2759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80d61c034c9148d597d3c8b80c6d9a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b330b2a21bf44df5a8f06bb0c8112a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a48b986efe34fedb5734210be3ce239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9007640337545b38db808f457b5f87b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78f0eb31cde1494690644c7013b8bed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c1dec5f1bb0467fade6e624061d06d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08530c885f474023b458060f4d9460c5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dabc9975560649308e6c65b3df7da70e",
              "IPY_MODEL_9262fb87802b4d5b948047fca1058409",
              "IPY_MODEL_2454e52c15754719a8321149cb7b2479"
            ]
          }
        },
        "08530c885f474023b458060f4d9460c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dabc9975560649308e6c65b3df7da70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c7df5e02458b4618bccf4a84b6f2da67",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Shuffling imdb_reviews-train.tfrecord...:  95%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6447d4b4a5a340d4bbba348d6576f428"
          }
        },
        "9262fb87802b4d5b948047fca1058409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c16e2929caf647838d0cf7f62eaf45f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 25000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab8dadbbc48e40dfb590602996a35ab8"
          }
        },
        "2454e52c15754719a8321149cb7b2479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2faf11bd100f4cf9bb1d32b4172cfe0d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23852/25000 [00:00&lt;00:00, 133803.41 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2450d6706fd64d12a3a7d36f210c11d5"
          }
        },
        "c7df5e02458b4618bccf4a84b6f2da67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6447d4b4a5a340d4bbba348d6576f428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c16e2929caf647838d0cf7f62eaf45f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab8dadbbc48e40dfb590602996a35ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2faf11bd100f4cf9bb1d32b4172cfe0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2450d6706fd64d12a3a7d36f210c11d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6db30e7736342b9b237403f7855aa5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e94a16a3ec3f46b1a7453b9a548f1a92",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fa9abd680f1e4b8595657934fef784f3",
              "IPY_MODEL_7d3f5264b72641568d3f8b7c6f7745b9",
              "IPY_MODEL_ce56210ff95f48cf9a92122232e36d46"
            ]
          }
        },
        "e94a16a3ec3f46b1a7453b9a548f1a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa9abd680f1e4b8595657934fef784f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_510bc1bd58434524b3374c4bb9db1834",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Generating test examples...: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4de3dafab8644180830ea94c83377922"
          }
        },
        "7d3f5264b72641568d3f8b7c6f7745b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d3d77098f5914c60a92fbb0d6c79fa2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 25000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66eb91b37b8f4810b99a2d172069df2a"
          }
        },
        "ce56210ff95f48cf9a92122232e36d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46a9e43884c44dafba7c875a89f4fe7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 24978/25000 [00:10&lt;00:00, 2540.88 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0b52ccfc656431aa90d66108f3be3f2"
          }
        },
        "510bc1bd58434524b3374c4bb9db1834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4de3dafab8644180830ea94c83377922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3d77098f5914c60a92fbb0d6c79fa2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66eb91b37b8f4810b99a2d172069df2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46a9e43884c44dafba7c875a89f4fe7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0b52ccfc656431aa90d66108f3be3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "835e6b4676d14ff08e05ce031dc175e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cba76259ad63468091c71d51c0eebe6a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99e82dc1b64d4a4e9b662cbe8819b08f",
              "IPY_MODEL_0c180b17ead74979aec1b934f2af8050",
              "IPY_MODEL_0cd09db387a34134941fa46fb66a5dcd"
            ]
          }
        },
        "cba76259ad63468091c71d51c0eebe6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99e82dc1b64d4a4e9b662cbe8819b08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b67bbd36ec16403cb969ac7c1eaafd1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Shuffling imdb_reviews-test.tfrecord...:  90%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b720b6aa8c4648c49f05968f8e20f47e"
          }
        },
        "0c180b17ead74979aec1b934f2af8050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_97f7cb20fc4045249a85d6b5c3628b96",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 25000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcfd723789c646b09e8356bd705af0f3"
          }
        },
        "0cd09db387a34134941fa46fb66a5dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66fa5757482246a198428e4302a425b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 22417/25000 [00:00&lt;00:00, 126568.85 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab48a9a6fe854397822f35b059fd9bdc"
          }
        },
        "b67bbd36ec16403cb969ac7c1eaafd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b720b6aa8c4648c49f05968f8e20f47e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97f7cb20fc4045249a85d6b5c3628b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcfd723789c646b09e8356bd705af0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66fa5757482246a198428e4302a425b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab48a9a6fe854397822f35b059fd9bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6e12461ec684d7ab629c4d7e2ebab2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ce9b871289a4c24894a296a4f90fa24",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_75c4bd3a2dc549bfa8e38b6415f070ee",
              "IPY_MODEL_d15c9c76a9124490bd840987c8af9dd9",
              "IPY_MODEL_a8cf41b1e0e9448ea3acbf6b00877e51"
            ]
          }
        },
        "8ce9b871289a4c24894a296a4f90fa24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75c4bd3a2dc549bfa8e38b6415f070ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_facbe82adfb04bf982bed1886d316d43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Generating unsupervised examples...: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b0d1995e4d04dfda8aa9d51dbceb818"
          }
        },
        "d15c9c76a9124490bd840987c8af9dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_103ad4ad39c24ce9ae6d373dbf7baf80",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 50000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a270ec492e014579a7e9ff54cc842a5a"
          }
        },
        "a8cf41b1e0e9448ea3acbf6b00877e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_900796159eae4671948b6879030d2349",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 49880/50000 [00:25&lt;00:00, 2527.17 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5740963bb2cb40b68d32b6af340c9b62"
          }
        },
        "facbe82adfb04bf982bed1886d316d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b0d1995e4d04dfda8aa9d51dbceb818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "103ad4ad39c24ce9ae6d373dbf7baf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a270ec492e014579a7e9ff54cc842a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "900796159eae4671948b6879030d2349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5740963bb2cb40b68d32b6af340c9b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "953aabb575a34891b175c3f25bcb56af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7d4340284b34e90ab546df4310b7527",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_efa89ee311554545b9929b702b35b315",
              "IPY_MODEL_e515681e40a74fae8afd839bb715a2a6",
              "IPY_MODEL_3cad977e94224132afb9b339bb3e7ccd"
            ]
          }
        },
        "a7d4340284b34e90ab546df4310b7527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efa89ee311554545b9929b702b35b315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35a128956519425f98a994be38e72e83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Shuffling imdb_reviews-unsupervised.tfrecord...:  77%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_979719cdd71e4308a5b443afc6f9fa8e"
          }
        },
        "e515681e40a74fae8afd839bb715a2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_37ec060f648a4feab297a48db453e1da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 50000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4963448793f54f00b9ef88f59ada9a8f"
          }
        },
        "3cad977e94224132afb9b339bb3e7ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08ced3c5d11b4fedbc1ce2fe3166cbe4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 38491/50000 [00:00&lt;00:00, 149854.24 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ec1a9d3d694412b8783df39f37b6c67"
          }
        },
        "35a128956519425f98a994be38e72e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "979719cdd71e4308a5b443afc6f9fa8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37ec060f648a4feab297a48db453e1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4963448793f54f00b9ef88f59ada9a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08ced3c5d11b4fedbc1ce2fe3166cbe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ec1a9d3d694412b8783df39f37b6c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/longformer/blob/main/bigbird.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0PWMub73Dv6"
      },
      "source": [
        "# Copyright 2020 The BigBird Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "#The most important directory is core. There are three main files in core.\n",
        "\n",
        "#attention.py: Contains BigBird linear attention mechanism\n",
        "#encoder.py: Contains the main long sequence encoder stack\n",
        "#modeling.py: Contains packaged BERT and seq2seq transformer models with BigBird attention"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsMpn6SZB2X6"
      },
      "source": [
        "# py 파일  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZW6qvFUB8aL"
      },
      "source": [
        "############## attention ################\n",
        "\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"BigBird Attention Layers.\"\"\"\n",
        "\n",
        "from absl import logging\n",
        "from bigbird.core import recompute_grad\n",
        "from bigbird.core import utils\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN = 4096\n",
        "\n",
        "\n",
        "def get_single_block_row_attention(block_id,\n",
        "                                   to_start_block_id,\n",
        "                                   to_end_block_id,\n",
        "                                   num_rand_blocks,\n",
        "                                   window_block_left=1,\n",
        "                                   window_block_right=1,\n",
        "                                   global_block_left=1,\n",
        "                                   global_block_right=1):\n",
        "  \"\"\"For a single row block get random row attention.\n",
        "  Args:\n",
        "    block_id: int. block id of row.\n",
        "    to_start_block_id: int. random attention coloum start id.\n",
        "    to_end_block_id: int. random attention coloum end id.\n",
        "    num_rand_blocks: int. number of random blocks to be selected.\n",
        "    window_block_left: int. number of blocks of window to left of a block.\n",
        "    window_block_right: int. number of blocks of window to right of a block.\n",
        "    global_block_left: int. Number of blocks globally used to the left.\n",
        "    global_block_right: int. Number of blocks globally used to the right.\n",
        "  Returns:\n",
        "    row containing the random attention vector of size num_rand_blocks.\n",
        "  \"\"\"\n",
        "\n",
        "  # list of to_blocks from which to choose random attention\n",
        "  to_block_list = np.arange(to_start_block_id, to_end_block_id,\n",
        "                            dtype=np.int32)\n",
        "  # permute the blocks\n",
        "  perm_block = np.random.permutation(to_block_list)\n",
        "  # print(perm_block)\n",
        "\n",
        "  # illegal blocks for the current block id, using window\n",
        "  illegal_blocks = list(\n",
        "      range(block_id - window_block_left, block_id + window_block_right + 1))\n",
        "\n",
        "  # Add blocks at the start and at the end\n",
        "  illegal_blocks.extend(list(range(global_block_left)))\n",
        "  illegal_blocks.extend(\n",
        "      list(range(to_end_block_id - global_block_right, to_end_block_id)))\n",
        "\n",
        "  # The second from_block cannot choose random attention on second last to_block\n",
        "  if block_id == 1:\n",
        "    illegal_blocks.append(to_end_block_id-2)\n",
        "\n",
        "  # The second last from_block cannot choose random attention on second to_block\n",
        "  if block_id == to_end_block_id - 2:\n",
        "    illegal_blocks.append(1)\n",
        "\n",
        "  selected_random_blokcs = []\n",
        "\n",
        "  for i in range(to_end_block_id - to_start_block_id):\n",
        "    if perm_block[i] not in illegal_blocks:\n",
        "      selected_random_blokcs.append(perm_block[i])\n",
        "    if len(selected_random_blokcs) == num_rand_blocks:\n",
        "      break\n",
        "  return np.array(selected_random_blokcs, dtype=np.int32)\n",
        "\n",
        "\n",
        "def bigbird_block_rand_mask_with_head(seq_length,\n",
        "                                      block_size,\n",
        "                                      num_heads,\n",
        "                                      plan_from_length,\n",
        "                                      plan_num_rand_blocks,\n",
        "                                      window_block_left=1,\n",
        "                                      window_block_right=1,\n",
        "                                      global_block_top=1,\n",
        "                                      global_block_bottom=1,\n",
        "                                      global_block_left=1,\n",
        "                                      global_block_right=1):\n",
        "  \"\"\"Create adjacency list of random attention.\n",
        "  Args:\n",
        "    seq_length: int. length of sequence.\n",
        "    block_size: int. size of block in sequence.\n",
        "    num_heads: int. total number of heads.\n",
        "    plan_from_length: list. plan from lenght where num_rand are choosen from.\n",
        "    plan_num_rand_blocks: list. number of rand blocks within the plan.\n",
        "    window_block_left: int. number of blocks of window to left of a block.\n",
        "    window_block_right: int. number of blocks of window to right of a block.\n",
        "    global_block_top: int. number of blocks at the top.\n",
        "    global_block_bottom: int. number of blocks at the bottom.\n",
        "    global_block_left: int. Number of blocks globally used to the left.\n",
        "    global_block_right: int. Number of blocks globally used to the right.\n",
        "  Returns:\n",
        "    adjacency list of size num_head where each element is of size\n",
        "    from_seq_length//from_block_size-2 by num_rand_blocks\n",
        "  \"\"\"\n",
        "  # Total number of blocks in the mmask\n",
        "  num_blocks = seq_length//block_size\n",
        "  # Number of blocks per plan\n",
        "  plan_block_length = np.array(plan_from_length) // block_size\n",
        "  # till when to follow plan\n",
        "  max_plan_idx = plan_from_length.index(seq_length)\n",
        "  # Random Attention adjajency list\n",
        "  rand_attn = [np.zeros((num_blocks,\n",
        "                         np.sum(plan_num_rand_blocks[:max_plan_idx+1])),\n",
        "                        dtype=np.int32) for i in range(num_heads)]\n",
        "\n",
        "  # We will go iteratively over the plan blocks and pick random number of\n",
        "  # Attention blocks from the legally allowed blocks\n",
        "  for plan_idx in range(max_plan_idx+1):\n",
        "    rnd_r_cnt = 0\n",
        "    if plan_idx > 0:\n",
        "      # set the row for all from_blocks starting from 0 to\n",
        "      # plan_block_length[plan_idx-1]\n",
        "      # column indx start fromm plan_block_length[plan_idx-1] and ends at\n",
        "      # plan_block_length[plan_idx]\n",
        "      if plan_num_rand_blocks[plan_idx] > 0:\n",
        "        rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))\n",
        "        curr_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx+1]))\n",
        "        for blk_rw_idx in range(global_block_top,\n",
        "                                plan_block_length[plan_idx-1]):\n",
        "          for h in range(num_heads):\n",
        "            # print(\"head\", h, \"blk_rw_idx\", blk_rw_idx)\n",
        "            rand_attn[h][blk_rw_idx,\n",
        "                         rnd_r_cnt:curr_r_cnt] = get_single_block_row_attention(\n",
        "                             block_id=blk_rw_idx,\n",
        "                             to_start_block_id=plan_block_length[plan_idx - 1],\n",
        "                             to_end_block_id=plan_block_length[plan_idx],\n",
        "                             num_rand_blocks=plan_num_rand_blocks[plan_idx],\n",
        "                             window_block_left=window_block_left,\n",
        "                             window_block_right=window_block_right,\n",
        "                             global_block_left=global_block_left,\n",
        "                             global_block_right=global_block_right)\n",
        "\n",
        "      for pl_id in range(plan_idx):\n",
        "        if plan_num_rand_blocks[pl_id] == 0:\n",
        "          continue\n",
        "        for blk_rw_idx in range(plan_block_length[plan_idx-1],\n",
        "                                plan_block_length[plan_idx]):\n",
        "          rnd_r_cnt = 0\n",
        "          to_start_block_id = 0\n",
        "          if pl_id > 0:\n",
        "            rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:pl_id]))\n",
        "            to_start_block_id = plan_block_length[pl_id-1]\n",
        "          curr_r_cnt = int(np.sum(plan_num_rand_blocks[:pl_id+1]))\n",
        "          for h in range(num_heads):\n",
        "            # print(\"head\", h, \"blk_rw_idx\", blk_rw_idx)\n",
        "            rand_attn[h][blk_rw_idx,\n",
        "                         rnd_r_cnt:curr_r_cnt] = get_single_block_row_attention(\n",
        "                             block_id=blk_rw_idx,\n",
        "                             to_start_block_id=to_start_block_id,\n",
        "                             to_end_block_id=plan_block_length[pl_id],\n",
        "                             num_rand_blocks=plan_num_rand_blocks[pl_id],\n",
        "                             window_block_left=window_block_left,\n",
        "                             window_block_right=window_block_right,\n",
        "                             global_block_left=global_block_left,\n",
        "                             global_block_right=global_block_right)\n",
        "\n",
        "    if plan_num_rand_blocks[plan_idx] == 0:\n",
        "      continue\n",
        "    # print(\"Start from here\")\n",
        "    curr_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx+1]))\n",
        "    from_start_block_id = global_block_top\n",
        "    to_start_block_id = 0\n",
        "    if plan_idx > 0:\n",
        "      rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))\n",
        "      from_start_block_id = plan_block_length[plan_idx-1]\n",
        "      to_start_block_id = plan_block_length[plan_idx-1]\n",
        "\n",
        "    for blk_rw_idx in range(from_start_block_id, plan_block_length[plan_idx]):\n",
        "      for h in range(num_heads):\n",
        "        # print(\"head\", h, \"blk_rw_idx\", blk_rw_idx)\n",
        "        rand_attn[h][blk_rw_idx,\n",
        "                     rnd_r_cnt:curr_r_cnt] = get_single_block_row_attention(\n",
        "                         block_id=blk_rw_idx,\n",
        "                         to_start_block_id=to_start_block_id,\n",
        "                         to_end_block_id=plan_block_length[plan_idx],\n",
        "                         num_rand_blocks=plan_num_rand_blocks[plan_idx],\n",
        "                         window_block_left=window_block_left,\n",
        "                         window_block_right=window_block_right,\n",
        "                         global_block_left=global_block_left,\n",
        "                         global_block_right=global_block_right)\n",
        "\n",
        "  for nh in range(num_heads):\n",
        "    rand_attn[nh] = rand_attn[nh][global_block_top:num_blocks -\n",
        "                                  global_block_bottom, :]\n",
        "  return rand_attn\n",
        "\n",
        "\n",
        "def get_rand_attn_plan(from_seq_length, from_block_size, num_rand_blocks):\n",
        "  \"\"\"Gives the plan of where to put random attention.\n",
        "  Args:\n",
        "    from_seq_length: int. length of from sequence.\n",
        "    from_block_size: int. size of block in from sequence.\n",
        "    num_rand_blocks: int. Number of random chunks per row.\n",
        "  Returns:\n",
        "    plan_from_length: ending location of from block\n",
        "    plan_num_rand_blocks: number of random ending location for each block\n",
        "  \"\"\"\n",
        "  # general plan\n",
        "  plan_from_length = []\n",
        "  plan_num_rand_blocks = []\n",
        "  if (2*num_rand_blocks + 5) < (from_seq_length // from_block_size):\n",
        "    plan_from_length.append(int((2*num_rand_blocks + 5)*from_block_size))\n",
        "    plan_num_rand_blocks.append(num_rand_blocks)\n",
        "    plan_from_length.append(from_seq_length)\n",
        "    plan_num_rand_blocks.append(0)\n",
        "  elif (num_rand_blocks + 5) < (from_seq_length // from_block_size):\n",
        "    plan_from_length.append(int((num_rand_blocks + 5)*from_block_size))\n",
        "    plan_num_rand_blocks.append(num_rand_blocks//2)\n",
        "    plan_from_length.append(from_seq_length)\n",
        "    plan_num_rand_blocks.append(num_rand_blocks - (num_rand_blocks//2))\n",
        "  else:\n",
        "    plan_from_length.append(from_seq_length)\n",
        "    plan_num_rand_blocks.append(num_rand_blocks)\n",
        "\n",
        "  return plan_from_length, plan_num_rand_blocks\n",
        "\n",
        "\n",
        "def bigbird_block_rand_mask(from_seq_length,\n",
        "                            to_seq_length,\n",
        "                            from_block_size,\n",
        "                            to_block_size,\n",
        "                            num_rand_blocks,\n",
        "                            last_idx=-1):\n",
        "  \"\"\"Create adjacency list of random attention.\n",
        "  Args:\n",
        "    from_seq_length: int. length of from sequence.\n",
        "    to_seq_length: int. length of to sequence.\n",
        "    from_block_size: int. size of block in from sequence.\n",
        "    to_block_size: int. size of block in to sequence.\n",
        "    num_rand_blocks: int. Number of random chunks per row.\n",
        "    last_idx: if -1 then num_rand_blocks blocks chosen anywhere in to sequence,\n",
        "      if positive then num_rand_blocks blocks choosen only upto last_idx.\n",
        "  Returns:\n",
        "    adjacency list of size from_seq_length//from_block_size-2 by num_rand_blocks\n",
        "  \"\"\"\n",
        "  rand_attn = np.zeros(\n",
        "      (from_seq_length // from_block_size - 2, num_rand_blocks), dtype=np.int32)\n",
        "  middle_seq = np.arange(1, to_seq_length // to_block_size - 1, dtype=np.int32)\n",
        "  last = to_seq_length // to_block_size - 1\n",
        "  if last_idx > (2 * to_block_size):\n",
        "    last = (last_idx // to_block_size) - 1\n",
        "\n",
        "  r = num_rand_blocks  # shorthand\n",
        "  for i in range(1, from_seq_length // from_block_size-1):\n",
        "    start = i-2\n",
        "    end = i\n",
        "    if i == 1:\n",
        "      rand_attn[i-1, :] = np.random.permutation(middle_seq[2:last])[:r]\n",
        "    elif i == 2:\n",
        "      rand_attn[i-1, :] = np.random.permutation(middle_seq[3:last])[:r]\n",
        "    elif i == from_seq_length // from_block_size - 3:\n",
        "      rand_attn[i-1, :] = np.random.permutation(middle_seq[:last])[:r]\n",
        "      # Missing -3: should have been sliced till last-3\n",
        "    elif i == from_seq_length // from_block_size - 2:\n",
        "      rand_attn[i-1, :] = np.random.permutation(middle_seq[:last])[:r]\n",
        "      # Missing -4: should have been sliced till last-4\n",
        "    else:\n",
        "      if start > last:\n",
        "        start = last\n",
        "        rand_attn[i-1, :] = np.random.permutation(middle_seq[:start])[:r]\n",
        "      elif (end+1) == last:\n",
        "        rand_attn[i-1, :] = np.random.permutation(middle_seq[:start])[:r]\n",
        "      else:\n",
        "        rand_attn[i-1, :] = np.random.permutation(\n",
        "            np.concatenate((middle_seq[:start], middle_seq[end+1:last])))[:r]\n",
        "  return rand_attn\n",
        "\n",
        "\n",
        "def full_bigbird_mask(from_seq_length,\n",
        "                      to_seq_length,\n",
        "                      from_block_size,\n",
        "                      to_block_size,\n",
        "                      rand_attn):\n",
        "  \"\"\"Calculate BigBird attention pattern as a full dense matrix.\n",
        "  Args:\n",
        "    from_seq_length: int. length of from sequence.\n",
        "    to_seq_length: int. length of to sequence.\n",
        "    from_block_size: int. size of block in from sequence.\n",
        "    to_block_size: int. size of block in to sequence.\n",
        "    rand_attn: adjajency matrix for random attention.\n",
        "  Returns:\n",
        "    attention mask matrix of shape [from_seq_length, to_seq_length]\n",
        "  \"\"\"\n",
        "\n",
        "  attn_mask = np.zeros((MAX_SEQ_LEN, MAX_SEQ_LEN), dtype=np.int32)\n",
        "  for i in range(1, (MAX_SEQ_LEN // from_block_size) - 1):\n",
        "    attn_mask[(i) * from_block_size:(i + 1) * from_block_size,\n",
        "              (i - 1) * to_block_size:(i + 2) * to_block_size] = 1\n",
        "    for j in rand_attn[i - 1, :]:\n",
        "      attn_mask[i * from_block_size:(i + 1) * from_block_size,\n",
        "                j * to_block_size:(j + 1) * to_block_size] = 1\n",
        "\n",
        "  attn_mask[:from_block_size, :] = 1\n",
        "  attn_mask[:, :to_block_size] = 1\n",
        "  attn_mask[:, -to_block_size:] = 1\n",
        "  attn_mask[-from_block_size:, :] = 1\n",
        "  clipped_attn_mask = attn_mask[:from_seq_length, :to_seq_length]\n",
        "  return np.array(clipped_attn_mask, dtype=bool)\n",
        "\n",
        "\n",
        "def create_rand_mask_from_inputs(from_blocked_mask,\n",
        "                                 to_blocked_mask,\n",
        "                                 rand_attn,\n",
        "                                 num_attention_heads,\n",
        "                                 num_rand_blocks,\n",
        "                                 from_seq_length,\n",
        "                                 from_block_size):\n",
        "  \"\"\"Create 4D attention mask from a 3D tensor mask.\n",
        "  Args:\n",
        "    from_blocked_mask: 2D Tensor of shape [batch_size,\n",
        "      from_seq_length//from_block_size, from_block_size].\n",
        "    to_blocked_mask: int32 Tensor of shape [batch_size,\n",
        "      to_seq_length//to_block_size, to_block_size].\n",
        "    rand_attn: [batch_size, num_attention_heads,\n",
        "      from_seq_length//from_block_size-2, num_rand_blocks]\n",
        "    num_attention_heads: int. Number of attention heads.\n",
        "    num_rand_blocks: int. Number of random chunks per row.\n",
        "    from_seq_length: int. length of from sequence.\n",
        "    from_block_size: int. size of block in from sequence.\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, num_attention_heads,\n",
        "                           from_seq_length//from_block_size-2,\n",
        "                           from_block_size, num_rand_blocks*to_block_size].\n",
        "  \"\"\"\n",
        "  num_windows = from_seq_length // from_block_size - 2\n",
        "  rand_mask = tf.reshape(\n",
        "      tf.gather(to_blocked_mask, rand_attn, batch_dims=1), [\n",
        "          -1, num_attention_heads, num_windows,\n",
        "          num_rand_blocks * from_block_size\n",
        "      ])\n",
        "  rand_mask = tf.einsum(\"BLQ,BHLK->BHLQK\", from_blocked_mask[:, 1:-1],\n",
        "                        rand_mask)\n",
        "  return rand_mask\n",
        "\n",
        "\n",
        "def create_band_mask_from_inputs(from_blocked_mask, to_blocked_mask):\n",
        "  \"\"\"Create 4D attention mask from a 3D blocked tensor mask.\n",
        "  Args:\n",
        "    from_blocked_mask: 3D Tensor of shape [batch_size,\n",
        "      from_seq_length//from_block_size, from_block_size].\n",
        "    to_blocked_mask: 3D Tensor of shape [batch_size,\n",
        "      to_seq_length//to_block_size, to_block_size].\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, 1, from_seq_length//from_block_size-4,\n",
        "                           from_block_size,  3*to_block_size].\n",
        "  \"\"\"\n",
        "  exp_blocked_to_pad = tf.concat(\n",
        "      [to_blocked_mask[:, 1:-3], to_blocked_mask[:, 2:-2],\n",
        "       to_blocked_mask[:, 3:-1]], 2)\n",
        "  band_mask = tf.einsum(\n",
        "      \"BLQ,BLK->BLQK\", from_blocked_mask[:, 2:-2], exp_blocked_to_pad)\n",
        "  band_mask = tf.expand_dims(band_mask, 1)\n",
        "  return band_mask\n",
        "\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_mask, to_mask):\n",
        "  \"\"\"Create attention mask from a 2D tensor mask.\n",
        "  Args:\n",
        "    from_mask: float32 Tensor of shape [batch_size, from_seq_length].\n",
        "    to_mask: float32 Tensor of shape [batch_size, to_seq_length].\n",
        "  Returns:\n",
        "    float32 Tensor of shape [batch_size, 1, from_seq_length, to_seq_length].\n",
        "  \"\"\"\n",
        "  mask = tf.einsum(\"BF,BT->BFT\", from_mask, to_mask)\n",
        "\n",
        "  # expand to create a slot for heads.\n",
        "  mask = tf.expand_dims(mask, 1)\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def bigbird_block_sparse_attention(query_layer,\n",
        "                                   key_layer,\n",
        "                                   value_layer,\n",
        "                                   band_mask,\n",
        "                                   from_mask,\n",
        "                                   to_mask,\n",
        "                                   from_blocked_mask,\n",
        "                                   to_blocked_mask,\n",
        "                                   rand_attn,\n",
        "                                   num_attention_heads,\n",
        "                                   size_per_head,\n",
        "                                   num_rand_blocks,\n",
        "                                   from_seq_length,\n",
        "                                   to_seq_length,\n",
        "                                   from_block_size,\n",
        "                                   to_block_size):\n",
        "  \"\"\"BigBird attention sparse calculation using blocks in linear time.\n",
        "  Assumes from_seq_length//from_block_size == to_seq_length//to_block_size.\n",
        "  A pure function with a long argument list to allow easy use outside our\n",
        "  framework.\n",
        "  Args:\n",
        "    query_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "      from_seq_length, size_per_head]\n",
        "    key_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "      to_seq_length, size_per_head]\n",
        "    value_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "      to_seq_length, size_per_head]\n",
        "    band_mask: float32 Tensor of shape [batch_size, 1,\n",
        "      from_seq_length//from_block_size-4, from_block_size, 3*to_block_size].\n",
        "      The values should be 1 or 0. The attention scores will effectively be\n",
        "      set to -infinity for any positions in the mask that are 0, and will be\n",
        "      unchanged for positions that are 1.\n",
        "    from_mask: float32 Tensor of shape [batch_size, 1, from_seq_length, 1].\n",
        "      The values should be 1 or 0. The attention scores will effectively be set\n",
        "      to -infinity for any positions in the mask that are 0, and will be\n",
        "      unchanged for positions that are 1.\n",
        "    to_mask: float32 Tensor of shape [batch_size, 1, 1, to_seq_length].\n",
        "      The values should be 1 or 0. The attention scores will effectively be set\n",
        "      to -infinity for any positions in the mask that are 0, and will be\n",
        "      unchanged for positions that are 1.\n",
        "    from_blocked_mask: float32 Tensor of shape [batch_size,\n",
        "      from_seq_length//from_block_size, from_block_size].\n",
        "      Same as from_mask, just reshaped.\n",
        "    to_blocked_mask: float32 Tensor of shape [batch_size,\n",
        "      to_seq_length//to_block_size, to_block_size].\n",
        "      Same as to_mask, just reshaped.\n",
        "    rand_attn: int32 Tensor of shape [num_attention_heads,\n",
        "      from_seq_length//from_block_size-2, num_rand_blocks] specifying which\n",
        "      blocks to attend to for each from sequence block (except 2 global ones).\n",
        "    num_attention_heads: int. Number of attention heads.\n",
        "    size_per_head: int. Size of each attention head.\n",
        "    num_rand_blocks: int. Number of random chunks per row.\n",
        "    from_seq_length: int. length of from sequence.\n",
        "    to_seq_length: int. length of to sequence.\n",
        "    from_block_size: int. size of block in from sequence.\n",
        "    to_block_size: int. size of block in to sequence.\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length, num_attention_heads,\n",
        "      size_per_head].\n",
        "  \"\"\"\n",
        "  assert from_seq_length//from_block_size == to_seq_length//to_block_size\n",
        "\n",
        "  # repeat for batch size\n",
        "  batch_size = utils.get_shape_list(query_layer)[0]\n",
        "  rand_attn = tf.expand_dims(rand_attn, 0)\n",
        "  rand_attn = tf.repeat(rand_attn, batch_size, 0)\n",
        "\n",
        "  rand_mask = create_rand_mask_from_inputs(\n",
        "      from_blocked_mask, to_blocked_mask, rand_attn,\n",
        "      num_attention_heads, num_rand_blocks,\n",
        "      from_seq_length, from_block_size)\n",
        "\n",
        "  # Define shorthands\n",
        "  # b = batch_size\n",
        "  h = num_attention_heads\n",
        "  r = num_rand_blocks\n",
        "  d = size_per_head\n",
        "  m = from_seq_length\n",
        "  n = to_seq_length\n",
        "  wm = from_block_size\n",
        "  wn = to_block_size\n",
        "\n",
        "  blocked_query_matrix = tf.reshape(query_layer, (-1, h, m // wm, wm, d))\n",
        "  blocked_key_matrix = tf.reshape(key_layer, (-1, h, n // wn, wn, d))\n",
        "  blocked_value_matrix = tf.reshape(value_layer, (-1, h, n // wn, wn, d))\n",
        "  gathered_key = tf.reshape(\n",
        "      tf.gather(blocked_key_matrix, rand_attn, batch_dims=2, name=\"gather_key\"),\n",
        "      (-1, h, m // wm - 2, r * wn, d))  # [b, h, n//wn-2, r, wn, -1]\n",
        "  gathered_value = tf.reshape(\n",
        "      tf.gather(\n",
        "          blocked_value_matrix, rand_attn, batch_dims=2, name=\"gather_value\"),\n",
        "      (-1, h, m // wm - 2, r * wn, d))  # [b, h, n//wn-2, r, wn, -1]\n",
        "\n",
        "  first_product = tf.einsum(\n",
        "      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, 0],\n",
        "      key_layer)  # [b, h, wm, -1] x [b, h, n, -1] ==> [b, h, wm, n]\n",
        "  first_product = tf.multiply(first_product, 1.0 / np.sqrt(d))\n",
        "  first_product += (1.0 - to_mask) * -10000.0\n",
        "  first_attn_weights = tf.nn.softmax(first_product)  # [b, h, wm, n]\n",
        "  first_context_layer = tf.einsum(\n",
        "      \"BHQK,BHKD->BHQD\", first_attn_weights,\n",
        "      value_layer)  # [b, h, wm, n] x [b, h, n, -1] ==> [b, h, wm, -1]\n",
        "  first_context_layer = tf.expand_dims(first_context_layer, 2)\n",
        "\n",
        "  second_key_mat = tf.concat([\n",
        "      blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, 1],\n",
        "      blocked_key_matrix[:, :, 2], blocked_key_matrix[:, :, -1],\n",
        "      gathered_key[:, :, 0]], 2)  # [b, h, (4+r)*wn, -1]\n",
        "  second_value_mat = tf.concat([\n",
        "      blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, 1],\n",
        "      blocked_value_matrix[:, :, 2], blocked_value_matrix[:, :, -1],\n",
        "      gathered_value[:, :, 0]], 2)  # [b, h, (4+r)*wn, -1]\n",
        "  second_product = tf.einsum(\n",
        "      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, 1], second_key_mat\n",
        "  )  # [b, h, wm, -1] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, (4+r)*wn]\n",
        "  second_seq_pad = tf.concat([\n",
        "      to_mask[:, :, :, :3 * wn], to_mask[:, :, :, -wn:],\n",
        "      tf.ones_like(rand_mask[:, :1, 0, :1])], 3)\n",
        "  second_rand_pad = tf.concat(\n",
        "      [tf.ones_like(second_product[:, :, :, :4 * wn]), rand_mask[:, :, 0]], 3)\n",
        "  second_product = tf.multiply(second_product, 1.0 / np.sqrt(d))\n",
        "  second_product += (1.0 -\n",
        "                     tf.minimum(second_seq_pad, second_rand_pad)) * -10000.0\n",
        "  second_attn_weights = tf.nn.softmax(second_product)  # [b , h, wm, (4+r)*wn]\n",
        "  second_context_layer = tf.einsum(\n",
        "      \"BHQK,BHKD->BHQD\", second_attn_weights, second_value_mat\n",
        "  )  # [b, h, wm, (4+r)*wn] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, -1]\n",
        "  second_context_layer = tf.expand_dims(second_context_layer, 2)\n",
        "\n",
        "  exp_blocked_key_matrix = tf.concat([\n",
        "      blocked_key_matrix[:, :, 1:-3], blocked_key_matrix[:, :, 2:-2],\n",
        "      blocked_key_matrix[:, :, 3:-1]], 3)  # [b, h, m//wm-4, 3*wn, -1]\n",
        "  exp_blocked_value_matrix = tf.concat([\n",
        "      blocked_value_matrix[:, :, 1:-3], blocked_value_matrix[:, :, 2:-2],\n",
        "      blocked_value_matrix[:, :, 3:-1]], 3)  # [b, h, m//wm-4, 3*wn, -1]\n",
        "  middle_query_matrix = blocked_query_matrix[:, :, 2:-2]\n",
        "  inner_band_product = tf.einsum(\n",
        "      \"BHLQD,BHLKD->BHLQK\", middle_query_matrix, exp_blocked_key_matrix\n",
        "  )  # [b, h, m//wm-4, wm, -1] x [b, h, m//wm-4, 3*wn, -1]\n",
        "  #     ==> [b, h, m//wm-4, wm, 3*wn]\n",
        "  inner_band_product = tf.multiply(inner_band_product, 1.0 / np.sqrt(d))\n",
        "  rand_band_product = tf.einsum(\n",
        "      \"BHLQD,BHLKD->BHLQK\", middle_query_matrix, gathered_key[:, :, 1:-1]\n",
        "  )  # [b, h, m//wm-4, wm, -1] x [b, h, m//wm-4, r*wn, -1]\n",
        "  #     ==> [b, h, m//wm-4, wm, r*wn]\n",
        "  rand_band_product = tf.multiply(rand_band_product, 1.0 / np.sqrt(d))\n",
        "  first_band_product = tf.einsum(\n",
        "      \"BHLQD,BHKD->BHLQK\", middle_query_matrix, blocked_key_matrix[:, :, 0]\n",
        "  )  # [b, h, m//wm-4, wm, -1] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, wn]\n",
        "  first_band_product = tf.multiply(first_band_product, 1.0 / np.sqrt(d))\n",
        "  last_band_product = tf.einsum(\n",
        "      \"BHLQD,BHKD->BHLQK\", middle_query_matrix, blocked_key_matrix[:, :, -1]\n",
        "  )  # [b, h, m//wm-4, wm, -1] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, wn]\n",
        "  last_band_product = tf.multiply(last_band_product, 1.0 / np.sqrt(d))\n",
        "  inner_band_product += (1.0 - band_mask) * -10000.0\n",
        "  first_band_product += (\n",
        "      1.0 - tf.expand_dims(to_mask[:, :, :, :wn], 3)) * -10000.0\n",
        "  last_band_product += (\n",
        "      1.0 - tf.expand_dims(to_mask[:, :, :, -wn:], 3)) * -10000.0\n",
        "  rand_band_product += (1.0 - rand_mask[:, :, 1:-1]) * -10000.0\n",
        "  band_product = tf.concat([\n",
        "      first_band_product, inner_band_product, rand_band_product,\n",
        "      last_band_product], -1)  # [b, h, m//wm-4, wm, (5+r)*wn]\n",
        "  attn_weights = tf.nn.softmax(band_product)  # [b, h, m//wm-4, wm, (5+r)*wn]\n",
        "  context_layer = tf.einsum(\n",
        "      \"BHLQK,BHLKD->BHLQD\", attn_weights[:, :, :, :, wn:4 * wn],\n",
        "      exp_blocked_value_matrix\n",
        "  )  # [b, h, m//wm-4, wm, 3*wn] x [b, h, m//wm-4, 3*wn, -1]\n",
        "  #     ==> [b, h, m//wm-4, wm, -1]\n",
        "  context_layer += tf.einsum(\n",
        "      \"BHLQK,BHLKD->BHLQD\", attn_weights[:, :, :, :, 4 * wn:-wn],\n",
        "      gathered_value[:, :, 1:-1]\n",
        "  )  # [b, h, m//wm-4, wm, r*wn] x [b, h, m//wm-4, r*wn, -1]\n",
        "  #     ==> [b, h, m//wm-4, wm, -1]\n",
        "  context_layer += tf.einsum(\n",
        "      \"BHLQK,BHKD->BHLQD\", attn_weights[:, :, :, :, :wn],\n",
        "      blocked_value_matrix[:, :, 0]\n",
        "  )  # [b, h, m//wm-4, wm, wn] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, -1]\n",
        "  context_layer += tf.einsum(\n",
        "      \"BHLQK,BHKD->BHLQD\", attn_weights[:, :, :, :, -wn:],\n",
        "      blocked_value_matrix[:, :, -1]\n",
        "  )  # [b, h, m//wm-4, wm, wn] x [b, h, wn, -1] ==> [b, h, m//wm-4, wm, -1]\n",
        "\n",
        "  second_last_key_mat = tf.concat([\n",
        "      blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, -3],\n",
        "      blocked_key_matrix[:, :, -2], blocked_key_matrix[:, :, -1],\n",
        "      gathered_key[:, :, -1]], 2)  # [b, h, (4+r)*wn, -1]\n",
        "  second_last_value_mat = tf.concat([\n",
        "      blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, -3],\n",
        "      blocked_value_matrix[:, :, -2], blocked_value_matrix[:, :, -1],\n",
        "      gathered_value[:, :, -1]], 2)  # [b, h, (4+r)*wn, -1]\n",
        "  second_last_product = tf.einsum(\n",
        "      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, -2], second_last_key_mat\n",
        "  )  # [b, h, wm, -1] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, (4+r)*wn]\n",
        "  second_last_seq_pad = tf.concat([\n",
        "      to_mask[:, :, :, :wn], to_mask[:, :, :, -3 * wn:],\n",
        "      tf.ones_like(rand_mask[:, :1, 0, :1])], 3)\n",
        "  second_last_rand_pad = tf.concat(\n",
        "      [tf.ones_like(second_last_product[:, :, :, :4 * wn]),\n",
        "       rand_mask[:, :, -1]], 3)\n",
        "  second_last_product = tf.multiply(second_last_product, 1.0 / np.sqrt(d))\n",
        "  second_last_product += (\n",
        "      1.0 - tf.minimum(second_last_seq_pad, second_last_rand_pad)) * -10000.0\n",
        "  second_last_attn_weights = tf.nn.softmax(\n",
        "      second_last_product)  # [b, h, wm, (4+r)*wn]\n",
        "  second_last_context_layer = tf.einsum(\n",
        "      \"BHQK,BHKD->BHQD\", second_last_attn_weights, second_last_value_mat\n",
        "  )  # [b, h, wm, (4+r)*wn] x [b, h, (4+r)*wn, -1] ==> [b, h, wm, -1]\n",
        "  second_last_context_layer = tf.expand_dims(second_last_context_layer, 2)\n",
        "\n",
        "  last_product = tf.einsum(\n",
        "      \"BHQD,BHKD->BHQK\", blocked_query_matrix[:, :, -1],\n",
        "      key_layer)  # [b, h, wm, -1] x [b, h, n, -1] ==> [b, h, wm, n]\n",
        "  last_product = tf.multiply(last_product, 1.0 / np.sqrt(d))\n",
        "  last_product += (1.0 - to_mask) * -10000.0\n",
        "  last_attn_weights = tf.nn.softmax(last_product)  # [b, h, wm, n]\n",
        "  last_context_layer = tf.einsum(\n",
        "      \"BHQK,BHKD->BHQD\", last_attn_weights,\n",
        "      value_layer)  # [b, h, wm, n] x [b, h, n, -1] ==> [b, h, wm, -1]\n",
        "  last_context_layer = tf.expand_dims(last_context_layer, 2)\n",
        "\n",
        "  context_layer = tf.concat([\n",
        "      first_context_layer, second_context_layer, context_layer,\n",
        "      second_last_context_layer, last_context_layer\n",
        "  ], 2)\n",
        "  context_layer = tf.reshape(context_layer, (-1, h, m, d)) * from_mask\n",
        "  context_layer = tf.transpose(context_layer, (0, 2, 1, 3))\n",
        "  return context_layer\n",
        "\n",
        "\n",
        "class MultiHeadedAttentionLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"A multi-headed attention layer.\n",
        "  It implements following types of multi-headed attention:\n",
        "  - original_full attention from \"Attention is all you Need\".\n",
        "  - simulated_sparse attention from BigBird with full quadratic implemention.\n",
        "  - block_sparse attention from BigBird with memory efficient linear impl.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               attention_type,\n",
        "               num_attention_heads=1,\n",
        "               size_per_head=512,\n",
        "               num_rand_blocks=3,\n",
        "               from_seq_length=1024,\n",
        "               to_seq_length=1024,\n",
        "               from_block_size=64,\n",
        "               to_block_size=64,\n",
        "               attention_probs_dropout_prob=0.0,\n",
        "               initializer_range=0.02,\n",
        "               use_bias=True,\n",
        "               seed=None,\n",
        "               query_act=None,\n",
        "               key_act=None,\n",
        "               value_act=None,\n",
        "               name=None):\n",
        "    \"\"\"Constructor for a multi-headed attention layer.\n",
        "    Args:\n",
        "      attention_type: Type of attention, needs to be one of ['original_full',\n",
        "        'simulated_sparse', 'block_sparse'].\n",
        "      num_attention_heads: (optional) int. Number of attention heads.\n",
        "      size_per_head: (optional) int. Size of each attention head.\n",
        "      num_rand_blocks: (optional) int. Number of random chunks per row.\n",
        "      from_seq_length: int. (optional) length of from sequence.\n",
        "      to_seq_length: int. (optional) length of to sequence.\n",
        "      from_block_size: (optional) int. size of block in from sequence.\n",
        "      to_block_size: (optional) int. size of block in to sequence.\n",
        "      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention probabilities.\n",
        "      initializer_range: (optional) float. Range of the weight initializer.\n",
        "      use_bias: Whether the layer uses a bias vector.\n",
        "      seed: (Optional) int. Reandom seed for generating random mask.\n",
        "      query_act: (optional) Activation function for the query transform.\n",
        "      key_act: (optional) Activation function for the key transform.\n",
        "      value_act: (optional) Activation function for the value transform.\n",
        "      name: The name scope of this layer.\n",
        "    \"\"\"\n",
        "    super(MultiHeadedAttentionLayer, self).__init__(name=name)\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.size_per_head = size_per_head\n",
        "    self.num_rand_blocks = num_rand_blocks\n",
        "    self.from_seq_length = from_seq_length\n",
        "    self.to_seq_length = to_seq_length\n",
        "    self.from_block_size = from_block_size\n",
        "    self.to_block_size = to_block_size\n",
        "    self.seed = seed\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      self.query_layer = utils.Dense3dLayer(\n",
        "          num_attention_heads, size_per_head,\n",
        "          utils.create_initializer(initializer_range), query_act,\n",
        "          \"query\", head_first=True, use_bias=use_bias)\n",
        "\n",
        "      self.key_layer = utils.Dense3dLayer(\n",
        "          num_attention_heads, size_per_head,\n",
        "          utils.create_initializer(initializer_range), key_act,\n",
        "          \"key\", head_first=True, use_bias=use_bias)\n",
        "\n",
        "      self.value_layer = utils.Dense3dLayer(\n",
        "          num_attention_heads, size_per_head,\n",
        "          utils.create_initializer(initializer_range), value_act,\n",
        "          \"value\", head_first=True, use_bias=use_bias)\n",
        "\n",
        "    if attention_type == \"original_full\":\n",
        "      logging.info(\"**** Using original full attention ****\")\n",
        "      self.attention_dropout = recompute_grad.RecomputingDropout(\n",
        "          attention_probs_dropout_prob)\n",
        "      self.attn_impl = self.original_full_attention\n",
        "    elif attention_type == \"simulated_sparse\":\n",
        "      logging.info(\"**** Using simulated sparse attention ****\")\n",
        "      self.attention_dropout = lambda x, training=None: x\n",
        "      self.rand_attn = self.generate_rand_attn_list()\n",
        "      self.rand_block_mask = self.convert_attn_list_to_mask(self.rand_attn)\n",
        "      self.attn_impl = self.bigbird_simulated_attention\n",
        "    elif attention_type == \"block_sparse\":\n",
        "      logging.info(\"**** Using block sparse attention ****\")\n",
        "      assert from_seq_length//from_block_size == to_seq_length//to_block_size, (\n",
        "          \"Error the number of blocks needs to be same!\")\n",
        "      self.attention_dropout = None\n",
        "      self.rand_attn = self.generate_rand_attn_list()\n",
        "      self.attn_impl = self.bigbird_block_sparse_attention\n",
        "    else:\n",
        "      raise NotImplementedError(\n",
        "          \"Attention type {} is not implemented\".format(attention_type))\n",
        "\n",
        "  def generate_rand_attn_list(self):\n",
        "    # generate random attention and corresponding masks\n",
        "    if self.seed is not None:\n",
        "      np.random.seed(self.seed)\n",
        "    # old plans used in paper\n",
        "    if self.from_seq_length in [1024, 2048, 3072, 4096]:\n",
        "      rand_attn = [\n",
        "          bigbird_block_rand_mask(  # pylint: disable=g-complex-comprehension\n",
        "              MAX_SEQ_LEN, MAX_SEQ_LEN,\n",
        "              self.from_block_size, self.to_block_size, self.num_rand_blocks,\n",
        "              last_idx=1024\n",
        "          )[:(self.from_seq_length // self.from_block_size - 2)]\n",
        "          for _ in range(self.num_attention_heads)\n",
        "      ]\n",
        "    else:\n",
        "      plan_from_length, plan_num_rand_blocks = get_rand_attn_plan(\n",
        "          self.from_seq_length, self.from_block_size, self.num_rand_blocks)\n",
        "      rand_attn = bigbird_block_rand_mask_with_head(\n",
        "          seq_length=self.from_seq_length,\n",
        "          block_size=self.from_block_size,\n",
        "          num_heads=self.num_attention_heads,\n",
        "          plan_from_length=plan_from_length,\n",
        "          plan_num_rand_blocks=plan_num_rand_blocks)\n",
        "    rand_attn = np.stack(rand_attn, axis=0)\n",
        "    return tf.constant(rand_attn, dtype=tf.int32)\n",
        "\n",
        "  def convert_attn_list_to_mask(self, rand_attn):\n",
        "    temp_mask = [\n",
        "        full_bigbird_mask(  # pylint: disable=g-complex-comprehension\n",
        "            self.from_seq_length, self.to_seq_length,\n",
        "            self.from_block_size, self.to_block_size,\n",
        "            rand_attn=rand_attn[i])\n",
        "        for i in range(self.num_attention_heads)\n",
        "    ]\n",
        "    temp_mask = np.stack(temp_mask, axis=0)\n",
        "    temp_mask = np.array(temp_mask, dtype=bool)\n",
        "    rand_block_mask = tf.constant(temp_mask, dtype=tf.bool)  # [N, F, T]\n",
        "    return tf.cast(rand_block_mask, tf.float32)\n",
        "\n",
        "  def original_full_attention(self,\n",
        "                              query_layer,\n",
        "                              key_layer,\n",
        "                              value_layer,\n",
        "                              masks,\n",
        "                              training=None):\n",
        "    \"\"\"Full quadratic attention calculation.\n",
        "    Args:\n",
        "      query_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        from_seq_length, size_per_head]\n",
        "      key_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        to_seq_length, size_per_head]\n",
        "      value_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        to_seq_length, size_per_head]\n",
        "      masks: a list containing float32 Tensor representing attention_mask\n",
        "        of shape [batch_size, from_seq_length, to_seq_length].\n",
        "        The values should be 1 or 0. The attention scores will effectively be\n",
        "        set to -infinity for any positions in the mask that are 0, and\n",
        "        will be unchanged for positions that are 1.\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, from_seq_length, num_attention_heads,\n",
        "        size_per_head].\n",
        "    \"\"\"\n",
        "    attention_mask = masks[0]\n",
        "\n",
        "    # Directly take n^2 dot product between \"query\" and \"key\".\n",
        "    attention_scores = tf.einsum(\"BNFH,BNTH->BNFT\", query_layer, key_layer)\n",
        "    attention_scores = tf.multiply(attention_scores,\n",
        "                                   1.0 / np.sqrt(float(self.size_per_head)))\n",
        "\n",
        "    if attention_mask is not None:\n",
        "      # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "      # masked positions, this operation will create a tensor which is 0.0 for\n",
        "      # positions we want to attend and -10000.0 for masked positions.\n",
        "      adder = (1.0 - attention_mask) * -10000.0\n",
        "\n",
        "      # Since we are adding it to the raw scores before the softmax, this is\n",
        "      # effectively the same as removing these entirely.\n",
        "      attention_scores += adder\n",
        "\n",
        "    # Normalize the attention scores to probabilities.\n",
        "    # `attention_probs` = [B, N, F, T]\n",
        "    attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "    # This is actually dropping out entire tokens to attend to, which might\n",
        "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    attention_probs = self.attention_dropout(attention_probs, training=training)\n",
        "\n",
        "    # `context_layer` = [B, F, N, H]\n",
        "    context_layer = tf.einsum(\"BNFT,BNTH->BFNH\", attention_probs, value_layer)\n",
        "    return context_layer\n",
        "\n",
        "  def bigbird_simulated_attention(self,\n",
        "                                  query_layer,\n",
        "                                  key_layer,\n",
        "                                  value_layer,\n",
        "                                  masks,\n",
        "                                  training=None):\n",
        "    \"\"\"BigBird attention calculation using masks in quadratic time.\n",
        "    Args:\n",
        "      query_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        from_seq_length, size_per_head]\n",
        "      key_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        to_seq_length, size_per_head]\n",
        "      value_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        to_seq_length, size_per_head]\n",
        "      masks: a list containing float32 Tensor representing attention_mask\n",
        "        of shape [batch_size, from_seq_length, to_seq_length].\n",
        "        The values should be 1 or 0. The attention scores will effectively be\n",
        "        set to -infinity for any positions in the mask that are 0, and\n",
        "        will be unchanged for positions that are 1.\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, from_seq_length, num_attention_heads,\n",
        "        size_per_head].\n",
        "    \"\"\"\n",
        "    attention_mask = masks[0]\n",
        "    rand_block_mask = tf.expand_dims(self.rand_block_mask, 0)  # [1, N, F, T]\n",
        "    if attention_mask is not None:\n",
        "      attention_mask = tf.minimum(attention_mask, rand_block_mask)\n",
        "    else:\n",
        "      attention_mask = rand_block_mask\n",
        "    return self.original_full_attention(\n",
        "        query_layer, key_layer, value_layer, [attention_mask],\n",
        "        training=training)\n",
        "\n",
        "  def bigbird_block_sparse_attention(self,\n",
        "                                     query_layer,\n",
        "                                     key_layer,\n",
        "                                     value_layer,\n",
        "                                     masks,\n",
        "                                     training=None):\n",
        "    \"\"\"BigBird attention sparse calculation using blocks in linear time.\n",
        "    Args:\n",
        "      query_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        from_seq_length, size_per_head]\n",
        "      key_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        to_seq_length, size_per_head]\n",
        "      value_layer: float Tensor of shape [batch_size, num_attention_heads,\n",
        "        to_seq_length, size_per_head]\n",
        "      masks: A list of 5 masks used in BigBird attention at position 1 to 5.\n",
        "        Position 0 (first element) is not used can be left as none. In the mask,\n",
        "        the values should be 1 or 0. The attention scores will effectively\n",
        "        be set to -infinity for any positions in the mask that are 0,\n",
        "        and will be unchanged for positions that are 1.\n",
        "           \"None\": Not needed.\n",
        "            \"band_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, 1, from_seq_length//from_block_size-4,\n",
        "              from_block_size, 3*to_block_size].\n",
        "            \"from_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, 1, from_seq_length, 1].\n",
        "            \"to_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, 1, 1, to_seq_length].\n",
        "            \"from_blocked_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, from_seq_length//from_block_size, from_block_size].\n",
        "              Same as from_mask, just reshaped.\n",
        "            \"to_blocked_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, to_seq_length//to_block_size, to_block_size].\n",
        "              Same as to_mask, just reshaped.}\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, from_seq_length, num_attention_heads,\n",
        "        size_per_head].\n",
        "    \"\"\"\n",
        "\n",
        "    (_, band_mask, from_mask, to_mask,\n",
        "     from_blocked_mask, to_blocked_mask) = masks\n",
        "\n",
        "    return bigbird_block_sparse_attention(\n",
        "        query_layer, key_layer, value_layer,\n",
        "        band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask,\n",
        "        self.rand_attn, self.num_attention_heads, self.size_per_head,\n",
        "        self.num_rand_blocks, self.from_seq_length, self.to_seq_length,\n",
        "        self.from_block_size, self.to_block_size)\n",
        "\n",
        "  def call(self,\n",
        "           from_tensor,\n",
        "           to_tensor,\n",
        "           masks,\n",
        "           cache=None,\n",
        "           decode_i=None,\n",
        "           training=None):\n",
        "    \"\"\"Implements a multi-headed attention layer from from_tensor to to_tensor.\n",
        "    Args:\n",
        "      from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
        "        from_width]\n",
        "      to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
        "      masks: A list of masks used in different attention. Only relevant masks\n",
        "        need to be supplied and at other positions place None. In the mask,\n",
        "        the values should be 1 or 0. The attention scores will effectively\n",
        "        be set to -infinity for any positions in the mask that are 0,\n",
        "        and will be unchanged for positions that are 1.\n",
        "           \"attention_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, from_seq_length, to_seq_length].\n",
        "            \"band_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, 1, from_seq_length//from_block_size-4,\n",
        "              from_block_size, 3*to_block_size].\n",
        "            \"from_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, 1, from_seq_length, 1].\n",
        "            \"to_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, 1, 1, to_seq_length].\n",
        "            \"from_blocked_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, from_seq_length//from_block_size, from_block_size].\n",
        "              Same as from_mask, just reshaped.\n",
        "            \"to_blocked_mask\": (optional) float32 Tensor of shape\n",
        "              [batch_size, to_seq_length//to_block_size, to_block_size].\n",
        "              Same as to_mask, just reshaped.}\n",
        "      cache: (Used during prediction) A dictionary with tensors containing\n",
        "        results of previous attentions. The dictionary must have the items:\n",
        "            {\"k\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head],\n",
        "             \"v\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head]}\n",
        "      decode_i: (Used during prediction) current location of decoding\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, from_seq_length, num_attention_heads,\n",
        "        size_per_head].\n",
        "    Raises:\n",
        "      ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "      NotImplementedError: For unknown attention type.\n",
        "    \"\"\"\n",
        "\n",
        "    # Scalar dimensions referenced here:\n",
        "    #   b = batch size (number of sequences)\n",
        "    #   m = `from_tensor` sequence length\n",
        "    #   n = `to_tensor` sequence length\n",
        "    #   h = `num_attention_heads`\n",
        "    #   d = `size_per_head`\n",
        "\n",
        "    # `query` = [b, h, m, d]\n",
        "    query = self.query_layer(from_tensor)\n",
        "\n",
        "    # `key` = [b, h, n, d]\n",
        "    key = self.key_layer(to_tensor)\n",
        "\n",
        "    # `value_layer` = [b, h, n, d]\n",
        "    value = self.value_layer(to_tensor)\n",
        "\n",
        "    if cache is not None and decode_i is not None:\n",
        "      max_len = utils.get_shape_list(cache[\"k\"])[2]\n",
        "      indices_select = tf.reshape(\n",
        "          tf.one_hot(decode_i, max_len, dtype=to_tensor.dtype),\n",
        "          [1, 1, max_len, 1])\n",
        "      key = cache[\"k\"] + key * indices_select\n",
        "      value = cache[\"v\"] + value * indices_select\n",
        "      cache[\"k\"] = key\n",
        "      cache[\"v\"] = value\n",
        "\n",
        "    contextual_output = self.attn_impl(\n",
        "        query, key, value, masks, training=training)\n",
        "\n",
        "    return contextual_output\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fj5kWF8Cg4b"
      },
      "source": [
        "####################### modeling #########################\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"The main BigBird model and related functions.\"\"\"\n",
        "\n",
        "import copy\n",
        "\n",
        "from absl import logging\n",
        "from bigbird.core import decoder\n",
        "from bigbird.core import encoder\n",
        "from bigbird.core import utils\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "class BertModel(tf.keras.layers.Layer):\n",
        "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
        "  Example usage:\n",
        "  ```python\n",
        "  # Already been converted into SentencePiece token ids\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "  params = utils.BigBirdConfig(vocab_size=32000, hidden_size=512,\n",
        "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "  model = modeling.BertModel(params, train=True)\n",
        "  _, pooled_output = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
        "  label_embeddings = tf.get_variable(...)\n",
        "  logits = tf.matmul(pooled_output, label_embeddings)\n",
        "  ...\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "    \"\"\"Constructor for BertModel.\n",
        "    Args:\n",
        "      params: `BigBirdConfig` dictionary.\n",
        "    \"\"\"\n",
        "    self.params = copy.deepcopy(params)\n",
        "    self.scope = params[\"scope\"]\n",
        "    super(BertModel, self).__init__(name=self.scope)\n",
        "\n",
        "    # validate params\n",
        "    self.pad = lambda x: x\n",
        "    if params[\"max_encoder_length\"] <= 512:\n",
        "      logging.info(\"Switching to full attention for short sequences\")\n",
        "      self.params[\"attention_type\"] = \"original_full\"\n",
        "    if self.params[\"attention_type\"] == \"simulated_sparse\" or self.params[\n",
        "        \"attention_type\"] == \"block_sparse\":\n",
        "      if params[\"max_encoder_length\"] % params[\"block_size\"]:\n",
        "        logging.info(\"Expand max_encoder_length to next multiple of block_size\")\n",
        "        self.params[\"max_encoder_length\"] = (\n",
        "            params[\"max_encoder_length\"] // params[\"block_size\"] +\n",
        "            1) * params[\"block_size\"]\n",
        "        pad_size = self.params[\"max_encoder_length\"] - params[\n",
        "            \"max_encoder_length\"]\n",
        "        paddings = [[0, 0], [0, pad_size]]\n",
        "        self.pad = lambda x: tf.pad(x, paddings)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(self.scope, reuse=tf.compat.v1.AUTO_REUSE):\n",
        "      self.embeder = utils.EmbeddingLayer(\n",
        "          vocab_size=self.params[\"vocab_size\"],\n",
        "          emb_dim=self.params[\"hidden_size\"],\n",
        "          initializer=utils.create_initializer(\n",
        "              self.params[\"initializer_range\"]),\n",
        "          scale_emb=self.params[\"rescale_embedding\"],\n",
        "          use_token_type=True,\n",
        "          num_token_types=self.params[\"type_vocab_size\"],\n",
        "          use_position_embeddings=True,\n",
        "          max_position_embeddings=self.params[\"max_position_embeddings\"],\n",
        "          dropout_prob=self.params[\"hidden_dropout_prob\"])\n",
        "      self.encoder = encoder.EncoderStack(self.params)\n",
        "      self.pooler = utils.SimpleDenseLayer(\n",
        "          input_size=self.params[\"hidden_size\"],\n",
        "          output_size=self.params[\"hidden_size\"],\n",
        "          initializer=utils.create_initializer(\n",
        "              self.params[\"initializer_range\"]),\n",
        "          activation=tf.tanh,\n",
        "          name=\"pooler/dense\")\n",
        "\n",
        "  def call(self,\n",
        "           input_ids,\n",
        "           token_type_ids=None,\n",
        "           training=None):\n",
        "    \"\"\"Constructor for BertModel.\n",
        "    Args:\n",
        "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
        "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      sequence_output: Tensor of shape [batch_size, seq_length, hidden_size]\n",
        "      pooled_output: Tensor of shape [batch_size, hidden_size]\n",
        "    Raises:\n",
        "      ValueError: The config is invalid or one of the input tensor shapes\n",
        "        is invalid.\n",
        "    \"\"\"\n",
        "    # pad if needed\n",
        "    input_ids = self.pad(input_ids)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros_like(input_ids, dtype=tf.int32)\n",
        "    else:\n",
        "      token_type_ids = self.pad(token_type_ids)\n",
        "\n",
        "    # Perform embedding lookup on the word ids.\n",
        "    embedding_output = self.embeder(input_ids,\n",
        "                                    self.params[\"max_encoder_length\"],\n",
        "                                    token_type_ids=token_type_ids,\n",
        "                                    training=training)\n",
        "\n",
        "    # Generate mask.\n",
        "    input_mask = tf.where(input_ids > 0,\n",
        "                          tf.ones_like(input_ids), tf.zeros_like(input_ids))\n",
        "\n",
        "    # Run the stacked transformer.\n",
        "    sequence_output = self.encoder(embedding_output, input_mask, training)\n",
        "\n",
        "    # The \"pooler\" converts the encoded sequence tensor of shape\n",
        "    # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
        "    # [batch_size, hidden_size]. This is necessary for segment-level\n",
        "    # (or segment-pair-level) classification tasks where we need a fixed\n",
        "    # dimensional representation of the segment.\n",
        "    first_token_tensor = sequence_output[:, 0, :]\n",
        "    # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "    # to the first token. We assume that this has been pre-trained\n",
        "    pooled_output = self.pooler(first_token_tensor)\n",
        "\n",
        "    return sequence_output, pooled_output\n",
        "\n",
        "\n",
        "class TransformerModel(tf.keras.layers.Layer):\n",
        "  \"\"\"Encoder-Decoder transformer model.\n",
        "  Example usage:\n",
        "  ```python\n",
        "  # Already been converted into SentencePiece token ids\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  target_ids = tf.constant([[43, 76, 38], [56, 8, 0]])\n",
        "  params = utils.BigBirdConfig(vocab_size=32000, hidden_size=512,\n",
        "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "  model = modeling.TransformerModel(params, train=True)\n",
        "  predictions, _ = model(input_ids=input_ids, target_ids=target_ids)\n",
        "  log_probs, logits, pred_ids = predictions\n",
        "  ...\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "    \"\"\"Constructor for TransformerModel.\n",
        "    Args:\n",
        "      params: `BigBirdConfig` dictionary.\n",
        "    \"\"\"\n",
        "    self.params = copy.deepcopy(params)\n",
        "    self.scope = params[\"scope\"]\n",
        "    super(TransformerModel, self).__init__(name=self.scope)\n",
        "\n",
        "    # validate params\n",
        "    self.pad = lambda x: x\n",
        "    if params[\"max_encoder_length\"] <= 512:\n",
        "      logging.info(\"Switching to full attention for short sequences\")\n",
        "      self.params[\"attention_type\"] = \"original_full\"\n",
        "    if self.params[\"attention_type\"] == \"simulated_sparse\" or self.params[\n",
        "        \"attention_type\"] == \"block_sparse\":\n",
        "      if params[\"max_encoder_length\"] % params[\"block_size\"]:\n",
        "        logging.info(\"Expand max_encoder_length to next multiple of block_size\")\n",
        "        self.params[\"max_encoder_length\"] = (\n",
        "            params[\"max_encoder_length\"] // params[\"block_size\"] +\n",
        "            1) * params[\"block_size\"]\n",
        "        pad_size = self.params[\"max_encoder_length\"] - params[\n",
        "            \"max_encoder_length\"]\n",
        "        paddings = [[0, 0], [0, pad_size]]\n",
        "        self.pad = lambda x: tf.pad(x, paddings)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(self.scope, reuse=tf.compat.v1.AUTO_REUSE):\n",
        "      self.embeder = utils.EmbeddingLayer(\n",
        "          vocab_size=self.params[\"vocab_size\"],\n",
        "          emb_dim=self.params[\"hidden_size\"],\n",
        "          initializer=utils.create_initializer(\n",
        "              self.params[\"initializer_range\"]),\n",
        "          scale_emb=self.params[\"rescale_embedding\"],\n",
        "          use_token_type=False,\n",
        "          num_token_types=None,\n",
        "          use_position_embeddings=True,\n",
        "          max_position_embeddings=self.params[\"max_position_embeddings\"],\n",
        "          dropout_prob=self.params[\"hidden_dropout_prob\"])\n",
        "      self.encoder = encoder.EncoderStack(self.params)\n",
        "      self.decoder = decoder.DecoderStack(self.params)\n",
        "\n",
        "  def _encode(self, input_ids, training=None):\n",
        "    \"\"\"Generate continuous representation for ids.\n",
        "    Args:\n",
        "      input_ids: Int tensor with shape [batch_size, input_length].\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      A float tensors of shape\n",
        "          [batch_size, input_length, hidden_size].\n",
        "    \"\"\"\n",
        "    # pad if needed\n",
        "    input_ids = self.pad(input_ids)\n",
        "\n",
        "    # Perform embedding lookup on the word ids.\n",
        "    input_embs = self.embeder(\n",
        "        input_ids, self.params[\"max_encoder_length\"], training=training)\n",
        "\n",
        "    # Generate mask.\n",
        "    input_mask = tf.where(input_ids > 0,\n",
        "                          tf.ones_like(input_ids), tf.zeros_like(input_ids))\n",
        "\n",
        "    # Run the stacked transformer.\n",
        "    encoder_output = self.encoder(input_embs, input_mask, training=training)\n",
        "\n",
        "    return encoder_output, input_mask\n",
        "\n",
        "  def _get_start_token_ids(self, tensor_for_shape):\n",
        "    start_token_id = 2\n",
        "    batch_size = utils.get_shape_list(tensor_for_shape)[0]\n",
        "    return tf.ones([batch_size], dtype=tf.int32) * start_token_id\n",
        "\n",
        "  def get_inputs_from_targets(self, targets, start_token_ids):\n",
        "    \"\"\"Converts target ids to input ids, i.e. adds <s> and removes last.\"\"\"\n",
        "    length = tf.math.count_nonzero(targets, axis=1, dtype=tf.int32)\n",
        "    # Add start token ids.\n",
        "    inputs = tf.concat([tf.expand_dims(start_token_ids, axis=1), targets], 1)\n",
        "    # Remove </s> from the input.\n",
        "    mask = tf.sequence_mask(length, self.params[\"max_decoder_length\"]+1,\n",
        "                            dtype=tf.int32)\n",
        "    inputs = (mask * inputs)[:, :-1]\n",
        "    return inputs\n",
        "\n",
        "  def _decode(self, target_ids, target_mask, start_token_ids,\n",
        "              encoder_output, encoder_mask, training=None):\n",
        "    \"\"\"Compute likelihood of target tokens under the model.\n",
        "    Args:\n",
        "      target_ids: tensor with shape [batch_size, target_length, hidden_size]\n",
        "      target_mask: self-attention bias for decoder attention layer. [batch_size,\n",
        "        input_length]\n",
        "      start_token_ids: int32 tensor of shape [batch_size] for first decoder\n",
        "        input.\n",
        "      encoder_output: Continuous representation of input sequence. Float tensor\n",
        "        with shape [batch_size, input_length, hidden_size].\n",
        "      encoder_mask: Float tensor with shape [batch_size, input_length].\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      A dict containing the output ids, the output log-probs, the output logits.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare inputs to decoder layers by shifting targets, embedding ids,\n",
        "    # adding positional encoding and applying dropout.\n",
        "    input_ids = self.get_inputs_from_targets(target_ids, start_token_ids)\n",
        "\n",
        "    input_embs = self.embeder(input_ids, self.params[\"max_decoder_length\"],\n",
        "                              training=training)\n",
        "\n",
        "    outputs = self.decoder(input_embs, target_mask,\n",
        "                           encoder_output, encoder_mask, training=training)\n",
        "\n",
        "    logits = self.embeder.linear(outputs)\n",
        "    output_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
        "\n",
        "    log_probs = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=target_ids, logits=logits)\n",
        "    log_probs = tf.where(target_ids > 0, log_probs,\n",
        "                         tf.zeros_like(log_probs, tf.float32))\n",
        "\n",
        "    return (tf.identity(log_probs, name=\"log_probs\"),\n",
        "            tf.identity(logits, name=\"logits\"),\n",
        "            tf.cast(output_ids, tf.int32, name=\"pred_ids\"),)\n",
        "\n",
        "  def _init_cache(self, batch_size):\n",
        "    \"\"\"Initialize cache for decoding.\"\"\"\n",
        "\n",
        "    max_decode_len = self.params[\"max_decoder_length\"]\n",
        "    num_heads = self.params[\"num_attention_heads\"]\n",
        "    head_size = int(self.params[\"hidden_size\"] / num_heads)\n",
        "\n",
        "    cache = {}\n",
        "    for layer in range(self.params[\"num_hidden_layers\"]):\n",
        "      cache[\"layer_%d\" % layer] = {\n",
        "          \"k\": tf.zeros([batch_size, num_heads, max_decode_len, head_size]),\n",
        "          \"v\": tf.zeros([batch_size, num_heads, max_decode_len, head_size]),\n",
        "      }\n",
        "    return cache\n",
        "\n",
        "  def _get_symbols_to_logits_fn(self, decoder_self_attention_mask):\n",
        "    \"\"\"Returns a decoding function that calculates logits of the next tokens.\"\"\"\n",
        "\n",
        "    max_decode_len = self.params[\"max_decoder_length\"]\n",
        "\n",
        "    def _symbols_to_logits_fn(target_ids, cache, i):\n",
        "      \"\"\"Generate logits for next candidate IDs.\n",
        "      Args:\n",
        "        target_ids: Current decoded sequences. int tensor with shape\n",
        "          [batch_size, i + 1]\n",
        "        cache: dictionary of values storing the encoder output, encoder-decoder\n",
        "          attention bias, and previous decoder attention values.\n",
        "        i: Loop index\n",
        "      Returns:\n",
        "        Tuple of\n",
        "          (logits with shape [batch_size * beam_size, vocab_size],\n",
        "           updated cache values)\n",
        "      \"\"\"\n",
        "      decoder_input = tf.slice(target_ids,\n",
        "                               [0, tf.maximum(tf.cast(0, i.dtype), i - 1)],\n",
        "                               [target_ids.shape[0], 1])\n",
        "      self_attention_mask = tf.slice(decoder_self_attention_mask, [0, 0, i, 0],\n",
        "                                     [1, 1, 1, max_decode_len])\n",
        "\n",
        "      # Preprocess decoder input by getting embeddings and adding timing signal.\n",
        "      decoder_input = self.embeder(\n",
        "          decoder_input, 1, start_pos=i, training=False)\n",
        "\n",
        "      decoder_output = self.decoder(\n",
        "          decoder_input, self_attention_mask,\n",
        "          cache.get(\"encoder_output\"), cache.get(\"encoder_mask\"),\n",
        "          cache=cache, decode_i=i, training=False)\n",
        "\n",
        "      logits = self.embeder.linear(decoder_output)\n",
        "      logits = tf.squeeze(logits, axis=[1])\n",
        "\n",
        "      return logits\n",
        "\n",
        "    return _symbols_to_logits_fn\n",
        "\n",
        "  def _predict(self, target_ids, target_mask, start_token_ids,\n",
        "               encoder_output, encoder_mask):\n",
        "    \"\"\"Beam decode output tokens and probabilities.\n",
        "    Args:\n",
        "      target_ids: tensor with shape [batch_size, target_length, hidden_size]\n",
        "      target_mask: self-attention bias for decoder attention layer. [batch_size,\n",
        "        input_length]\n",
        "      start_token_ids: int32 tensor of shape [batch_size] for first decoder\n",
        "        input.\n",
        "      encoder_output: Continuous representation of input sequence. Float\n",
        "        tensor with shape [batch_size, target_length, num_hidden_layers,\n",
        "        hidden_size]\n",
        "      encoder_mask: bias for encoder-decoder attention layer. [batch_size,\n",
        "        input_length]\n",
        "    Returns:\n",
        "      A tuple of:\n",
        "        `log_probs`: Log-probs of output tokens.\n",
        "        `logits`: Logits of output tokens.\n",
        "        `pred_ids`: Predicted output sequence.\n",
        "    \"\"\"\n",
        "    batch_size = utils.get_shape_list(start_token_ids)[0]\n",
        "    end_token_id = 1\n",
        "\n",
        "    # One step logit function.\n",
        "    symbols_to_logits_fn = self._get_symbols_to_logits_fn(target_mask)\n",
        "\n",
        "    # Create cache storing decoder attention values for each layer.\n",
        "    cache = self._init_cache(batch_size)\n",
        "\n",
        "    if encoder_output is not None:\n",
        "      # Add encoder output and attention bias to the cache.\n",
        "      cache[\"encoder_output\"] = encoder_output\n",
        "      cache[\"encoder_mask\"] = encoder_mask\n",
        "\n",
        "    decoded_ids = decoder.left2right_decode(\n",
        "        symbols_to_logits_fn,\n",
        "        start_token_ids,\n",
        "        cache,\n",
        "        batch_size,\n",
        "        self.params[\"max_decoder_length\"],\n",
        "        vocab_size=self.params[\"vocab_size\"],\n",
        "        beam_size=self.params[\"beam_size\"],\n",
        "        beam_start=5,\n",
        "        beam_alpha=self.params[\"alpha\"],\n",
        "        beam_min=0,\n",
        "        beam_max=-1,\n",
        "        eos_id=end_token_id)\n",
        "\n",
        "    # Get the top sequence for each batch element\n",
        "    output_ids = tf.cast(decoded_ids, tf.int32, name=\"pred_ids\")\n",
        "\n",
        "    # Calculate log probs for given sequence if available.\n",
        "    calc_ids = output_ids if target_ids is None else target_ids\n",
        "    output_log_probs, output_logits, _ = self._decode(\n",
        "        calc_ids, target_mask, start_token_ids,\n",
        "        encoder_output, encoder_mask, training=False)\n",
        "\n",
        "    return (output_log_probs, output_logits, output_ids)\n",
        "\n",
        "  def _decode_and_predict(self, target_ids, encoder_output, encoder_mask,\n",
        "                          training=None):\n",
        "    \"\"\"Decodes a sequence given the input and the encoder.\n",
        "    Args:\n",
        "      target_ids: tensor with shape [batch_size, target_length, hidden_size]\n",
        "      encoder_output: Continuous representation of input sequence. Float\n",
        "        tensor with shape [batch_size, target_length, num_hidden_layers,\n",
        "        hidden_size]\n",
        "      encoder_mask: bias for encoder-decoder attention layer. [batch_size,\n",
        "        input_length]\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      A tuple of:\n",
        "        `log_probs`: Log-probs of output tokens.\n",
        "        `logits`: Logits of output tokens.\n",
        "        `pred_ids`: Predicted output sequence.\n",
        "    \"\"\"\n",
        "    # Create initial set of IDs that will be passed into symbols_to_logits_fn.\n",
        "    start_token_ids = self._get_start_token_ids(encoder_output)\n",
        "\n",
        "    # Create causal self-attention mask for decoder.\n",
        "    target_mask = decoder.create_self_attention_mask(\n",
        "        self.params[\"max_decoder_length\"])\n",
        "\n",
        "    predictions = {}\n",
        "    if training:\n",
        "      predictions = self._decode(target_ids, target_mask, start_token_ids,\n",
        "                                 encoder_output, encoder_mask, training=True)\n",
        "    else:\n",
        "      predictions = self._predict(target_ids, target_mask, start_token_ids,\n",
        "                                  encoder_output, encoder_mask)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "  def call(self,\n",
        "           input_ids,\n",
        "           target_ids=None,\n",
        "           training=None):\n",
        "    # Run the inputs through the encoder layer to map the symbol\n",
        "    # representations to continuous representations.\n",
        "    encoder_output, encoder_mask = self._encode(input_ids, training=training)\n",
        "\n",
        "    # Decode.\n",
        "    predictions = self._decode_and_predict(target_ids, encoder_output,\n",
        "                                           encoder_mask, training=training)\n",
        "\n",
        "    return predictions, encoder_output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGtuOIPB7Oq"
      },
      "source": [
        "#######################  beam search #######################\n",
        "\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Beam search branched from Pegasus.\n",
        "Original source:\n",
        "https://github.com/google-research/pegasus/blob/master/pegasus/layers/beam_search.py\n",
        "This beam search implementation is designed for TPU usage only and prefers\n",
        "flexibility over efficiency. Transformer attention caching is not enabled yet.\n",
        "Mostly follows implementation in T2T. Several difference to pure beamsearch:\n",
        "1. has finished and alive seqs, use 2 * beam_size to grow alive seqs,\n",
        "   which makes beam_size=1 doesn't equal greedy.\n",
        "2. prefers finished seq over alive seqs.\n",
        "3. prefers lower indices when equal probability (though unlikely).\n",
        "4. with custom length normalization and constraint.\n",
        "Notations:\n",
        "  B: batch_size, M: beam_size, T: max_decode_len, V: vocab_size, U: undefined\n",
        "\"\"\"\n",
        "# pylint: disable=invalid-name\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "def length_normalization(start, alpha, min_len, max_len, out_of_range_penalty):\n",
        "  r\"\"\"Create length normalization function.\n",
        "  Combines length penalty from https://arxiv.org/abs/1609.08144,\n",
        "  and length constraint from https://www.aclweb.org/anthology/W18-2706.pdf.\n",
        "  scores = \\sum_j log(P_j) / ((start + lengths)/(1 + start))**alpha\n",
        "          + out_of_range_penalty * (length > max_len or length < min_len)\n",
        "  Args:\n",
        "    start: int, length normalization start offset.\n",
        "    alpha: float, [0, 1.0],  length normalization power.\n",
        "    min_len: int, minimum decode length.\n",
        "    max_len: int, maximum decode lengths.\n",
        "    out_of_range_penalty: float, penalty for lengths outside min len and max\n",
        "      len. Use a negative number that penalize out of range decodes, does hard\n",
        "      constraint if set to -inf.\n",
        "  Returns:\n",
        "    fn(log_probs_BxM, length)->scores_BxM: a function to normalize sum log\n",
        "    probabilities of sequence with current decoding lengths.\n",
        "  \"\"\"\n",
        "\n",
        "  def length_norm_fn(log_probs_BxM, length_int):\n",
        "    \"\"\"Normalize sum log probabilities given a sequence length.\"\"\"\n",
        "    dtype = log_probs_BxM.dtype\n",
        "    norm_flt = tf.pow(((start + tf.cast(length_int, dtype)) / (1. + start)),\n",
        "                      alpha)\n",
        "    log_probs_BxM /= norm_flt\n",
        "    too_short_bool = tf.less(length_int, min_len)\n",
        "    too_long_bool = tf.logical_and(tf.greater(length_int, max_len), max_len > 0)\n",
        "    out_of_range_bool = tf.logical_or(too_long_bool, too_short_bool)\n",
        "    log_probs_BxM += out_of_range_penalty * tf.cast(out_of_range_bool, dtype)\n",
        "    return log_probs_BxM\n",
        "\n",
        "  return length_norm_fn\n",
        "\n",
        "\n",
        "def beam_search(symbols_to_logits_fn,\n",
        "                init_seq_BxT,\n",
        "                initial_cache_BxU,\n",
        "                vocab_size,\n",
        "                beam_size,\n",
        "                length_norm_fn,\n",
        "                eos_id=1):\n",
        "  \"\"\"Beam search.\n",
        "  Args:\n",
        "    symbols_to_logits_fn: fn(seq_BxT, cache_BxU, i) -> (logits_BxV, cache_BxU)\n",
        "    init_seq_BxT: initial sequence ids.\n",
        "    initial_cache_BxU: dictionary of tensors with shape BxU.\n",
        "    vocab_size: vocabulary size.\n",
        "    beam_size: beam size.\n",
        "    length_norm_fn: length normalization function.\n",
        "    eos_id: end of sequence.\n",
        "  Returns:\n",
        "    Tuple of (beams_BxMxT, scores_BxM). Beam searched sequences and scores.\n",
        "  \"\"\"\n",
        "  B, T = init_seq_BxT.shape\n",
        "  M, V = beam_size, vocab_size\n",
        "  dtype = tf.float32\n",
        "  int_dtype = init_seq_BxT.dtype\n",
        "\n",
        "  def _loop_body(i, alive_seq_BxMxT, alive_log_probs_BxM, alive_cache_BxMxU,\n",
        "                 finished_seq_BxMxT, finished_scores_BxM):\n",
        "    \"\"\"Beam search loop body.\"\"\"\n",
        "    # Decode one step with beam\n",
        "    logits_BMxV, cache_BMxU = symbols_to_logits_fn(\n",
        "        _flatten_beam_dim(alive_seq_BxMxT),\n",
        "        tf.nest.map_structure(_flatten_beam_dim, alive_cache_BxMxU), i)\n",
        "    logits_BxMxV = _unflatten_beam_dim(logits_BMxV, M)\n",
        "    new_cache_BxMxU = tf.nest.map_structure(lambda t: _unflatten_beam_dim(t, M),\n",
        "                                            cache_BMxU)\n",
        "\n",
        "    # select top 2 * beam_size and fill alive and finished.\n",
        "    log_probs_BxMxV = logits_BxMxV - tf.reduce_logsumexp(\n",
        "        logits_BxMxV, axis=2, keepdims=True)\n",
        "    log_probs_BxMxV += tf.expand_dims(alive_log_probs_BxM, axis=2)\n",
        "    log_probs_BxMV = tf.reshape(log_probs_BxMxV, [B, -1])\n",
        "    new_log_probs_Bx2M, topk_indices_Bx2M = tf.nn.top_k(log_probs_BxMV, k=2 * M)\n",
        "    topk_beam_Bx2M = topk_indices_Bx2M // V\n",
        "    topk_seq_Bx2MxT, new_cache_Bx2MxU = _gather_nested(\n",
        "        [alive_seq_BxMxT, new_cache_BxMxU], topk_beam_Bx2M)\n",
        "    topk_ids_Bx2M = topk_indices_Bx2M % V\n",
        "    new_seq_Bx2MxT = _update_i(topk_seq_Bx2MxT, topk_ids_Bx2M, i)\n",
        "    new_finished_flags_Bx2M = tf.cast(\n",
        "        tf.reduce_any(tf.equal(new_seq_Bx2MxT, eos_id), axis=-1), dtype)\n",
        "\n",
        "    # get new alive\n",
        "    _, topk_alive_indices_BxM = tf.nn.top_k(\n",
        "        new_log_probs_Bx2M + new_finished_flags_Bx2M * dtype.min, k=M)\n",
        "    (alive_seq_BxMxT, alive_log_probs_BxM, alive_cache_BxMxU) = _gather_nested(\n",
        "        [new_seq_Bx2MxT, new_log_probs_Bx2M, new_cache_Bx2MxU],\n",
        "        topk_alive_indices_BxM)\n",
        "\n",
        "    # get new finished\n",
        "    new_scores_Bx2M = length_norm_fn(new_log_probs_Bx2M, i + 1)\n",
        "    new_scores_Bx2M += (1 - new_finished_flags_Bx2M) * dtype.min\n",
        "    finished_seq_Bx3MxT = tf.concat([finished_seq_BxMxT, new_seq_Bx2MxT],\n",
        "                                    axis=1)\n",
        "    finished_scores_Bx3M = tf.concat([finished_scores_BxM, new_scores_Bx2M],\n",
        "                                     axis=1)\n",
        "    _, topk_finished_indices_BxM = tf.nn.top_k(finished_scores_Bx3M, k=M)\n",
        "    (finished_seq_BxMxT, finished_scores_BxM) = _gather_nested(\n",
        "        [finished_seq_Bx3MxT, finished_scores_Bx3M], topk_finished_indices_BxM)\n",
        "\n",
        "    return [\n",
        "        i + 1, alive_seq_BxMxT, alive_log_probs_BxM, alive_cache_BxMxU,\n",
        "        finished_seq_BxMxT, finished_scores_BxM\n",
        "    ]\n",
        "\n",
        "  # initialize.\n",
        "  init_i = tf.constant(0, dtype=int_dtype)\n",
        "  init_alive_seq_BxMxT = _expand_to_beam_size(init_seq_BxT, M)\n",
        "  log_probs_1xM = tf.constant([[0.] + [dtype.min] * (M - 1)], dtype=dtype)\n",
        "  init_alive_log_probs_BxM = tf.tile(log_probs_1xM, [B, 1])\n",
        "  init_alive_cache_BxMxU = tf.nest.map_structure(\n",
        "      lambda t: _expand_to_beam_size(t, M), initial_cache_BxU)\n",
        "  init_finished_seq_BxMxT = tf.zeros(tf.shape(init_alive_seq_BxMxT), int_dtype)\n",
        "  init_finished_scores_BxM = tf.zeros([B, M], dtype=dtype) + dtype.min\n",
        "\n",
        "  # run loop.\n",
        "  (_, final_alive_seq_BxMxT, final_alive_scores_BxM, _,\n",
        "   final_finished_seq_BxMxT, final_finished_scores_BxM) = tf.while_loop(\n",
        "       lambda *args: True,  # Always do T iterations\n",
        "       _loop_body,\n",
        "       loop_vars=[\n",
        "           init_i, init_alive_seq_BxMxT, init_alive_log_probs_BxM,\n",
        "           init_alive_cache_BxMxU, init_finished_seq_BxMxT,\n",
        "           init_finished_scores_BxM\n",
        "       ],\n",
        "       parallel_iterations=1,\n",
        "       back_prop=False,\n",
        "       maximum_iterations=T,\n",
        "   )\n",
        "\n",
        "  # process finished.\n",
        "  final_finished_flag_BxMx1 = tf.reduce_any(\n",
        "      tf.equal(final_finished_seq_BxMxT, eos_id), axis=-1, keepdims=True)\n",
        "  final_seq_BxMxT = tf.where(\n",
        "      tf.tile(final_finished_flag_BxMx1, [1, 1, T]), final_finished_seq_BxMxT,\n",
        "      final_alive_seq_BxMxT)\n",
        "  final_scores_BxM = tf.where(\n",
        "      tf.squeeze(final_finished_flag_BxMx1, axis=-1), final_finished_scores_BxM,\n",
        "      final_alive_scores_BxM)\n",
        "  return final_seq_BxMxT, final_scores_BxM\n",
        "\n",
        "\n",
        "def _update_i(tensor_BxNxT, updates_BxN, i):\n",
        "  B, N, T = tensor_BxNxT.shape\n",
        "  tensor_BNxT = tf.reshape(tensor_BxNxT, [-1, T])\n",
        "  updates_BN = tf.reshape(updates_BxN, [-1])\n",
        "  batch_BN = tf.range(B * N, dtype=tf.int32)\n",
        "  i_BN = tf.fill([B * N], i)\n",
        "  ind_BNx2 = tf.stack([batch_BN, i_BN], axis=-1)\n",
        "  tensor_BNxT = tf.tensor_scatter_nd_update(tensor_BNxT, ind_BNx2, updates_BN)\n",
        "  return tf.reshape(tensor_BNxT, [B, N, T])\n",
        "\n",
        "\n",
        "def _expand_to_beam_size(tensor_BxU, beam_size):\n",
        "  tensor_Bx1xU = tf.expand_dims(tensor_BxU, axis=1)\n",
        "  tile_dims = [1] * tensor_Bx1xU.shape.ndims\n",
        "  tile_dims[1] = beam_size\n",
        "  tensor_BxMxU = tf.tile(tensor_Bx1xU, tile_dims)\n",
        "  return tensor_BxMxU\n",
        "\n",
        "\n",
        "def _flatten_beam_dim(tensor_BxMxU):\n",
        "  shape = tensor_BxMxU.shape.as_list()\n",
        "  tensor_BMxU = tf.reshape(tensor_BxMxU, [shape[0] * shape[1]] + shape[2:])\n",
        "  return tensor_BMxU\n",
        "\n",
        "\n",
        "def _unflatten_beam_dim(tensor_BMxU, M):\n",
        "  shape = tensor_BMxU.shape.as_list()\n",
        "  tensor_BxMxU = tf.reshape(tensor_BMxU, [shape[0] // M, M] + shape[1:])\n",
        "  return tensor_BxMxU\n",
        "\n",
        "\n",
        "def _gather_nested(nested_BxMxU, indices_BxN):\n",
        "\n",
        "  def _gather_beam(tensor_BxMxU):\n",
        "    tensor_BxNxU = tf.gather(tensor_BxMxU, indices_BxN, batch_dims=1, axis=1)\n",
        "    return tensor_BxNxU\n",
        "\n",
        "  return tf.nest.map_structure(_gather_beam, nested_BxMxU)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1ObYhS5EJ88"
      },
      "source": [
        "##################### decoder #######################\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"BigBird Decoder Layers.\"\"\"\n",
        "\n",
        "from bigbird.core import attention\n",
        "from bigbird.core import beam_search\n",
        "from bigbird.core import recompute_grad\n",
        "from bigbird.core import utils\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "class PrenormDecoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Decoder layer of a transformer in Pegasus style.\n",
        "  The layer_norm is taken before self-attention.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               hidden_size=768,\n",
        "               intermediate_size=3072,\n",
        "               intermediate_act_fn=utils.gelu,\n",
        "               attention_probs_dropout_prob=0.0,\n",
        "               hidden_dropout_prob=0.1,\n",
        "               initializer_range=0.02,\n",
        "               num_attention_heads=12,\n",
        "               use_bias=True,\n",
        "               name=None):\n",
        "    \"\"\"Constructor of a decoder layer of a transformer in Pegasus style.\n",
        "    Args:\n",
        "      hidden_size: (optional) int. Size of hidden dimension.\n",
        "      intermediate_size: (optional) int. Size of intermediate dimension.\n",
        "      intermediate_act_fn: optional) Activation function for intermediate layer.\n",
        "      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention probabilities.\n",
        "      hidden_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention.\n",
        "      initializer_range: (optional) float. Range of the weight initializer.\n",
        "      num_attention_heads: (optional) int. Number of attention heads.\n",
        "      use_bias: (optional) bool. Whether key/query/value uses a bias vector.\n",
        "      name: The name scope of this layer.\n",
        "    \"\"\"\n",
        "    super(PrenormDecoderLayer, self).__init__(name=name)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "\n",
        "      attention_head_size = hidden_size // num_attention_heads\n",
        "      with tf.compat.v1.variable_scope(\"attention\"):\n",
        "        # Pre-Normalization layer\n",
        "        with tf.compat.v1.variable_scope(\"self\"):\n",
        "          self.first_layer_norm = utils.NormLayer(hidden_size)\n",
        "        # Self-Attention layer\n",
        "        self.self_attn_layer = attention.MultiHeadedAttentionLayer(\n",
        "            \"original_full\", use_bias=use_bias, name=\"self\",\n",
        "            num_attention_heads=num_attention_heads,\n",
        "            size_per_head=attention_head_size,\n",
        "            initializer_range=initializer_range,\n",
        "            attention_probs_dropout_prob=attention_probs_dropout_prob)\n",
        "        # Feedforward layer\n",
        "        with tf.compat.v1.variable_scope(\"output\"):\n",
        "          self.self_proj_layer = utils.Dense3dProjLayer(\n",
        "              num_attention_heads, attention_head_size,\n",
        "              utils.create_initializer(initializer_range), None,\n",
        "              \"dense\", use_bias)\n",
        "        # Dropout\n",
        "        self.self_attn_dropout = recompute_grad.RecomputingDropout(\n",
        "            hidden_dropout_prob)\n",
        "        # Pre-Normalization layer\n",
        "        with tf.compat.v1.variable_scope(\"encdec\"):\n",
        "          self.second_layer_norm = utils.NormLayer(hidden_size)\n",
        "        # Cross-Attention layer\n",
        "        self.cross_attn_layer = attention.MultiHeadedAttentionLayer(\n",
        "            \"original_full\", use_bias=use_bias, name=\"encdec\",\n",
        "            num_attention_heads=num_attention_heads,\n",
        "            size_per_head=attention_head_size,\n",
        "            initializer_range=initializer_range,\n",
        "            attention_probs_dropout_prob=attention_probs_dropout_prob)\n",
        "        # Feedforward layer\n",
        "        with tf.compat.v1.variable_scope(\"encdec_output\"):\n",
        "          self.cross_proj_layer = utils.Dense3dProjLayer(\n",
        "              num_attention_heads, attention_head_size,\n",
        "              utils.create_initializer(initializer_range), None,\n",
        "              \"dense\", use_bias)\n",
        "        # Dropout\n",
        "        self.cross_attn_dropout = recompute_grad.RecomputingDropout(\n",
        "            hidden_dropout_prob)\n",
        "\n",
        "      with tf.compat.v1.variable_scope(\"intermediate\"):\n",
        "        # Normalization layer\n",
        "        self.third_layer_norm = utils.NormLayer(hidden_size)\n",
        "        # Feedforward layer\n",
        "        self.expand_layer = utils.Dense2dLayer(\n",
        "            hidden_size, intermediate_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            intermediate_act_fn, \"dense\")\n",
        "\n",
        "      with tf.compat.v1.variable_scope(\"output\"):\n",
        "        # Feedforward layer\n",
        "        self.contract_layer = utils.Dense2dLayer(\n",
        "            intermediate_size, hidden_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            None, \"dense\")\n",
        "        # Dropout\n",
        "        self.output_dropout = recompute_grad.RecomputingDropout(\n",
        "            hidden_dropout_prob)\n",
        "\n",
        "  def call(self,\n",
        "           layer_input,\n",
        "           encoder_outputs,\n",
        "           self_attention_mask,\n",
        "           attention_mask,\n",
        "           cache=None,\n",
        "           decode_i=None,\n",
        "           training=None):\n",
        "    \"\"\"Implements a decoder layer of a transformer in Pegasus style.\n",
        "    The layer_norm is taken after self-attention.\n",
        "    Args:\n",
        "      layer_input: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "      encoder_outputs: tensors with shape [batch_size, input_length,\n",
        "          num_hidden_layers, hidden_size]\n",
        "      self_attention_mask: bias for decoder self-attention layer. [1, 1,\n",
        "        target_length, target_length]\n",
        "      attention_mask: bias for encoder-decoder attention layer. [batch_size, 1,\n",
        "        1, input_length]\n",
        "      cache: (Used during prediction) A dictionary with tensors containing\n",
        "        results of previous attentions. The dictionary must have the items:\n",
        "            {\"k\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head],\n",
        "             \"v\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head]}\n",
        "      decode_i: (Used during prediction) current location of decoding\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    Raises:\n",
        "      ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "      NotImplementedError: For unknown attention type.\n",
        "    \"\"\"\n",
        "    # self-attention\n",
        "    normalized_layer_input = self.first_layer_norm(layer_input)\n",
        "    self_attention_output = self.self_attn_layer(\n",
        "        normalized_layer_input, normalized_layer_input, [self_attention_mask],\n",
        "        cache=cache, decode_i=decode_i, training=training)\n",
        "\n",
        "    # Run a linear projection of `hidden_size` then add a residual\n",
        "    # with `layer_input`.\n",
        "    self_attention_output = self.self_proj_layer(self_attention_output)\n",
        "    self_attention_output = self.self_attn_dropout(self_attention_output,\n",
        "                                                   training=training)\n",
        "    self_attention_output = self_attention_output + layer_input\n",
        "\n",
        "    # Cross-attention\n",
        "    normalized_self_attention_output = self.second_layer_norm(\n",
        "        self_attention_output)\n",
        "    attention_output = self.cross_attn_layer(\n",
        "        normalized_self_attention_output, encoder_outputs, [attention_mask],\n",
        "        training=training)\n",
        "\n",
        "    # Run a linear projection of `hidden_size` then add a residual\n",
        "    # with `layer_input`.\n",
        "    attention_output = self.cross_proj_layer(attention_output)\n",
        "    attention_output = self.cross_attn_dropout(attention_output,\n",
        "                                               training=training)\n",
        "    attention_output = attention_output + self_attention_output\n",
        "\n",
        "    # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "    normalized_attention_output = self.third_layer_norm(attention_output)\n",
        "    intermediate_output = self.expand_layer(normalized_attention_output)\n",
        "\n",
        "    # Down-project back to `hidden_size` then add the residual.\n",
        "    layer_output = self.contract_layer(intermediate_output)\n",
        "    layer_output = self.output_dropout(layer_output, training=training)\n",
        "    layer_output = layer_output + attention_output\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "class PostnormDecoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Decoder layer of a transformer in BERT style.\n",
        "  The layer_norm is taken before self-attention.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               hidden_size=768,\n",
        "               intermediate_size=3072,\n",
        "               intermediate_act_fn=utils.gelu,\n",
        "               attention_probs_dropout_prob=0.0,\n",
        "               hidden_dropout_prob=0.1,\n",
        "               initializer_range=0.02,\n",
        "               num_attention_heads=12,\n",
        "               use_bias=True,\n",
        "               name=None):\n",
        "    \"\"\"Constructor of a decoder layer of a transformer in BERT style.\n",
        "    Args:\n",
        "      hidden_size: (optional) int. Size of hidden dimension.\n",
        "      intermediate_size: (optional) int. Size of intermediate dimension.\n",
        "      intermediate_act_fn: optional) Activation function for intermediate layer.\n",
        "      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention probabilities.\n",
        "      hidden_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention.\n",
        "      initializer_range: (optional) float. Range of the weight initializer.\n",
        "      num_attention_heads: (optional) int. Number of attention heads.\n",
        "      use_bias: (optional) bool. Whether key/query/value uses a bias vector.\n",
        "      name: The name scope of this layer.\n",
        "    \"\"\"\n",
        "    super(PostnormDecoderLayer, self).__init__(name=name)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "\n",
        "      attention_head_size = hidden_size // num_attention_heads\n",
        "      with tf.compat.v1.variable_scope(\"attention\"):\n",
        "        # Self-Attention layers\n",
        "        self.self_attn_layer = attention.MultiHeadedAttentionLayer(\n",
        "            \"original_full\", use_bias=use_bias, name=\"self\",\n",
        "            num_attention_heads=num_attention_heads,\n",
        "            size_per_head=attention_head_size,\n",
        "            initializer_range=initializer_range,\n",
        "            attention_probs_dropout_prob=attention_probs_dropout_prob)\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"output\"):\n",
        "          # Feedforward layer\n",
        "          self.self_proj_layer = utils.Dense3dProjLayer(\n",
        "              num_attention_heads, attention_head_size,\n",
        "              utils.create_initializer(initializer_range), None,\n",
        "              \"dense\", use_bias)\n",
        "          # Post-Normalization layer\n",
        "          self.first_layer_norm = utils.NormLayer(hidden_size)\n",
        "          # Dropout\n",
        "          self.self_attn_dropout = recompute_grad.RecomputingDropout(\n",
        "              hidden_dropout_prob)\n",
        "\n",
        "        # Cross-Attention layers\n",
        "        self.cross_attn_layer = attention.MultiHeadedAttentionLayer(\n",
        "            \"original_full\", use_bias=use_bias, name=\"encdec\",\n",
        "            num_attention_heads=num_attention_heads,\n",
        "            size_per_head=attention_head_size,\n",
        "            initializer_range=initializer_range,\n",
        "            attention_probs_dropout_prob=attention_probs_dropout_prob)\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"encdec_output\"):\n",
        "          # Feedforward layer\n",
        "          self.cross_proj_layer = utils.Dense3dProjLayer(\n",
        "              num_attention_heads, attention_head_size,\n",
        "              utils.create_initializer(initializer_range), None,\n",
        "              \"dense\", use_bias)\n",
        "          # Post-Normalization layer\n",
        "          self.second_layer_norm = utils.NormLayer(hidden_size)\n",
        "          # Dropout\n",
        "          self.cross_attn_dropout = recompute_grad.RecomputingDropout(\n",
        "              hidden_dropout_prob)\n",
        "\n",
        "      with tf.compat.v1.variable_scope(\"intermediate\"):\n",
        "        # Feedforward layer\n",
        "        self.expand_layer = utils.Dense2dLayer(\n",
        "            hidden_size, intermediate_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            intermediate_act_fn, \"dense\")\n",
        "\n",
        "      with tf.compat.v1.variable_scope(\"output\"):\n",
        "        # Feedforward layer\n",
        "        self.contract_layer = utils.Dense2dLayer(\n",
        "            intermediate_size, hidden_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            None, \"dense\")\n",
        "        # Normalization layer\n",
        "        self.third_layer_norm = utils.NormLayer(hidden_size)\n",
        "        # Dropout\n",
        "        self.output_dropout = recompute_grad.RecomputingDropout(\n",
        "            hidden_dropout_prob)\n",
        "\n",
        "  def call(self,\n",
        "           layer_input,\n",
        "           encoder_outputs,\n",
        "           self_attention_mask,\n",
        "           attention_mask,\n",
        "           cache=None,\n",
        "           decode_i=None,\n",
        "           training=None):\n",
        "    \"\"\"Implements a decoder layer of a transformer in BERT style.\n",
        "    The layer_norm is taken after self-attention.\n",
        "    Args:\n",
        "      layer_input: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "      encoder_outputs: tensors with shape [batch_size, input_length,\n",
        "          num_hidden_layers, hidden_size]\n",
        "      self_attention_mask: bias for decoder self-attention layer. [1, 1,\n",
        "        target_length, target_length]\n",
        "      attention_mask: bias for encoder-decoder attention layer. [batch_size, 1,\n",
        "        1, input_length]\n",
        "      cache: (Used during prediction) A dictionary with tensors containing\n",
        "        results of previous attentions. The dictionary must have the items:\n",
        "            {\"k\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head],\n",
        "             \"v\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head]}\n",
        "      decode_i: (Used during prediction) current location of decoding\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    Raises:\n",
        "      ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "      NotImplementedError: For unknown attention type.\n",
        "    \"\"\"\n",
        "    # self-attention\n",
        "    self_attention_output = self.self_attn_layer(\n",
        "        layer_input, layer_input, [self_attention_mask],\n",
        "        cache=cache, decode_i=decode_i, training=training)\n",
        "\n",
        "    # Run a linear projection of `hidden_size` then add a residual\n",
        "    # with `layer_input`.\n",
        "    self_attention_output = self.self_proj_layer(self_attention_output)\n",
        "    self_attention_output = self.self_attn_dropout(self_attention_output,\n",
        "                                                   training=training)\n",
        "    self_attention_output = self.first_layer_norm(\n",
        "        self_attention_output + layer_input)\n",
        "\n",
        "    # cross-attention\n",
        "    attention_output = self.cross_attn_layer(\n",
        "        self_attention_output, encoder_outputs, [attention_mask],\n",
        "        training=training)\n",
        "\n",
        "    # Run a linear projection of `hidden_size` then add a residual\n",
        "    # with `layer_input`.\n",
        "    attention_output = self.cross_proj_layer(attention_output)\n",
        "    attention_output = self.cross_attn_dropout(attention_output,\n",
        "                                               training=training)\n",
        "    attention_output = self.second_layer_norm(\n",
        "        attention_output + self_attention_output)\n",
        "\n",
        "    # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "    intermediate_output = self.expand_layer(attention_output)\n",
        "\n",
        "    # Down-project back to `hidden_size` then add the residual.\n",
        "    layer_output = self.contract_layer(intermediate_output)\n",
        "    layer_output = self.output_dropout(layer_output, training=training)\n",
        "    layer_output = self.third_layer_norm(layer_output + attention_output)\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "def add_gradient_recomputation(original_class):\n",
        "  \"\"\"Creats a subclass which enables gradient checkpointing.\"\"\"\n",
        "\n",
        "  class RecomputeLayer(original_class):\n",
        "    \"\"\"Transformer layer that recomputes the forward pass during backprop.\"\"\"\n",
        "\n",
        "    def call(self,\n",
        "             layer_input,\n",
        "             encoder_outputs,\n",
        "             self_attention_mask,\n",
        "             attention_mask,\n",
        "             cache=None,\n",
        "             decode_i=None,\n",
        "             training=None):\n",
        "\n",
        "      def f(layer_input, encoder_outputs):\n",
        "        x = super(RecomputeLayer, self).call(\n",
        "            layer_input, encoder_outputs, self_attention_mask, attention_mask,\n",
        "            cache, decode_i, training=training)\n",
        "        return x\n",
        "\n",
        "      f = recompute_grad.recompute_grad(f)\n",
        "\n",
        "      return f(layer_input, encoder_outputs)\n",
        "  return RecomputeLayer\n",
        "\n",
        "\n",
        "class DecoderStack(tf.keras.layers.Layer):\n",
        "  \"\"\"Transformer decoder stack.\"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "    if params[\"couple_encoder_decoder\"]:\n",
        "      name = \"encoder\"\n",
        "      super(DecoderStack, self).__init__(name=name)\n",
        "    else:\n",
        "      name = \"decoder\"\n",
        "      super(DecoderStack, self).__init__(name=name)\n",
        "\n",
        "    self.params = params\n",
        "\n",
        "    if params[\"norm_type\"] == \"prenorm\":\n",
        "      decoder_class = PrenormDecoderLayer\n",
        "    elif params[\"norm_type\"] == \"postnorm\":\n",
        "      decoder_class = PostnormDecoderLayer\n",
        "    else:\n",
        "      raise NotImplementedError(\n",
        "          \"Norm type {} is not implemented\".format(params[\"norm_type\"]))\n",
        "\n",
        "    if params[\"use_gradient_checkpointing\"]:\n",
        "      decoder_class = add_gradient_recomputation(decoder_class)\n",
        "\n",
        "    if self.params.get(\"num_decoder_layers\", None) is not None:\n",
        "      num_hidden_layers = self.params[\"num_decoder_layers\"]\n",
        "    else:\n",
        "      num_hidden_layers = self.params[\"num_hidden_layers\"]\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      # Decoder layers\n",
        "      self.decoder_layers = [\n",
        "          decoder_class(  # pylint: disable=g-complex-comprehension\n",
        "              self.params[\"hidden_size\"],\n",
        "              self.params[\"intermediate_size\"],\n",
        "              utils.get_activation(self.params[\"hidden_act\"]),\n",
        "              self.params[\"attention_probs_dropout_prob\"],\n",
        "              self.params[\"hidden_dropout_prob\"],\n",
        "              self.params[\"initializer_range\"],\n",
        "              self.params[\"num_attention_heads\"],\n",
        "              self.params[\"use_bias\"],\n",
        "              name=\"layer_%d\" % layer_idx)\n",
        "          for layer_idx in range(num_hidden_layers)\n",
        "      ]\n",
        "\n",
        "      # Normalization layer\n",
        "      self.layer_norm = utils.NormLayer(self.params[\"hidden_size\"])\n",
        "\n",
        "  def call(self,\n",
        "           decoder_inputs,\n",
        "           self_attention_mask,\n",
        "           encoder_outputs,\n",
        "           encoder_mask,\n",
        "           cache=None,\n",
        "           decode_i=None,\n",
        "           training=None):\n",
        "    \"\"\"Return the output of the decoder layer stacks.\n",
        "    Args:\n",
        "      decoder_inputs: tensor with shape\n",
        "        [batch_size, target_length, hidden_size]\n",
        "      self_attention_mask: bias for decoder self-attention layer. [1, 1,\n",
        "        target_length, target_length]\n",
        "      encoder_outputs: tensors with shape [batch_size, input_length,\n",
        "        hidden_size]\n",
        "      encoder_mask: bias for encoder-decoder attention layer. [batch_size,\n",
        "        input_length]\n",
        "      cache: (Used during prediction) A dictionary with tensors containing\n",
        "        results of previous attentions. The dictionary must have the items:\n",
        "            {\"k\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head],\n",
        "             \"v\": tensor with shape\n",
        "                  [batch_size, max_len, num_attention_heads, size_per_head]}\n",
        "      decode_i: (Used during prediction) current location of decoding.\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      Output of decoder layer stack. A float32 tensor with shape [batch_size,\n",
        "        target_length, hidden_size]\n",
        "    \"\"\"\n",
        "    # Expand encoder mask to broadcast over num heads and from_seq axis\n",
        "    attention_mask = tf.expand_dims(tf.expand_dims(encoder_mask, 1), 1)\n",
        "    attention_mask = tf.cast(attention_mask, tf.float32)\n",
        "\n",
        "    if self.params[\"norm_type\"] == \"postnorm\":\n",
        "      decoder_inputs = self.layer_norm(decoder_inputs)\n",
        "\n",
        "    layer_output = decoder_inputs\n",
        "    for layer in self.decoder_layers:\n",
        "      layer_cache = cache[layer.name] if cache is not None else None\n",
        "      layer_output = layer(\n",
        "          layer_output, encoder_outputs, self_attention_mask, attention_mask,\n",
        "          layer_cache, decode_i, training=training)\n",
        "\n",
        "    if self.params[\"norm_type\"] == \"prenorm\":\n",
        "      layer_output = self.layer_norm(layer_output)\n",
        "\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "def create_self_attention_mask(length):\n",
        "  with tf.name_scope(\"decoder_self_attention_mask\"):\n",
        "    valid_locs = tf.linalg.band_part(tf.ones([length, length]), -1, 0)\n",
        "    valid_locs = tf.reshape(valid_locs, [1, 1, length, length])\n",
        "  return valid_locs\n",
        "\n",
        "\n",
        "def inplace_update_i(inp_tensor, updates, i):\n",
        "  \"\"\"Inplace update a tensor. B: batch_size, L: tensor length.\"\"\"\n",
        "  batch_size = inp_tensor.shape[0]\n",
        "  indices = tf.stack([\n",
        "      tf.range(batch_size, dtype=tf.int32),\n",
        "      tf.fill([batch_size], tf.cast(i, tf.int32))\n",
        "  ], axis=-1)\n",
        "  return tf.tensor_scatter_nd_update(inp_tensor, indices, updates)\n",
        "\n",
        "\n",
        "# pylint: disable=invalid-name\n",
        "def left2right_decode(symbols_to_logits_fn,\n",
        "                      start_symbols,\n",
        "                      context_BxU_dict,\n",
        "                      batch_size,\n",
        "                      max_decode_len,\n",
        "                      vocab_size,\n",
        "                      beam_size=1,\n",
        "                      beam_start=5,\n",
        "                      beam_alpha=0.6,\n",
        "                      beam_min=0,\n",
        "                      beam_max=-1,\n",
        "                      eos_id=1):\n",
        "  \"\"\"left to right decode.\n",
        "  Notations:\n",
        "    B: batch_size, V: vocab_size, T: decode_len, U: undefined dimensions\n",
        "  Args:\n",
        "    symbols_to_logits_fn: logits = fn(decodes, context, i). Shoud take\n",
        "      [batch_size, decoded_ids] and return [batch_size, vocab_size].\n",
        "    start_symbols: starting ids [batch_size]\n",
        "    context_BxU_dict: dict of Tensors.\n",
        "    batch_size: int, decode batch size.\n",
        "    max_decode_len: int, maximum number of steps to decode.\n",
        "    vocab_size: int, output vocab size.\n",
        "    beam_size: Number of beams to decode.\n",
        "    beam_start: start length for scaling, default to 5.\n",
        "    beam_alpha: Length penalty for decoding. Should be between 0 (shorter) and 1\n",
        "      (longer), default to 0.6.\n",
        "    beam_min: Minimum beam search lengths.\n",
        "    beam_max: Maximum beam search lengths. Set -1 to use unlimited.\n",
        "    eos_id: end of token id, default to 1.\n",
        "  Returns:\n",
        "    decodes: Tensor[batch, decode_len]\n",
        "  \"\"\"\n",
        "  dtype = tf.int32\n",
        "  start_symbols = tf.expand_dims(start_symbols, 1)\n",
        "  # When beam_size=1, beam_search does not behave exactly like greedy.\n",
        "  # This is due to using 2 * beam_size in grow_topk, and keep the top beam_size\n",
        "  # ones that haven't reached EOS into alive.\n",
        "  # In this case, alpha value for length penalty will take effect.\n",
        "  if beam_size == 1:\n",
        "\n",
        "    def decode_loop(i, decodes_BxT, cache_BxU_dict):\n",
        "      logits_BxV = symbols_to_logits_fn(decodes_BxT, cache_BxU_dict, i)\n",
        "      decodes_BxT = inplace_update_i(\n",
        "          decodes_BxT, tf.argmax(logits_BxV, -1, output_type=tf.int32), i)\n",
        "      return i + 1, decodes_BxT, cache_BxU_dict\n",
        "\n",
        "    def loop_cond(i, decodes_BxT, unused_cache_BxU_dict):\n",
        "      finished_B = tf.reduce_any(tf.equal(decodes_BxT, eos_id), axis=1)\n",
        "      return tf.logical_and(i < max_decode_len,\n",
        "                            tf.logical_not(tf.reduce_all(finished_B)))\n",
        "\n",
        "    init_dec_BxT = tf.concat([tf.cast(start_symbols, dtype=dtype),\n",
        "                              tf.zeros([batch_size, max_decode_len-1],\n",
        "                                       dtype=dtype)], axis=1)\n",
        "    _, decodes, _ = tf.while_loop(\n",
        "        loop_cond, decode_loop,\n",
        "        [tf.constant(0, dtype=dtype), init_dec_BxT, context_BxU_dict])\n",
        "    return decodes\n",
        "\n",
        "  else:\n",
        "\n",
        "    def symbols_to_logits_fn_with_sampling(decodes_BxT, states_BxU_dict, i):\n",
        "      logits_BxV = symbols_to_logits_fn(decodes_BxT, states_BxU_dict, i)\n",
        "      return logits_BxV, states_BxU_dict\n",
        "\n",
        "    length_norm_fn = beam_search.length_normalization(beam_start, beam_alpha,\n",
        "                                                      beam_min, beam_max, -1e3)\n",
        "\n",
        "    init_dec_BxT = tf.concat([tf.cast(start_symbols, dtype=tf.int32),\n",
        "                              tf.zeros([batch_size, max_decode_len-1],\n",
        "                                       dtype=tf.int32)], axis=1)\n",
        "\n",
        "    beams, _ = beam_search.beam_search(\n",
        "        symbols_to_logits_fn_with_sampling,\n",
        "        init_dec_BxT,\n",
        "        context_BxU_dict, vocab_size, beam_size, length_norm_fn, eos_id)\n",
        "    return beams[:, 0, :]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o5Urg5FEP3Z"
      },
      "source": [
        "########################### encoder ############################\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"BigBird Encoder Layers.\"\"\"\n",
        "\n",
        "from bigbird.core import attention\n",
        "from bigbird.core import recompute_grad\n",
        "from bigbird.core import utils\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "class PrenormEncoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Encoder layer of a transformer in Pegasus style.\n",
        "  The layer_norm is taken before self-attention.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               attention_type,\n",
        "               hidden_size=768,\n",
        "               intermediate_size=3072,\n",
        "               intermediate_act_fn=utils.gelu,\n",
        "               attention_probs_dropout_prob=0.0,\n",
        "               hidden_dropout_prob=0.1,\n",
        "               initializer_range=0.02,\n",
        "               num_attention_heads=12,\n",
        "               num_rand_blocks=3,\n",
        "               seq_length=1024,\n",
        "               block_size=64,\n",
        "               use_bias=True,\n",
        "               seed=None,\n",
        "               name=None):\n",
        "    \"\"\"Constructor of an encoder layer of a transformer in Pegasus style.\n",
        "    Args:\n",
        "      attention_type: Type of attention, needs to be one of ['original_full',\n",
        "        'simulated_sparse', 'block_sparse'].\n",
        "      hidden_size: (optional) int. Size of hidden dimension.\n",
        "      intermediate_size: (optional) int. Size of intermediate dimension.\n",
        "      intermediate_act_fn: optional) Activation function for intermediate layer.\n",
        "      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention probabilities.\n",
        "      hidden_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention.\n",
        "      initializer_range: (optional) float. Range of the weight initializer.\n",
        "      num_attention_heads: (optional) int. Number of attention heads.\n",
        "      num_rand_blocks: (optional) int. Number of random chunks per row.\n",
        "      seq_length: (optional) int. length of sequence.\n",
        "      block_size: (optional) int. size of block in sequence.\n",
        "      use_bias: (optional) bool. Whether key/query/value uses a bias vector.\n",
        "      seed: (Optional) int. Reandom seed for generating random mask.\n",
        "      name: The name scope of this layer.\n",
        "    \"\"\"\n",
        "    super(PrenormEncoderLayer, self).__init__(name=name)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "\n",
        "      attention_head_size = hidden_size // num_attention_heads\n",
        "      with tf.compat.v1.variable_scope(\"attention\"):\n",
        "        # Pre-Normalization layer\n",
        "        with tf.compat.v1.variable_scope(\"self\"):\n",
        "          self.first_layer_norm = utils.NormLayer(hidden_size)\n",
        "        # Self-Attention layer\n",
        "        self.attn_layer = attention.MultiHeadedAttentionLayer(\n",
        "            attention_type, num_attention_heads, attention_head_size,\n",
        "            num_rand_blocks, seq_length, seq_length, block_size, block_size,\n",
        "            attention_probs_dropout_prob, initializer_range, use_bias,\n",
        "            seed, name=\"self\")\n",
        "        # Feedforward layer\n",
        "        with tf.compat.v1.variable_scope(\"output\"):\n",
        "          self.projection_layer = utils.Dense3dProjLayer(\n",
        "              num_attention_heads, attention_head_size,\n",
        "              utils.create_initializer(initializer_range), None,\n",
        "              \"dense\", use_bias)\n",
        "        # Dropout\n",
        "        self.attention_dropout = recompute_grad.RecomputingDropout(\n",
        "            hidden_dropout_prob)\n",
        "\n",
        "      with tf.compat.v1.variable_scope(\"intermediate\"):\n",
        "        # Normalization layer\n",
        "        self.second_layer_norm = utils.NormLayer(hidden_size)\n",
        "        # Feedforward layer\n",
        "        self.expand_layer = utils.Dense2dLayer(\n",
        "            hidden_size, intermediate_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            intermediate_act_fn, \"dense\")\n",
        "      with tf.compat.v1.variable_scope(\"output\"):\n",
        "        # Feedforward layer\n",
        "        self.contract_layer = utils.Dense2dLayer(\n",
        "            intermediate_size, hidden_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            None, \"dense\")\n",
        "        # Dropout\n",
        "        self.output_dropout = recompute_grad.RecomputingDropout(\n",
        "            hidden_dropout_prob)\n",
        "\n",
        "  def call(self,\n",
        "           layer_input,\n",
        "           attention_mask=None,\n",
        "           band_mask=None,\n",
        "           from_mask=None,\n",
        "           to_mask=None,\n",
        "           input_blocked_mask=None,\n",
        "           training=None):\n",
        "    \"\"\"Implements a encoder layer of a transformer in Pegasus style.\n",
        "    Args:\n",
        "      layer_input: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "      attention_mask: (optional) float32 Tensor of shape [batch_size,\n",
        "        seq_length, seq_length]. The values should be 1 or 0. The\n",
        "        attention scores will effectively be set to -infinity for any positions\n",
        "        in the mask that are 0, and will be unchanged for positions that are 1.\n",
        "      band_mask: (optional) float32 Tensor of shape [batch_size, 1,\n",
        "        seq_length//block_size-4, block_size, 3*block_size].\n",
        "        The values should be 1 or 0. The attention scores will effectively be\n",
        "        set to -infinity for any positions in the mask that are 0, and will be\n",
        "        unchanged for positions that are 1.\n",
        "      from_mask: (optional) float32 Tensor of shape [batch_size, 1,\n",
        "        seq_length, 1]. The values should be 1 or 0. The\n",
        "        attention scores will effectively be set to -infinity for any positions\n",
        "        in the mask that are 0, and will be unchanged for positions that are 1.\n",
        "      to_mask: (optional) float32 Tensor of shape [batch_size, 1, 1,\n",
        "        seq_length]. The values should be 1 or 0. The\n",
        "        attention scores will effectively be set to -infinity for any positions\n",
        "        in the mask that are 0, and will be unchanged for positions that are 1.\n",
        "      input_blocked_mask: (optional) float32 Tensor of shape [batch_size,\n",
        "        seq_length//block_size, block_size]. Same as from/to_mask, just\n",
        "        reshaped.\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    Raises:\n",
        "      ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "      NotImplementedError: For unknown attention type.\n",
        "    \"\"\"\n",
        "    # self-attention\n",
        "    normalized_layer_input = self.first_layer_norm(layer_input)\n",
        "    attention_output = self.attn_layer(\n",
        "        normalized_layer_input, normalized_layer_input, [\n",
        "            attention_mask, band_mask, from_mask, to_mask, input_blocked_mask,\n",
        "            input_blocked_mask\n",
        "        ], training=training)\n",
        "\n",
        "    # Run a linear projection of `hidden_size` then add a residual\n",
        "    # with `layer_input`.\n",
        "    attention_output = self.projection_layer(attention_output)\n",
        "    attention_output = self.attention_dropout(attention_output,\n",
        "                                              training=training)\n",
        "    attention_output = attention_output + layer_input\n",
        "\n",
        "    # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "    normalized_attention_output = self.second_layer_norm(attention_output)\n",
        "    intermediate_output = self.expand_layer(normalized_attention_output)\n",
        "\n",
        "    # Down-project back to `hidden_size` then add the residual.\n",
        "    layer_output = self.contract_layer(intermediate_output)\n",
        "    layer_output = self.output_dropout(layer_output, training=training)\n",
        "    layer_output = layer_output + attention_output\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "class PostnormEncoderLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Encoder layer of a transformer in BERT style.\n",
        "  The layer_norm is taken after self-attention.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               attention_type,\n",
        "               hidden_size=768,\n",
        "               intermediate_size=3072,\n",
        "               intermediate_act_fn=utils.gelu,\n",
        "               attention_probs_dropout_prob=0.0,\n",
        "               hidden_dropout_prob=0.1,\n",
        "               initializer_range=0.02,\n",
        "               num_attention_heads=12,\n",
        "               num_rand_blocks=3,\n",
        "               seq_length=1024,\n",
        "               block_size=64,\n",
        "               use_bias=True,\n",
        "               seed=None,\n",
        "               name=None):\n",
        "    \"\"\"Constructor of an encoder layer of a transformer in BERT style.\n",
        "    Args:\n",
        "      attention_type: Type of attention, needs to be one of ['original_full',\n",
        "        'simulated_sparse', 'block_sparse'].\n",
        "      hidden_size: (optional) int. Size of hidden dimension.\n",
        "      intermediate_size: (optional) int. Size of intermediate dimension.\n",
        "      intermediate_act_fn: optional) Activation function for intermediate layer.\n",
        "      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention probabilities.\n",
        "      hidden_dropout_prob: (optional) float. Dropout probability of the\n",
        "        attention.\n",
        "      initializer_range: (optional) float. Range of the weight initializer.\n",
        "      num_attention_heads: (optional) int. Number of attention heads.\n",
        "      num_rand_blocks: (optional) int. Number of random chunks per row.\n",
        "      seq_length: (optional) int. length of sequence.\n",
        "      block_size: (optional) int. size of block in sequence.\n",
        "      use_bias: (optional) bool. Whether key/query/value uses a bias vector.\n",
        "      seed: (Optional) int. Reandom seed for generating random mask.\n",
        "      name: The name scope of this layer.\n",
        "    \"\"\"\n",
        "    super(PostnormEncoderLayer, self).__init__(name=name)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "\n",
        "      attention_head_size = hidden_size // num_attention_heads\n",
        "      with tf.compat.v1.variable_scope(\"attention\"):\n",
        "        # Self-Attention layer\n",
        "        self.attn_layer = attention.MultiHeadedAttentionLayer(\n",
        "            attention_type, num_attention_heads, attention_head_size,\n",
        "            num_rand_blocks, seq_length, seq_length, block_size, block_size,\n",
        "            attention_probs_dropout_prob, initializer_range, use_bias,\n",
        "            seed, name=\"self\")\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"output\"):\n",
        "          # Feedforward layer\n",
        "          self.projection_layer = utils.Dense3dProjLayer(\n",
        "              num_attention_heads, attention_head_size,\n",
        "              utils.create_initializer(initializer_range), None,\n",
        "              \"dense\", use_bias)\n",
        "          # Post-Normalization layer\n",
        "          self.first_layer_norm = utils.NormLayer(hidden_size)\n",
        "          # Dropout\n",
        "          self.attention_dropout = recompute_grad.RecomputingDropout(\n",
        "              hidden_dropout_prob)\n",
        "\n",
        "      with tf.compat.v1.variable_scope(\"intermediate\"):\n",
        "        # Feedforward layer\n",
        "        self.expand_layer = utils.Dense2dLayer(\n",
        "            hidden_size, intermediate_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            intermediate_act_fn, \"dense\")\n",
        "\n",
        "      with tf.compat.v1.variable_scope(\"output\"):\n",
        "        # Feedforward layer\n",
        "        self.contract_layer = utils.Dense2dLayer(\n",
        "            intermediate_size, hidden_size,\n",
        "            utils.create_initializer(initializer_range),\n",
        "            None, \"dense\")\n",
        "        # Normalization layer\n",
        "        self.second_layer_norm = utils.NormLayer(hidden_size)\n",
        "        # Dropout\n",
        "        self.output_dropout = recompute_grad.RecomputingDropout(\n",
        "            hidden_dropout_prob)\n",
        "\n",
        "  def call(self,\n",
        "           layer_input,\n",
        "           attention_mask=None,\n",
        "           band_mask=None,\n",
        "           from_mask=None,\n",
        "           to_mask=None,\n",
        "           input_blocked_mask=None,\n",
        "           training=None):\n",
        "    \"\"\"Implements a encoder layer of a transformer in BERT style.\n",
        "    Args:\n",
        "      layer_input: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "      attention_mask: (optional) float32 Tensor of shape [batch_size,\n",
        "        seq_length, seq_length]. The values should be 1 or 0. The\n",
        "        attention scores will effectively be set to -infinity for any positions\n",
        "        in the mask that are 0, and will be unchanged for positions that are 1.\n",
        "      band_mask: (optional) float32 Tensor of shape [batch_size, 1,\n",
        "        seq_length//block_size-4, block_size, 3*block_size].\n",
        "        The values should be 1 or 0. The attention scores will effectively be\n",
        "        set to -infinity for any positions in the mask that are 0, and will be\n",
        "        unchanged for positions that are 1.\n",
        "      from_mask: (optional) float32 Tensor of shape [batch_size, 1,\n",
        "        seq_length, 1]. The values should be 1 or 0. The\n",
        "        attention scores will effectively be set to -infinity for any positions\n",
        "        in the mask that are 0, and will be unchanged for positions that are 1.\n",
        "      to_mask: (optional) float32 Tensor of shape [batch_size, 1, 1,\n",
        "        seq_length]. The values should be 1 or 0. The\n",
        "        attention scores will effectively be set to -infinity for any positions\n",
        "        in the mask that are 0, and will be unchanged for positions that are 1.\n",
        "      input_blocked_mask: (optional) float32 Tensor of shape [batch_size,\n",
        "        seq_length//block_size, block_size]. Same as from/to_mask, just\n",
        "        reshaped.\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    Raises:\n",
        "      ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "      NotImplementedError: For unknown attention type.\n",
        "    \"\"\"\n",
        "    # self-attention\n",
        "    attention_output = self.attn_layer(\n",
        "        layer_input, layer_input, [\n",
        "            attention_mask, band_mask, from_mask, to_mask, input_blocked_mask,\n",
        "            input_blocked_mask\n",
        "        ], training=training)\n",
        "\n",
        "    # Run a linear projection of `hidden_size` then add a residual\n",
        "    # with `layer_input`.\n",
        "    attention_output = self.projection_layer(attention_output)\n",
        "    attention_output = self.attention_dropout(attention_output,\n",
        "                                              training=training)\n",
        "    attention_output = self.first_layer_norm(attention_output + layer_input)\n",
        "\n",
        "    # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "    intermediate_output = self.expand_layer(attention_output)\n",
        "\n",
        "    # Down-project back to `hidden_size` then add the residual.\n",
        "    layer_output = self.contract_layer(intermediate_output)\n",
        "    layer_output = self.output_dropout(layer_output, training=training)\n",
        "    layer_output = self.second_layer_norm(layer_output + attention_output)\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "def add_gradient_recomputation(original_class):\n",
        "  \"\"\"Creats a subclass which enables gradient checkpointing.\"\"\"\n",
        "\n",
        "  class RecomputeLayer(original_class):\n",
        "    \"\"\"Transformer layer that recomputes the forward pass during backprop.\"\"\"\n",
        "\n",
        "    def call(self,\n",
        "             layer_input,\n",
        "             attention_mask=None,\n",
        "             band_mask=None,\n",
        "             from_mask=None,\n",
        "             to_mask=None,\n",
        "             input_blocked_mask=None,\n",
        "             training=None):\n",
        "      def f(layer_input, attention_mask, band_mask,\n",
        "            from_mask, to_mask, input_blocked_mask):\n",
        "        x = super(RecomputeLayer, self).call(\n",
        "            layer_input, attention_mask, band_mask, from_mask, to_mask,\n",
        "            input_blocked_mask, training=training)\n",
        "        return x\n",
        "\n",
        "      f = recompute_grad.recompute_grad(f)\n",
        "\n",
        "      return f(layer_input, attention_mask, band_mask,\n",
        "               from_mask, to_mask, input_blocked_mask)\n",
        "  return RecomputeLayer\n",
        "\n",
        "\n",
        "class EncoderStack(tf.keras.layers.Layer):\n",
        "  \"\"\"Transformer encoder stack.\"\"\"\n",
        "\n",
        "  def __init__(self, params):\n",
        "    name = \"encoder\"\n",
        "    super(EncoderStack, self).__init__(name=name)\n",
        "    self.params = params\n",
        "\n",
        "    if params[\"norm_type\"] == \"prenorm\":\n",
        "      encoder_class = PrenormEncoderLayer\n",
        "    elif params[\"norm_type\"] == \"postnorm\":\n",
        "      encoder_class = PostnormEncoderLayer\n",
        "    else:\n",
        "      raise NotImplementedError(\n",
        "          \"Norm type {} is not implemented\".format(params[\"norm_type\"]))\n",
        "\n",
        "    if params[\"use_gradient_checkpointing\"]:\n",
        "      encoder_class = add_gradient_recomputation(encoder_class)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      # Encoder layers\n",
        "      self.encoder_layers = [\n",
        "          encoder_class(  # pylint: disable=g-complex-comprehension\n",
        "              self.params[\"attention_type\"],\n",
        "              self.params[\"hidden_size\"],\n",
        "              self.params[\"intermediate_size\"],\n",
        "              utils.get_activation(self.params[\"hidden_act\"]),\n",
        "              self.params[\"attention_probs_dropout_prob\"],\n",
        "              self.params[\"hidden_dropout_prob\"],\n",
        "              self.params[\"initializer_range\"],\n",
        "              self.params[\"num_attention_heads\"],\n",
        "              self.params[\"num_rand_blocks\"],\n",
        "              self.params[\"max_encoder_length\"],\n",
        "              self.params[\"block_size\"],\n",
        "              self.params[\"use_bias\"],\n",
        "              seed=layer_idx,\n",
        "              name=\"layer_%d\" % layer_idx)\n",
        "          for layer_idx in range(self.params[\"num_hidden_layers\"])\n",
        "      ]\n",
        "\n",
        "      # Normalization layer\n",
        "      self.layer_norm = utils.NormLayer(self.params[\"hidden_size\"])\n",
        "\n",
        "  def call(self,\n",
        "           encoder_inputs,\n",
        "           encoder_inputs_mask,\n",
        "           training=None):\n",
        "    \"\"\"Return the output of the decoder layer stacks.\n",
        "    Args:\n",
        "      encoder_inputs: tensor with shape\n",
        "        [batch_size, input_length, hidden_size]\n",
        "      encoder_inputs_mask: Mask for enccoder input. [batch_size, input_length]\n",
        "      training: Boolean indicating whether the call is training or inference.\n",
        "    Returns:\n",
        "      Finaly layer encoder output. float tensor with shape\n",
        "        [batch_size, input_length, hidden_size]\n",
        "    \"\"\"\n",
        "    if self.params[\"attention_type\"] == \"block_sparse\":\n",
        "      # reshape and cast for blocking\n",
        "      encoder_length = self.params[\"max_encoder_length\"]\n",
        "      encoder_block_size = self.params[\"block_size\"]\n",
        "      encoder_inputs_mask = tf.cast(encoder_inputs_mask, tf.float32)\n",
        "      blocked_encoder_mask = tf.reshape(\n",
        "          encoder_inputs_mask,\n",
        "          (-1, encoder_length//encoder_block_size, encoder_block_size))\n",
        "      encoder_from_mask = tf.reshape(encoder_inputs_mask,\n",
        "                                     (-1, 1, encoder_length, 1))\n",
        "      encoder_to_mask = tf.reshape(encoder_inputs_mask,\n",
        "                                   (-1, 1, 1, encoder_length))\n",
        "\n",
        "      # create band padding\n",
        "      band_mask = attention.create_band_mask_from_inputs(\n",
        "          blocked_encoder_mask, blocked_encoder_mask)\n",
        "\n",
        "      # For unused masks 0 instead of None for compatilibity with recompute_grad\n",
        "      attention_mask = 0.0\n",
        "\n",
        "    else:\n",
        "      # For unused masks 0 instead of None for compatilibity with recompute_grad\n",
        "      blocked_encoder_mask = 0.0\n",
        "      encoder_to_mask = 0.0\n",
        "      encoder_from_mask = 0.0\n",
        "      band_mask = 0.0\n",
        "\n",
        "      encoder_inputs_mask = tf.cast(encoder_inputs_mask, tf.float32)\n",
        "      attention_mask = attention.create_attention_mask_from_input_mask(\n",
        "          encoder_inputs_mask, encoder_inputs_mask)\n",
        "\n",
        "    if self.params[\"norm_type\"] == \"postnorm\":\n",
        "      encoder_inputs = self.layer_norm(encoder_inputs)\n",
        "\n",
        "    layer_output = encoder_inputs\n",
        "    for layer in self.encoder_layers:\n",
        "      layer_output = layer(\n",
        "          layer_output, attention_mask, band_mask,\n",
        "          encoder_from_mask, encoder_to_mask, blocked_encoder_mask,\n",
        "          training=training)\n",
        "\n",
        "    if self.params[\"norm_type\"] == \"prenorm\":\n",
        "      layer_output = self.layer_norm(layer_output)\n",
        "\n",
        "    return layer_output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHSS6zsEV6T"
      },
      "source": [
        "######################  optimization ####################\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Functions and classes related to optimization (weight updates).\"\"\"\n",
        "\n",
        "import re\n",
        "\n",
        "from absl import logging\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "# pylint: disable=g-direct-tensorflow-import\n",
        "from tensorflow.python.ops import resource_variable_ops\n",
        "\n",
        "\n",
        "def get_linear_warmup_linear_decay_lr(init_lr, num_train_steps,\n",
        "                                      num_warmup_steps):\n",
        "  \"\"\"Calculate learning rate with linear warmup and linear decay.\"\"\"\n",
        "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.compat.v1.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_step, tf.float32)\n",
        "    warmup_steps_float = tf.cast(num_warmup_steps, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  return learning_rate\n",
        "\n",
        "\n",
        "def get_linear_warmup_rsqrt_decay_lr(init_lr, hidden_size,\n",
        "                                     num_warmup_steps):\n",
        "  \"\"\"Calculate learning rate with linear warmup and rsqrt decay.\"\"\"\n",
        "  num_warmup_steps = tf.cast(num_warmup_steps, tf.float32)\n",
        "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "  global_step = tf.cast(global_step, tf.float32)\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "  learning_rate *= tf.math.rsqrt(tf.cast(hidden_size, tf.float32))\n",
        "  # Apply linear warmup\n",
        "  learning_rate *= tf.minimum(1.0, global_step / num_warmup_steps)\n",
        "  # Apply rsqrt decay\n",
        "  learning_rate *= tf.math.rsqrt(tf.maximum(global_step, num_warmup_steps))\n",
        "\n",
        "  return learning_rate\n",
        "\n",
        "\n",
        "def get_optimizer(params, learning_rate):\n",
        "  \"\"\"Gets the optimzer based on the hparams and current mode (TPU vs. CPU/GPU).\n",
        "  Args:\n",
        "      params: A dictionary containing training hyperparameters.\n",
        "      learning_rate: A float32 scalar.\n",
        "  Returns:\n",
        "    A string or an optimizer instance.\n",
        "  \"\"\"\n",
        "  optimizer = None\n",
        "\n",
        "  if params[\"optimizer\"] == \"Adafactor\":\n",
        "    try:\n",
        "      from tensor2tensor.utils import adafactor  # pylint: disable=g-import-not-at-top\n",
        "      optimizer = adafactor.AdafactorOptimizer(learning_rate=learning_rate)\n",
        "    except ImportError:\n",
        "      logging.error(\"tensor2tensor not installed. Cannot use Adafactor.\"\n",
        "                    \"Defaulting to Adam.\")\n",
        "      params[\"optimizer\"] = \"Adam\"\n",
        "\n",
        "  if params[\"optimizer\"] == \"Adam\":\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(\n",
        "        learning_rate,\n",
        "        beta1=params[\"optimizer_beta1\"],\n",
        "        beta2=params[\"optimizer_beta2\"],\n",
        "        epsilon=params[\"optimizer_epsilon\"])\n",
        "\n",
        "  if params[\"optimizer\"] == \"AdamWeightDecay\":\n",
        "    optimizer = AdamWeightDecayOptimizer(\n",
        "        learning_rate,\n",
        "        weight_decay_rate=params[\"weight_decay_rate\"],\n",
        "        beta_1=params[\"optimizer_beta1\"],\n",
        "        beta_2=params[\"optimizer_beta2\"],\n",
        "        epsilon=params[\"optimizer_epsilon\"],\n",
        "        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if params[\"optimizer\"] == \"SGD\":\n",
        "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
        "\n",
        "  if optimizer is None:\n",
        "    raise ValueError(\"Unknown optimizer: {}.\".format(params[\"optimizer\"]))\n",
        "\n",
        "  if params[\"use_tpu\"]:\n",
        "    # Average the gradients across TPU cores.\n",
        "    optimizer = tf.compat.v1.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  return optimizer\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.compat.v1.train.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "\n",
        "  def _create_slots(self, var_list):\n",
        "    # Create slots for the first and second moments.\n",
        "    for v in var_list:\n",
        "      self._zeros_slot(v, \"m\", self._name)\n",
        "      self._zeros_slot(v, \"v\", self._name)\n",
        "\n",
        "  def _apply_dense(self, grad, var):\n",
        "    param_name = self._get_variable_name(var.name)\n",
        "    m = self.get_slot(var, \"m\")\n",
        "    v = self.get_slot(var, \"v\")\n",
        "\n",
        "    # Standard Adam update.\n",
        "    next_m = (\n",
        "        tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "    next_v = (\n",
        "        tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                  tf.square(grad)))\n",
        "\n",
        "    update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "    # Just adding the square of the weights to the loss function is *not*\n",
        "    # the correct way of using L2 regularization/weight decay with Adam,\n",
        "    # since that will interact with the m and v parameters in strange ways.\n",
        "    #\n",
        "    # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "    # with the m/v parameters. This is equivalent to adding the square\n",
        "    # of the weights to the loss with plain (non-momentum) SGD.\n",
        "    if self._do_use_weight_decay(param_name):\n",
        "      update += self.weight_decay_rate * var\n",
        "\n",
        "    update_with_lr = self.learning_rate * update\n",
        "\n",
        "    next_param = var - update_with_lr\n",
        "\n",
        "    return tf.group(\n",
        "        [var.assign(next_param),\n",
        "         m.assign(next_m),\n",
        "         v.assign(next_v)])\n",
        "\n",
        "  def _resource_apply_dense(self, grad, var):\n",
        "    \"\"\"See `tf.train.Optimizer._resource_apply_dense()`.\"\"\"\n",
        "    return self._apply_dense(grad, var)\n",
        "\n",
        "  def _apply_sparse(self, grad, var):\n",
        "    \"\"\"See `tf.train.Optimizer._apply_sparse()`.\"\"\"\n",
        "    def scatter_update_fn(x, i, v):\n",
        "      return tf.compat.v1.scatter_update(x, i, v, use_locking=self._use_locking)\n",
        "    return self._apply_sparse_shared(\n",
        "        grad.values, grad.indices, var, scatter_update_fn)\n",
        "\n",
        "  def _resource_apply_sparse(self, grad, var, indices):\n",
        "    \"\"\"See `tf.train.Optimizer._resource_apply_spase()`.\"\"\"\n",
        "    def scatter_update_fn(x, i, v):\n",
        "      with tf.control_dependencies(\n",
        "          [resource_variable_ops.resource_scatter_update(x.handle, i, v)]):\n",
        "        return x.value()\n",
        "    return self._apply_sparse_shared(grad, indices, var, scatter_update_fn)\n",
        "\n",
        "  def _apply_sparse_shared(self, grad, indices, var, scatter_update_fn):\n",
        "    \"\"\"Applies sparse gradients to a variable.\n",
        "    Args:\n",
        "      grad: A tensor for the `values` of `tf.IndexedSlices`.\n",
        "      indices: A tensor for the `indices` of `tf.IndexedSlices`.\n",
        "      var: A `tf.Variable` object.\n",
        "      scatter_update_fn: A function which performs scattered update to\n",
        "        a `tf.Variable` object. It takes tuple of (x, i, v) where:\n",
        "          * x: A `tf.Variable` object which is updated by `i` and `v`,\n",
        "          * i: A tensor for the `indices` of `tf.IndexedSlices`,\n",
        "          * v: A tensor for the `values` of `tf.IndexedSlices`,\n",
        "        and returns a tensor after updating `x`.\n",
        "    Returns:\n",
        "      An op which updates `var` with `grad` and `indices`.\n",
        "    \"\"\"\n",
        "    param_name = self._get_variable_name(var.name)\n",
        "    m = self.get_slot(var, \"m\")\n",
        "    v = self.get_slot(var, \"v\")\n",
        "\n",
        "    # m_t = beta1 * m + (1 - beta1) * g_t\n",
        "    m_scaled_g_values = tf.multiply(1.0 - self.beta_1, grad)\n",
        "    m_t = m.assign(m * self.beta_1)\n",
        "    with tf.control_dependencies([m_t]):\n",
        "      m_slice = tf.gather(m, indices) + m_scaled_g_values\n",
        "      m_t = scatter_update_fn(m, indices, m_slice)\n",
        "\n",
        "    # v_t = beta2 * v + (1 - beta2) * g_t^2\n",
        "    v_scaled_g_values = tf.multiply(1.0 - self.beta_2, tf.square(grad))\n",
        "    v_t = v.assign(v * self.beta_2)\n",
        "    with tf.control_dependencies([v_t]):\n",
        "      v_slice = tf.gather(v, indices) + v_scaled_g_values\n",
        "      v_t = scatter_update_fn(v, indices, v_slice)\n",
        "\n",
        "    update = m_t / (tf.sqrt(v_t) + self.epsilon)\n",
        "\n",
        "    # Just adding the square of the weights to the loss function is *not*\n",
        "    # the correct way of using L2 regularization/weight decay with Adam,\n",
        "    # since that will interact with the m and v parameters in strange ways.\n",
        "    #\n",
        "    # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "    # with the m/v parameters. This is equivalent to adding the square\n",
        "    # of the weights to the loss with plain (non-momentum) SGD.\n",
        "    if self._do_use_weight_decay(param_name):\n",
        "      update += self.weight_decay_rate * var\n",
        "\n",
        "    update_with_lr = self.learning_rate * update\n",
        "\n",
        "    next_param = var - update_with_lr\n",
        "\n",
        "    return tf.group([var.assign(next_param), m_t, v_t])\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFIoXy4FFVX6"
      },
      "source": [
        "############################ utils #############################\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Helper and utility functions.\"\"\"\n",
        "\n",
        "import re\n",
        "\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "############################### SHAPE UTILS ####################################\n",
        "\n",
        "\n",
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "  Args:\n",
        "    tensor: A tf.Tensor object to find the shape of.\n",
        "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "      specified and the `tensor` has a different rank, and exception will be\n",
        "      thrown.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "  Returns:\n",
        "    A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    be returned as python integers, and dynamic dimensions will be returned\n",
        "    as tf.Tensor scalars.\n",
        "  \"\"\"\n",
        "  if not tf.executing_eagerly() and name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  # assert False, \"Static shape not available for {}\".format(tensor)\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name=None):\n",
        "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
        "  Args:\n",
        "    tensor: A tf.Tensor to check the rank of.\n",
        "    expected_rank: Python integer or list of integers, expected rank.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "  Raises:\n",
        "    ValueError: If the expected shape doesn't match the actual shape.\n",
        "  \"\"\"\n",
        "  if not tf.executing_eagerly() and name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, int):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.compat.v1.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `{}` in scope `{}`, the actual rank \"\n",
        "        \"`{}` (shape = {}) is not equal to the expected rank `{}`\".format(\n",
        "            name, scope_name, actual_rank, str(tensor.shape),\n",
        "            str(expected_rank)))\n",
        "\n",
        "\n",
        "############################### DENSE LAYERS ###################################\n",
        "\n",
        "\n",
        "def create_initializer(initializer_range=0.02):\n",
        "  \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
        "  return tf.compat.v1.truncated_normal_initializer(stddev=initializer_range)\n",
        "\n",
        "\n",
        "class Dense3dLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"A dense layer with 3D kernel.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               num_attention_heads,\n",
        "               size_per_head,\n",
        "               initializer,\n",
        "               activation,\n",
        "               name=None,\n",
        "               head_first=False,\n",
        "               use_bias=True):\n",
        "    \"\"\"Constructor for dense layer with 3D kernel.\n",
        "    Args:\n",
        "      num_attention_heads: The size of output dimension.\n",
        "      size_per_head: The size per attention head.\n",
        "      initializer: Kernel initializer.\n",
        "      activation: Actication function.\n",
        "      name: The name scope of this layer.\n",
        "      head_first: Whether to output head dimension before or after sequence dim.\n",
        "      use_bias: Whether the layer uses a bias vector.\n",
        "    \"\"\"\n",
        "    super(Dense3dLayer, self).__init__(name=name)\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.size_per_head = size_per_head\n",
        "    self.initializer = initializer\n",
        "    self.activation = activation\n",
        "    self.head_first = head_first\n",
        "    self.use_bias = use_bias\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      hidden_size = self.num_attention_heads * self.size_per_head\n",
        "      self.w = tf.compat.v1.get_variable(\n",
        "          name=\"kernel\",\n",
        "          shape=[hidden_size, hidden_size],\n",
        "          initializer=self.initializer)\n",
        "\n",
        "      if self.use_bias:\n",
        "        self.b = tf.compat.v1.get_variable(\n",
        "            name=\"bias\",\n",
        "            shape=[hidden_size],\n",
        "            initializer=tf.zeros_initializer())\n",
        "      else:\n",
        "        self.b = None\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    \"\"\"Constructor for dense layer with 3D kernel.\n",
        "    Args:\n",
        "      input_tensor: float Tensor of shape [batch, seq_length, hidden_size].\n",
        "    Returns:\n",
        "      float logits Tensor.\n",
        "    \"\"\"\n",
        "    hidden_size = self.num_attention_heads * self.size_per_head\n",
        "    reshape_w = tf.reshape(\n",
        "        self.w, [hidden_size, self.num_attention_heads, self.size_per_head])\n",
        "    if self.head_first:\n",
        "      ret = tf.einsum(\"abc,cde->adbe\", input_tensor, reshape_w)\n",
        "    else:\n",
        "      ret = tf.einsum(\"abc,cde->abde\", input_tensor, reshape_w)\n",
        "\n",
        "    if self.use_bias:\n",
        "      if self.head_first:\n",
        "        reshape_b = tf.reshape(\n",
        "            self.b, [1, self.num_attention_heads, 1, self.size_per_head])\n",
        "      else:\n",
        "        reshape_b = tf.reshape(\n",
        "            self.b, [self.num_attention_heads, self.size_per_head])\n",
        "      ret += reshape_b\n",
        "\n",
        "    if self.activation is not None:\n",
        "      return self.activation(ret)\n",
        "    else:\n",
        "      return ret\n",
        "\n",
        "\n",
        "class Dense3dProjLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"A dense layer with 3D kernel for projection.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               num_attention_heads,\n",
        "               size_per_head,\n",
        "               initializer,\n",
        "               activation,\n",
        "               name=None,\n",
        "               use_bias=True):\n",
        "    \"\"\"Constructor for dense layer with 3D kernel for projection.\n",
        "    Args:\n",
        "      num_attention_heads: The size of output dimension.\n",
        "      size_per_head: The size per attention head.\n",
        "      initializer: Kernel initializer.\n",
        "      activation: Actication function.\n",
        "      name: The name scope of this layer.\n",
        "      use_bias: Whether the layer uses a bias vector.\n",
        "    \"\"\"\n",
        "    super(Dense3dProjLayer, self).__init__(name=name)\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.size_per_head = size_per_head\n",
        "    self.initializer = initializer\n",
        "    self.activation = activation\n",
        "    self.use_bias = use_bias\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      hidden_size = self.num_attention_heads * self.size_per_head\n",
        "      self.w = tf.compat.v1.get_variable(\n",
        "          name=\"kernel\",\n",
        "          shape=[hidden_size, hidden_size],\n",
        "          initializer=self.initializer)\n",
        "\n",
        "      if self.use_bias:\n",
        "        self.b = tf.compat.v1.get_variable(\n",
        "            name=\"bias\",\n",
        "            shape=[hidden_size],\n",
        "            initializer=tf.zeros_initializer())\n",
        "      else:\n",
        "        self.b = None\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    \"\"\"Constructor for dense layer with 3D kernel for projection.\n",
        "    Args:\n",
        "      input_tensor: float Tensor of shape [batch,from_seq_length,\n",
        "        num_attention_heads, size_per_head].\n",
        "    Returns:\n",
        "      float logits Tensor.\n",
        "    \"\"\"\n",
        "    hidden_size = self.num_attention_heads * self.size_per_head\n",
        "    reshape_w = tf.reshape(\n",
        "        self.w, [self.num_attention_heads, self.size_per_head, hidden_size])\n",
        "    ret = tf.einsum(\"BFNH,NHD->BFD\", input_tensor, reshape_w)\n",
        "\n",
        "    if self.use_bias:\n",
        "      ret += self.b\n",
        "\n",
        "    if self.activation is not None:\n",
        "      return self.activation(ret)\n",
        "    else:\n",
        "      return ret\n",
        "\n",
        "\n",
        "class Dense2dLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"A dense layer with 2D kernel.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_size,\n",
        "               output_size,\n",
        "               initializer,\n",
        "               activation,\n",
        "               name=None,\n",
        "               use_bias=True):\n",
        "    \"\"\"Constructor for dense layer with 2D kernel.\n",
        "    Args:\n",
        "      input_size: The size of input dimension.\n",
        "      output_size: The size of output dimension.\n",
        "      initializer: Kernel initializer.\n",
        "      activation: Actication function.\n",
        "      name: The name scope of this layer.\n",
        "      use_bias: Whether the layer uses a bias vector.\n",
        "    \"\"\"\n",
        "    super(Dense2dLayer, self).__init__(name=name)\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.initializer = initializer\n",
        "    self.activation = activation\n",
        "    self.use_bias = use_bias\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      self.w = tf.compat.v1.get_variable(\n",
        "          name=\"kernel\",\n",
        "          shape=[self.input_size, self.output_size],\n",
        "          initializer=self.initializer)\n",
        "\n",
        "      if self.use_bias:\n",
        "        self.b = tf.compat.v1.get_variable(\n",
        "            name=\"bias\",\n",
        "            shape=[self.output_size],\n",
        "            initializer=tf.zeros_initializer())\n",
        "      else:\n",
        "        self.b = None\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    \"\"\"Forward pass for dense layer with 2D kernel.\n",
        "    Args:\n",
        "      input_tensor: Float tensor with rank 3.\n",
        "    Returns:\n",
        "      float logits Tensor.\n",
        "    \"\"\"\n",
        "    ret = tf.einsum(\"abc,cd->abd\", input_tensor, self.w)\n",
        "\n",
        "    if self.use_bias:\n",
        "      ret += self.b\n",
        "\n",
        "    if self.activation is not None:\n",
        "      return self.activation(ret)\n",
        "    else:\n",
        "      return ret\n",
        "\n",
        "\n",
        "class SimpleDenseLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"A simple dense layer with 2D kernel.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_size,\n",
        "               output_size,\n",
        "               initializer,\n",
        "               activation,\n",
        "               name=None,\n",
        "               use_bias=True):\n",
        "    \"\"\"Constructor for dense layer with 2D kernel.\n",
        "    Args:\n",
        "      input_size: The size of input dimension.\n",
        "      output_size: The size of output dimension.\n",
        "      initializer: Kernel initializer.\n",
        "      activation: Actication function.\n",
        "      name: The name scope of this layer.\n",
        "      use_bias: Whether the layer uses a bias vector.\n",
        "    \"\"\"\n",
        "    super(SimpleDenseLayer, self).__init__(name=name)\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.initializer = initializer\n",
        "    self.activation = activation\n",
        "    self.use_bias = use_bias\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      self.w = tf.compat.v1.get_variable(\n",
        "          name=\"kernel\",\n",
        "          shape=[self.input_size, self.output_size],\n",
        "          initializer=self.initializer)\n",
        "\n",
        "      if self.use_bias:\n",
        "        self.b = tf.compat.v1.get_variable(\n",
        "            name=\"bias\",\n",
        "            shape=[self.output_size],\n",
        "            initializer=tf.zeros_initializer())\n",
        "      else:\n",
        "        self.b = None\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    \"\"\"Forward pass for dense layer with 2D kernel.\n",
        "    Args:\n",
        "      input_tensor: Float tensor with rank 2.\n",
        "    Returns:\n",
        "      float logits Tensor.\n",
        "    \"\"\"\n",
        "    ret = tf.einsum(\"ab,bc->ac\", input_tensor, self.w)\n",
        "\n",
        "    if self.use_bias:\n",
        "      ret += self.b\n",
        "\n",
        "    if self.activation is not None:\n",
        "      return self.activation(ret)\n",
        "    else:\n",
        "      return ret\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x: float Tensor to perform activation.\n",
        "  Returns:\n",
        "    `x` with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5 * (1.0 + tf.tanh(\n",
        "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
        "  Args:\n",
        "    activation_string: String name of the activation function.\n",
        "  Returns:\n",
        "    A Python function corresponding to the activation function. If\n",
        "    `activation_string` is None, empty, or \"linear\", this will return None.\n",
        "    If `activation_string` is not a string, it will return `activation_string`.\n",
        "  Raises:\n",
        "    ValueError: The `activation_string` does not correspond to a known\n",
        "      activation.\n",
        "  \"\"\"\n",
        "\n",
        "  # We assume that anything that\"s not a string is already an activation\n",
        "  # function, so we just return it.\n",
        "  if not isinstance(activation_string, str):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == \"linear\":\n",
        "    return None\n",
        "  elif act == \"relu\":\n",
        "    return tf.nn.relu\n",
        "  elif act == \"gelu\":\n",
        "    return gelu\n",
        "  elif act == \"tanh\":\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
        "\n",
        "\n",
        "############################## NORM LAYERS #####################################\n",
        "\n",
        "\n",
        "class NormLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Replacement for contrib_layers.layer_norm.\"\"\"\n",
        "\n",
        "  def __init__(self, hdim, dtype=tf.float32, name=\"LayerNorm\"):\n",
        "    super(NormLayer, self).__init__(name=name)\n",
        "    self._dtype = dtype\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      self.beta = tf.compat.v1.get_variable(\n",
        "          \"beta\", [hdim], dtype=dtype, initializer=tf.zeros_initializer())\n",
        "      self.gamma = tf.compat.v1.get_variable(\n",
        "          \"gamma\", [hdim], dtype=dtype, initializer=tf.ones_initializer())\n",
        "\n",
        "  def call(self, inputs):\n",
        "    inputs_shape = inputs.shape\n",
        "\n",
        "    # Compute norm along last axis\n",
        "    mean, variance = tf.nn.moments(inputs, [-1], keepdims=True)\n",
        "    # Compute layer normalization using the batch_normalization function.\n",
        "    # Note that epsilon must be increased for float16 due to the limited\n",
        "    # representable range.\n",
        "    variance_epsilon = 1e-12 if self._dtype != tf.float16 else 1e-3\n",
        "    outputs = tf.nn.batch_normalization(\n",
        "        inputs,\n",
        "        mean,\n",
        "        variance,\n",
        "        offset=self.beta,\n",
        "        scale=self.gamma,\n",
        "        variance_epsilon=variance_epsilon)\n",
        "    outputs.set_shape(inputs_shape)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "############################# EMBEDDING LAYER ##################################\n",
        "\n",
        "\n",
        "class EmbeddingLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"An embedding layer.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               emb_dim,\n",
        "               initializer,\n",
        "               scale_emb=False,\n",
        "               use_token_type=False,\n",
        "               num_token_types=16,\n",
        "               use_position_embeddings=True,\n",
        "               max_position_embeddings=4096,\n",
        "               dropout_prob=0.0,\n",
        "               name=\"embeddings\"):\n",
        "    super(EmbeddingLayer, self).__init__(name=name)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.scale_emb = scale_emb\n",
        "    self.num_token_types = num_token_types\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.dropout_prob = dropout_prob\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "      self.word_embeddings = tf.compat.v1.get_variable(\n",
        "          \"word_embeddings\", [vocab_size, emb_dim],\n",
        "          dtype=tf.float32, initializer=initializer)\n",
        "\n",
        "      if use_token_type:\n",
        "        self.token_type_table = tf.compat.v1.get_variable(\n",
        "            \"token_type_embeddings\", [num_token_types, emb_dim],\n",
        "            dtype=tf.float32, initializer=initializer)\n",
        "      else:\n",
        "        self.token_type_table = None\n",
        "\n",
        "      if use_position_embeddings:\n",
        "        self.position_embeddings = tf.compat.v1.get_variable(\n",
        "            \"position_embeddings\", [max_position_embeddings, emb_dim],\n",
        "            dtype=tf.float32, initializer=initializer)\n",
        "      else:\n",
        "        self.position_embeddings = None\n",
        "\n",
        "  def call(self,\n",
        "           input_ids,\n",
        "           seq_length,\n",
        "           start_pos=0,\n",
        "           token_type_ids=None,\n",
        "           training=None):\n",
        "    if input_ids is None:\n",
        "      return None\n",
        "\n",
        "    # subtoken embedding\n",
        "    output = tf.nn.embedding_lookup(params=self.word_embeddings, ids=input_ids)\n",
        "\n",
        "    if self.scale_emb:\n",
        "      output = output * self.emb_dim ** 0.5\n",
        "\n",
        "    if self.token_type_table is not None:\n",
        "      # This vocab will be small so we always do one-hot here, since it is\n",
        "      # always faster for a small vocabulary.\n",
        "      one_hot_ids = tf.one_hot(token_type_ids, depth=self.num_token_types)\n",
        "      token_type_embeddings = tf.tensordot(\n",
        "          one_hot_ids, self.token_type_table, 1)\n",
        "      output += token_type_embeddings\n",
        "\n",
        "    if self.position_embeddings is not None:\n",
        "      # assert_op = tf.compat.v1.assert_less_equal(\n",
        "      #     start_pos + seq_length, self.max_position_embeddings)\n",
        "      # with tf.control_dependencies([assert_op]):\n",
        "      # So `position_embeddings` is effectively an embedding table for\n",
        "      # position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
        "      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
        "      # perform a slice.\n",
        "      position_embeddings = tf.slice(self.position_embeddings, [start_pos, 0],\n",
        "                                     [seq_length, self.emb_dim])\n",
        "      output += tf.expand_dims(position_embeddings, axis=0)\n",
        "\n",
        "    if training and self.dropout_prob > 0:\n",
        "      output = tf.nn.dropout(output, self.dropout_prob)\n",
        "    return output\n",
        "\n",
        "  def linear(self, x):\n",
        "    \"\"\"Computes logits by running x through a linear layer.\n",
        "    Args:\n",
        "      x: A float32 tensor with shape [..., hidden_size]\n",
        "    Returns:\n",
        "      float32 tensor with shape [..., vocab_size].\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.name_scope(\"presoftmax_linear\"):\n",
        "      logits = tf.tensordot(x, self.word_embeddings, [[-1], [1]])\n",
        "    return logits\n",
        "\n",
        "\n",
        "########################## TPU/CHECKPOINT UTILS ################################\n",
        "\n",
        "\n",
        "def get_estimator(config, model_fn, keep_checkpoint_max=10):\n",
        "  \"\"\"Create TPUEstimator object for given config and model_fn.\"\"\"\n",
        "  tpu_cluster_resolver = None\n",
        "  if config[\"use_tpu\"] and config[\"tpu_name\"]:\n",
        "    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
        "        config[\"tpu_name\"],\n",
        "        zone=config[\"tpu_zone\"],\n",
        "        project=config[\"gcp_project\"])\n",
        "\n",
        "  # Batch size book-keeping\n",
        "  # Estimators handle batch sizes differently among GPUs and TPUs\n",
        "  # GPU: Estimator needs per core batch size\n",
        "  # TPU: Estimator needs total batch size, i.e. num_cores * per core batch size\n",
        "  config_train_batch_size = config[\"train_batch_size\"]     # For estimator\n",
        "  config_eval_batch_size = config[\"eval_batch_size\"]       # For estimator\n",
        "  effective_train_batch_size = config[\"train_batch_size\"]  # For human\n",
        "  effective_eval_batch_size = config[\"eval_batch_size\"]    # For human\n",
        "  session_config = None\n",
        "  if config[\"use_tpu\"]:\n",
        "    sliced_eval_mode = tf.compat.v1.estimator.tpu.InputPipelineConfig.SLICED\n",
        "    distribute_strategy = None\n",
        "    config_train_batch_size *= config[\"num_tpu_cores\"]\n",
        "    config_eval_batch_size *= config[\"num_tpu_cores\"]\n",
        "    effective_train_batch_size = config_train_batch_size\n",
        "    effective_eval_batch_size = config_eval_batch_size\n",
        "  else:\n",
        "    session_config = tf.compat.v1.ConfigProto(\n",
        "        allow_soft_placement=True,\n",
        "        gpu_options=tf.compat.v1.GPUOptions(\n",
        "            per_process_gpu_memory_fraction=1.2))\n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
        "    with tf.compat.v1.Session(cluster_resolver.master(),\n",
        "                              config=session_config) as sess:\n",
        "      logging.info(sess.list_devices())\n",
        "    sliced_eval_mode = tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V1\n",
        "    distribute_strategy = tf.distribute.MirroredStrategy(devices=None)\n",
        "    effective_train_batch_size *= distribute_strategy.num_replicas_in_sync\n",
        "    # effective_eval_batch_size *= distribute_strategy.num_replicas_in_sync\n",
        "\n",
        "  is_per_host = tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      master=config[\"master\"],\n",
        "      model_dir=config[\"output_dir\"],\n",
        "      save_checkpoints_steps=config[\"save_checkpoints_steps\"],\n",
        "      keep_checkpoint_max=keep_checkpoint_max,\n",
        "      train_distribute=distribute_strategy,\n",
        "      session_config=session_config,\n",
        "      tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n",
        "          tpu_job_name=config[\"tpu_job_name\"],\n",
        "          iterations_per_loop=config[\"iterations_per_loop\"],\n",
        "          num_shards=config[\"num_tpu_cores\"],\n",
        "          per_host_input_for_training=is_per_host,\n",
        "          eval_training_input_configuration=sliced_eval_mode))\n",
        "\n",
        "  if config[\"init_checkpoint\"]:\n",
        "    ckpt_var_list = tf.compat.v1.train.list_variables(config[\"init_checkpoint\"])\n",
        "    ckpt_var_list = {\n",
        "        name: shape for name, shape in ckpt_var_list\n",
        "        if not re.findall(\"(Adam|Adafactor|global_step)\", name)\n",
        "    }\n",
        "    vars_to_warm_start = \"({})\".format(\"|\".join(ckpt_var_list.keys()))\n",
        "    warm_start_settings = tf.estimator.WarmStartSettings(\n",
        "        ckpt_to_initialize_from=config[\"init_checkpoint\"],\n",
        "        vars_to_warm_start=vars_to_warm_start)\n",
        "  else:\n",
        "    ckpt_var_list = {}\n",
        "    warm_start_settings = None\n",
        "  config[\"ckpt_var_list\"] = ckpt_var_list\n",
        "\n",
        "  # If no TPU, this will fall back to normal Estimator on CPU or GPU.\n",
        "  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n",
        "      use_tpu=config[\"use_tpu\"],\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=config_train_batch_size,\n",
        "      eval_batch_size=config_eval_batch_size,\n",
        "      warm_start_from=warm_start_settings)\n",
        "\n",
        "  # assign batch sizes\n",
        "  estimator.train_batch_size = effective_train_batch_size\n",
        "  estimator.eval_batch_size = effective_eval_batch_size\n",
        "\n",
        "  return estimator\n",
        "\n",
        "\n",
        "def log_variables(variables, ckpt_var_list):\n",
        "  \"\"\"Log trainable variables.\"\"\"\n",
        "  logging.info(\"**** Trainable Variables ****\")\n",
        "\n",
        "  model_var_list = {var.name: var.get_shape().as_list() for var in variables}\n",
        "  num_params = sum(np.prod(shape) for shape in model_var_list.values())\n",
        "  length = max(len(name) for name in model_var_list) + 2\n",
        "  line = \"{{:<{}}}{{:<13}}{{}}\".format(length)\n",
        "\n",
        "  logging.info(\"The model has {} trainable variables \"\n",
        "               \"({:,} parameters):\\n\".format(len(model_var_list), num_params))\n",
        "  logging.info(line.format(\"Name\", \"Initialized\", \"Shape\"))\n",
        "  logging.info(line.format(\"----\", \"-----------\", \"-----\"))\n",
        "\n",
        "  ckpt_var_list = ckpt_var_list.copy()\n",
        "  for name, shape in model_var_list.items():\n",
        "    name = name.split(\":\")[0]\n",
        "    if name in ckpt_var_list:\n",
        "      warm_started = \"from ckpt\"\n",
        "      del ckpt_var_list[name]\n",
        "    else:\n",
        "      warm_started = \"random\"\n",
        "    logging.info(line.format(name, warm_started, shape))\n",
        "\n",
        "  if ckpt_var_list:\n",
        "    logging.warning(\n",
        "        \"The warm start checkpoint contained %d variables that were not used \"\n",
        "        \"for the model:\\n\", len(ckpt_var_list))\n",
        "    for name, shape in ckpt_var_list.items():\n",
        "      logging.warning(line.format(name, \"not used\", shape))\n",
        "\n",
        "\n",
        "def add_scalars_to_summary(summary_dir, scalar_tensors_dict):\n",
        "  \"\"\"Creates a host_call function that writes summaries on TPU.\"\"\"\n",
        "\n",
        "  #  All tensors outfed from TPU should preserve batch size dimension.\n",
        "  scalar_tensors_dict = {\n",
        "      k: tf.reshape(v, [1]) for k, v in scalar_tensors_dict.items()\n",
        "  }\n",
        "\n",
        "  def host_call_fn(**kwargs):\n",
        "    writer = tf.summary.create_file_writer(summary_dir, max_queue=1000)\n",
        "    always_record = tf.summary.record_if(True)\n",
        "    with writer.as_default(), always_record:\n",
        "      for name, scalar in kwargs.items():\n",
        "        tf.summary.scalar(name, tf.reduce_mean(scalar),\n",
        "                          tf.compat.v1.train.get_or_create_global_step())\n",
        "      return tf.compat.v1.summary.all_v2_summary_ops()\n",
        "\n",
        "  return host_call_fn, scalar_tensors_dict\n",
        "\n",
        "\n",
        "########################## DEFAULT CONFIG UTILS ################################\n",
        "\n",
        "\n",
        "def get_default_config():\n",
        "  \"\"\"Default values for BigBird.\"\"\"\n",
        "\n",
        "  default_config = {\n",
        "      # transformer basic configs\n",
        "      \"attention_probs_dropout_prob\": 0.1,\n",
        "      \"hidden_act\": \"gelu\",\n",
        "      \"hidden_dropout_prob\": 0.1,\n",
        "      \"hidden_size\": 768,\n",
        "      \"initializer_range\": 0.02,\n",
        "      \"intermediate_size\": 3072,\n",
        "      \"max_position_embeddings\": 4096,\n",
        "      \"num_attention_heads\": 12,\n",
        "      \"num_hidden_layers\": 12,\n",
        "      \"type_vocab_size\": 2,\n",
        "      \"use_bias\": True,\n",
        "      \"rescale_embedding\": False,\n",
        "      \"scope\": \"bert\",\n",
        "      # sparse mask configs\n",
        "      \"attention_type\": \"block_sparse\",\n",
        "      \"norm_type\": \"postnorm\",\n",
        "      \"block_size\": 16,\n",
        "      \"num_rand_blocks\": 3,\n",
        "      # common bert configs\n",
        "      \"max_encoder_length\": 1024,\n",
        "      \"max_decoder_length\": 64,\n",
        "      \"couple_encoder_decoder\": False,\n",
        "      \"beam_size\": 5,\n",
        "      \"alpha\": 0.7,\n",
        "      \"label_smoothing\": 0.1,\n",
        "      \"weight_decay_rate\": 0.01,\n",
        "      \"optimizer_beta1\": 0.9,\n",
        "      \"optimizer_beta2\": 0.999,\n",
        "      \"optimizer_epsilon\": 1e-6,\n",
        "      # TPU settings\n",
        "      \"use_tpu\": True,\n",
        "      \"tpu_name\": None,\n",
        "      \"tpu_zone\": None,\n",
        "      \"tpu_job_name\": None,\n",
        "      \"gcp_project\": None,\n",
        "      \"master\": None,\n",
        "      \"num_tpu_cores\": 8,\n",
        "      \"iterations_per_loop\": \"1000\",\n",
        "  }\n",
        "\n",
        "  return default_config"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4lLR6-4FacN"
      },
      "source": [
        "######################### recompute grade ########################\n",
        "# Copyright 2021 The BigBird Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Library for rematerialization.\n",
        "Incubates a version of tf.recompute_grad that is XLA compatible.\n",
        "\"\"\"\n",
        "import collections\n",
        "import numbers\n",
        "import os\n",
        "import threading\n",
        "from typing import Deque, List, NamedTuple, Optional, Sequence, Text, Union\n",
        "\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "# pylint: disable=g-direct-tensorflow-import\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import custom_gradient\n",
        "\n",
        "\n",
        "# Remove when https://github.com/tensorflow/tensorflow/pull/45298\n",
        "# gets merged\n",
        "def get_variable_by_name(var_name):\n",
        "  \"\"\"Retrieves tf.Variable from name in MirroredStrategy (multi-gpu).\"\"\"\n",
        "\n",
        "  # Get all variables, but it will have copies from different replicas\n",
        "  all_global_vars = ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)\n",
        "\n",
        "  def _replica_filter(var):\n",
        "    \"\"\"Filter out variables from different context.\"\"\"\n",
        "    try:\n",
        "      return var_name == var.op.name\n",
        "    except AttributeError:\n",
        "      return False\n",
        "  candidate_vars = list(filter(_replica_filter, all_global_vars))\n",
        "\n",
        "  if len(candidate_vars) >= 1:\n",
        "    # Filter out non-trainable variables.\n",
        "    candidate_vars = [v for v in candidate_vars if v.trainable]\n",
        "  else:\n",
        "    raise ValueError('Unsuccessful at finding variable {}.'.format(var_name))\n",
        "\n",
        "  if len(candidate_vars) == 1:\n",
        "    return candidate_vars[0]\n",
        "  elif len(candidate_vars) > 1:\n",
        "    raise ValueError(\n",
        "        'Unsuccessful at finding trainable variable {}. '\n",
        "        'Number of candidates: {}. '\n",
        "        'Candidates: {}'.format(var_name, len(candidate_vars), candidate_vars))\n",
        "  else:\n",
        "    # The variable is not trainable.\n",
        "    return None\n",
        "custom_gradient.get_variable_by_name = get_variable_by_name\n",
        "\n",
        "\n",
        "class RecomputeContext(\n",
        "    NamedTuple('RecomputeContext', [\n",
        "        ('is_recomputing', bool),\n",
        "        ('seed', tf.Tensor),\n",
        "        ('children', Deque['RecomputeContext']),\n",
        "    ])):\n",
        "  \"\"\"Context for recomputation.\n",
        "  Attributes:\n",
        "    is_recomputing: Whether we are in a recomputation phase.\n",
        "    seed: Scalar integer tensor that should be used with stateless random ops\n",
        "      for deterministic behavior and correct computation of the gradient.\n",
        "    children: Nested `RecomputeContext` instances. Used internally by\n",
        "      `recompute_grad` to track nested instances of `RecomputeContext`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __enter__(self):\n",
        "    return _context_stack.push(self)\n",
        "\n",
        "  def __exit__(self, exc_type, exc_value, traceback):\n",
        "    _context_stack.pop(self)\n",
        "\n",
        "\n",
        "# Simplified version of `_DefaultStack` in\n",
        "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py.\n",
        "class _ContextStack(threading.local):\n",
        "  \"\"\"A thread-local stack for providing implicit recompute contexts.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(_ContextStack, self).__init__()\n",
        "    self._stack = []\n",
        "\n",
        "  def top(self) -> Optional[RecomputeContext]:\n",
        "    return self._stack[-1] if self._stack else None\n",
        "\n",
        "  def push(self, context: RecomputeContext):\n",
        "    self._stack.append(context)\n",
        "    return context\n",
        "\n",
        "  def pop(self, context: RecomputeContext):\n",
        "    if self._stack[-1] is not context:\n",
        "      raise AssertionError('Nesting violated for RecomputeContext.')\n",
        "    self._stack.pop()\n",
        "\n",
        "\n",
        "_context_stack = _ContextStack()\n",
        "\n",
        "\n",
        "def get_recompute_context() -> Optional[RecomputeContext]:\n",
        "  \"\"\"Returns the current recomputing context if it exists.\"\"\"\n",
        "  return _context_stack.top()\n",
        "\n",
        "\n",
        "# Adapted from\n",
        "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_util.py.\n",
        "def _get_containing_xla_context(graph: tf.Graph) -> Optional[object]:\n",
        "  \"\"\"Returns the first ancestor `XLAControlFlowContext` in the `graph`.\"\"\"\n",
        "  ctxt = graph._get_control_flow_context()  # pylint: disable=protected-access\n",
        "  while ctxt:\n",
        "    if ctxt.IsXLAContext():\n",
        "      return ctxt\n",
        "    ctxt = ctxt.outer_context\n",
        "  return None\n",
        "\n",
        "\n",
        "def _in_xla_context(graph: Optional[tf.Graph] = None) -> bool:\n",
        "  \"\"\"Detects whether we are in an XLA context.\"\"\"\n",
        "  if '--tf_xla_auto_jit=2' in os.environ.get('TF_XLA_FLAGS', ''):\n",
        "    return True\n",
        "  graph = tf.compat.v1.get_default_graph() if graph is None else graph\n",
        "  while True:\n",
        "    if _get_containing_xla_context(graph) is not None:\n",
        "      return True\n",
        "    try:\n",
        "      graph = graph.outer_graph\n",
        "    except AttributeError:\n",
        "      return False\n",
        "\n",
        "\n",
        "def _force_data_dependency(\n",
        "    first_compute: Sequence[tf.Tensor],\n",
        "    then_compute: Sequence[tf.Tensor]) -> List[tf.Tensor]:\n",
        "  \"\"\"Force all of `then_compute` to depend on all of `first_compute`.\n",
        "  Uses a dummy data dependency, which is useful when running on TPUs because\n",
        "  XLA ignores control dependencies. Only supports float arguments.\n",
        "  Args:\n",
        "    first_compute: Sequence of `Tensor`s to be executed before `then_compute`.\n",
        "    then_compute: Sequence of `Tensor`s to executed after `first_compute`.\n",
        "  Returns:\n",
        "    Sequence of `Tensor`s with same length of `then_compute`.\n",
        "  Raises:\n",
        "    ValueError: if ranks are unknown or types are not floating.\n",
        "  \"\"\"\n",
        "\n",
        "  def _first_element(x):\n",
        "    if x.shape.ndims is None:\n",
        "      raise ValueError('Rank of Tensor %s must be known' % x)\n",
        "    ndims = x.shape.ndims\n",
        "    begin = tf.zeros(ndims, dtype=tf.int32)\n",
        "    size = tf.ones(ndims, dtype=tf.int32)\n",
        "    return tf.reshape(tf.slice(x, begin, size), [])\n",
        "\n",
        "  first_compute_sum = tf.add_n(\n",
        "      [_first_element(x) for x in first_compute if x is not None])\n",
        "  dtype = first_compute_sum.dtype\n",
        "  if not dtype.is_floating:\n",
        "    raise ValueError('_force_data_dependency only supports floating dtypes.')\n",
        "  zero = np.finfo(dtype.as_numpy_dtype).tiny * first_compute_sum\n",
        "  return [\n",
        "      x + tf.cast(zero, x.dtype) if x is not None else None\n",
        "      for x in then_compute\n",
        "  ]\n",
        "\n",
        "\n",
        "def _make_seed_if_none(seed: Optional[tf.Tensor]) -> tf.Tensor:\n",
        "  \"\"\"Uses the global generator to make a seed if necessary.\"\"\"\n",
        "  if seed is not None:\n",
        "    return seed\n",
        "  generator = tf.random.experimental.get_global_generator()\n",
        "  # The two seeds for stateless random ops don't have individual semantics and\n",
        "  # are scrambled together, so providing one seed is fine. This makes it easier\n",
        "  # for users to provide a local seed without worrying about integer overflow.\n",
        "  # See `make_seeds` in\n",
        "  # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/stateful_random_ops.py.\n",
        "  try:\n",
        "    return generator.uniform_full_int([], tf.int32, name='recompute_grad_seed')\n",
        "  except (RuntimeError, TypeError, ValueError, tf.errors.NotFoundError) as e:\n",
        "    # For a number of reasons, the above operation can fail like using multiple\n",
        "    # graphs or toggling between eager and graph modes. Reset the generator.\n",
        "    logging.warn('Resetting the generator. %s: %s', type(e), e)\n",
        "    tf.random.experimental.set_global_generator(None)\n",
        "    generator = tf.random.experimental.get_global_generator()\n",
        "    return generator.uniform_full_int([], tf.int32, name='recompute_grad_seed')\n",
        "\n",
        "\n",
        "def recompute_grad(f, seed=None):\n",
        "  \"\"\"An eager-compatible version of recompute_grad.\n",
        "  For f(*args, **kwargs), this supports gradients with respect to args, or to\n",
        "  gradients with respect to any variables residing in the kwarg 'variables'.\n",
        "  Note that for keras layer and model objects, this is handled automatically.\n",
        "  Warning: If `f` was originally a tf.keras Model or Layer object, `g` will not\n",
        "  be able to access the member variables of that object, because `g` returns\n",
        "  through the wrapper function `inner`.  When recomputing gradients through\n",
        "  objects that inherit from keras, we suggest keeping a reference to the\n",
        "  underlying object around for the purpose of accessing these variables.\n",
        "  Args:\n",
        "    f: function `f(*x)` that returns a `Tensor` or sequence of `Tensor` outputs.\n",
        "    seed: Optional seed for random ops. `seed` should an integer scalar\n",
        "      `Tensor`. When compiling to XLA, `seed` must have dtype `tf.int32`. If\n",
        "      `seed` is not provided one will be generated.\n",
        "  Returns:\n",
        "   A function `g` that wraps `f`, but which recomputes `f` on the backwards\n",
        "   pass of a gradient call.\n",
        "  \"\"\"\n",
        "\n",
        "  @tf.custom_gradient\n",
        "  def inner(*args, **kwargs):\n",
        "    \"\"\"Inner function closure for calculating gradients.\"\"\"\n",
        "    # Detect when we're nested and in the backwards pass, so we don't generate\n",
        "    # an additional seed.\n",
        "    parent_context = get_recompute_context()\n",
        "    if parent_context is not None and parent_context.is_recomputing:\n",
        "      # Use the cached context in the recomputation phase.\n",
        "      with parent_context.children.popleft()._replace(\n",
        "          is_recomputing=True) as context:\n",
        "        result = f(*args, **kwargs)\n",
        "    else:\n",
        "      with RecomputeContext(\n",
        "          is_recomputing=False,\n",
        "          seed=_make_seed_if_none(seed),\n",
        "          children=collections.deque()) as context:\n",
        "        result = f(*args, **kwargs)\n",
        "        # In the forward pass, build up a tree of recomputation contexts.\n",
        "        if parent_context is not None and not parent_context.is_recomputing:\n",
        "          parent_context.children.append(context)\n",
        "\n",
        "    def grad(*dresult, **grad_kwargs):\n",
        "      \"\"\"Gradient function calculation for inner function.\"\"\"\n",
        "      variables = grad_kwargs.pop('variables', None)\n",
        "      if grad_kwargs:\n",
        "        raise ValueError('Found unexpected kwargs for `grad`: ',\n",
        "                         list(grad_kwargs.keys()))\n",
        "      inputs, seed = list(args), context.seed\n",
        "      if _in_xla_context():\n",
        "        inputs = _force_data_dependency(\n",
        "            tf.nest.flatten(dresult), inputs + [seed])\n",
        "        seed = inputs.pop()\n",
        "      # tf.keras.backend.set_learning_phase(1)\n",
        "      with tf.GradientTape() as tape:\n",
        "        tape.watch(inputs)\n",
        "        if variables is not None:\n",
        "          tape.watch(variables)\n",
        "        with tf.control_dependencies(dresult):\n",
        "          with context._replace(is_recomputing=True, seed=seed):\n",
        "            result = f(*inputs, **kwargs)\n",
        "      kw_vars = []\n",
        "      if variables is not None:\n",
        "        kw_vars = list(variables)\n",
        "      grads = tape.gradient(\n",
        "          result, list(inputs) + kw_vars, output_gradients=dresult)\n",
        "      return grads[:len(inputs)], grads[len(inputs):]\n",
        "\n",
        "    return result, grad\n",
        "\n",
        "  return inner\n",
        "\n",
        "\n",
        "######################## STATELESS DROPOUT LAYERS ##############################\n",
        "\n",
        "\n",
        "def _as_shape(shape: Union[Sequence[int], tf.TensorShape]) -> tf.TensorShape:\n",
        "  \"\"\"Converts the given object to a TensorShape.\"\"\"\n",
        "  return shape if isinstance(shape, tf.TensorShape) else tf.TensorShape(shape)\n",
        "\n",
        "\n",
        "def _get_noise_shape(\n",
        "    x: tf.Tensor, noise_shape: Union[Sequence[int], tf.TensorShape]\n",
        ") -> Union[tf.Tensor, tf.TensorShape, Sequence[int]]:\n",
        "  \"\"\"Computes the shape of the binary mask for dropout.\"\"\"\n",
        "  # If noise_shape is none return immediately.\n",
        "  if noise_shape is None:\n",
        "    return tf.shape(x)\n",
        "\n",
        "  try:\n",
        "    # Best effort to figure out the intended shape.\n",
        "    # If not possible, let the op to handle it.\n",
        "    # In eager mode exception will show up.\n",
        "    noise_shape_ = _as_shape(noise_shape)\n",
        "  except (TypeError, ValueError):\n",
        "    return noise_shape\n",
        "\n",
        "  if x.shape.dims is not None and len(x.shape.dims) == len(noise_shape_.dims):\n",
        "    new_dims = []\n",
        "    for i, dim in enumerate(x.shape.dims):\n",
        "      if noise_shape_.dims[i].value is None and dim.value is not None:\n",
        "        new_dims.append(dim.value)\n",
        "      else:\n",
        "        new_dims.append(noise_shape_.dims[i].value)\n",
        "    return tf.TensorShape(new_dims)\n",
        "\n",
        "  return noise_shape\n",
        "\n",
        "\n",
        "def stateless_dropout(x: tf.Tensor,\n",
        "                      rate: float,\n",
        "                      seed: tf.Tensor,\n",
        "                      noise_shape: Optional[Union[Sequence[int],\n",
        "                                                  tf.TensorShape]] = None,\n",
        "                      name: Optional[Text] = None) -> tf.Tensor:\n",
        "  \"\"\"Computes dropout: randomly sets elements to zero to prevent overfitting.\n",
        "  See https://www.tensorflow.org/api_docs/python/tf/nn/dropout.\n",
        "  This version differs in that the seed is required if the rate is nonzero.\n",
        "  Args:\n",
        "    x: A floating point tensor.\n",
        "    rate: A scalar `Tensor` with the same type as x. The probability that each\n",
        "      element is dropped. For example, setting rate=0.1 would drop 10% of input\n",
        "      elements.\n",
        "    seed: A shape [2] integer Tensor of seeds to the random number generator.\n",
        "      Must have dtype `tf.int32` when compiling to XLA.\n",
        "    noise_shape: A 1-D `Tensor` of type `int32`, representing the shape for\n",
        "      randomly generated keep/drop flags.\n",
        "    name: A name for this operation (optional).\n",
        "  Returns:\n",
        "    A `Tensor` of the same shape of `x`.\n",
        "  Raises:\n",
        "    ValueError: If `rate` is not in `[0, 1)` or if `x` is not a floating point\n",
        "      tensor. `rate=1` is disallowed, because the output would be all zeros,\n",
        "      which is likely not what was intended.\n",
        "  \"\"\"\n",
        "  with tf.name_scope(name or 'stateless_dropout') as name:\n",
        "    x = tf.convert_to_tensor(x, name='x')\n",
        "    if not x.dtype.is_floating:\n",
        "      raise ValueError('x has to be a floating point tensor since it\\'s going '\n",
        "                       ' to be scaled. Got a %s tensor instead.' % x.dtype)\n",
        "    if isinstance(rate, numbers.Real):\n",
        "      if not (rate >= 0 and rate < 1):\n",
        "        raise ValueError('rate must be a scalar tensor or a float in the '\n",
        "                         'range [0, 1), got %g' % rate)\n",
        "      if rate > 0.5:\n",
        "        logging.log_first_n(\n",
        "            logging.WARN, 'Large dropout rate: %g (>0.5). In TensorFlow '\n",
        "            '.x, dropout() uses dropout rate instead of keep_prob. '\n",
        "            'Please ensure that this is intended.', 5, rate)\n",
        "\n",
        "    # Early return if nothing needs to be dropped.\n",
        "    if tf.get_static_value(rate) == 0:\n",
        "      return x\n",
        "\n",
        "    rate = tf.convert_to_tensor(rate, dtype=x.dtype, name='rate')\n",
        "    rate.shape.assert_has_rank(0)\n",
        "    noise_shape = _get_noise_shape(x, noise_shape)\n",
        "    # Sample a uniform distribution on [0.0, 1.0) and select values larger than\n",
        "    # rate.\n",
        "    #\n",
        "    # NOTE: Random uniform actually can only generate 2^23 floats on [1.0, 2.0)\n",
        "    # and subtract 1.0.\n",
        "    random_tensor = tf.random.stateless_uniform(\n",
        "        noise_shape, seed=seed, dtype=x.dtype)\n",
        "    keep_prob = 1 - rate\n",
        "    scale = 1 / keep_prob\n",
        "    # NOTE: if (1.0 + rate) - 1 is equal to rate, then we want to consider that\n",
        "    # float to be selected, hence we use a >= comparison.\n",
        "    keep_mask = random_tensor >= rate\n",
        "    ret = x * scale * tf.cast(keep_mask, x.dtype)\n",
        "    if not tf.executing_eagerly():\n",
        "      ret.set_shape(x.get_shape())\n",
        "    return ret\n",
        "\n",
        "\n",
        "# Reimplements internal function\n",
        "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/smart_cond.py.\n",
        "def smart_cond(pred, true_fn=None, false_fn=None, name=None):\n",
        "  \"\"\"Return either `true_fn()` if predicate `pred` is true else `false_fn()`.\n",
        "  If `pred` is a bool or has a constant value, we return either `true_fn()`\n",
        "  or `false_fn()`, otherwise we use `tf.cond` to dynamically route to both.\n",
        "  Arguments:\n",
        "    pred: A scalar determining whether to return the result of `true_fn` or\n",
        "      `false_fn`.\n",
        "    true_fn: The callable to be performed if pred is true.\n",
        "    false_fn: The callable to be performed if pred is false.\n",
        "    name: Optional name prefix when using `tf.cond`.\n",
        "  Returns:\n",
        "    Tensors returned by the call to either `true_fn` or `false_fn`.\n",
        "  Raises:\n",
        "    TypeError: If `true_fn` or `false_fn` is not callable.\n",
        "  \"\"\"\n",
        "  if not callable(true_fn):\n",
        "    raise TypeError('`true_fn` must be callable.')\n",
        "  if not callable(false_fn):\n",
        "    raise TypeError('`false_fn` must be callable.')\n",
        "  pred_value = tf.get_static_value(pred)\n",
        "  if isinstance(pred, tf.Variable) or pred_value is None:\n",
        "    return tf.cond(\n",
        "        pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
        "  if pred_value:\n",
        "    return true_fn()\n",
        "  else:\n",
        "    return false_fn()\n",
        "\n",
        "\n",
        "# See https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout.\n",
        "class RecomputingDropout(tf.keras.layers.Layer):\n",
        "  \"\"\"`tf.keras.layers.Dropout` that supports `recompute_grad`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               rate,\n",
        "               noise_shape=None,\n",
        "               seed=None,\n",
        "               force_recomputation=False,\n",
        "               **kwargs):\n",
        "    \"\"\"Initializes `RecomputingDropout`.\n",
        "    Args:\n",
        "      rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "      noise_shape: 1D integer tensor representing the shape of the binary\n",
        "        dropout mask that will be multiplied with the input. For instance, if\n",
        "        inputs have shape `(batch_size, timesteps, features)` and you want the\n",
        "        dropout mask to be the same for all timesteps, you can use\n",
        "        `noise_shape=(batch_size, 1, features)`.\n",
        "      seed: A Python integer to use as random seed.\n",
        "      force_recomputation: If `True`, then raises an error if called outside a\n",
        "        recompute context.\n",
        "      **kwargs: Keyword arguments for `tf.keras.layers.Layer`.\n",
        "    \"\"\"\n",
        "\n",
        "    super(RecomputingDropout, self).__init__(**kwargs)\n",
        "    self.rate = rate\n",
        "    self.noise_shape = noise_shape\n",
        "    self.seed = seed\n",
        "    self.force_recomputation = force_recomputation\n",
        "    self.supports_masking = True\n",
        "    # Create a layer-specific seed to combine with the global recompute seed.\n",
        "    self._recompute_seed = (\n",
        "        np.random.randint(-2**31, 2**31, dtype=np.int32)\n",
        "        if seed is None else seed)\n",
        "\n",
        "  def _get_noise_shape(self, inputs):\n",
        "    # Subclasses of `Dropout` may implement `_get_noise_shape(self, inputs)`,\n",
        "    # which will override `self.noise_shape`, and allows for custom noise\n",
        "    # shapes with dynamically sized inputs.\n",
        "    if self.noise_shape is None:\n",
        "      return None\n",
        "\n",
        "    concrete_inputs_shape = tf.shape(inputs)\n",
        "    noise_shape = []\n",
        "    for i, value in enumerate(self.noise_shape):\n",
        "      noise_shape.append(concrete_inputs_shape[i] if value is None else value)\n",
        "    return tf.convert_to_tensor(noise_shape)\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    \"\"\"Builds computation graph.\n",
        "    Args:\n",
        "      inputs: Input tensor (of any rank).\n",
        "      training: Python boolean indicating whether the layer should behave in\n",
        "        training mode (adding dropout) or in inference mode (doing nothing).\n",
        "    Returns:\n",
        "      `inputs` masked according to layer configuration.\n",
        "    Raises:\n",
        "      ValueError: If `force_recomputation` is `True` and called outside a\n",
        "        a recompute context.\n",
        "    \"\"\"\n",
        "    if self.rate == 0:\n",
        "      return inputs\n",
        "\n",
        "    if training is None:\n",
        "      training = tf.keras.backend.learning_phase()\n",
        "\n",
        "    def dropped_inputs():\n",
        "      \"\"\"Randomly drops elements of `inputs` when `training=True`.\"\"\"\n",
        "      recompute_context = get_recompute_context()\n",
        "      if recompute_context is None:\n",
        "        if self.force_recomputation:\n",
        "          raise ValueError(\n",
        "              'RecomputeContext is required when force_recomputation=True.')\n",
        "        return tf.nn.dropout(\n",
        "            inputs,\n",
        "            noise_shape=self._get_noise_shape(inputs),\n",
        "            seed=self.seed,\n",
        "            rate=self.rate)\n",
        "      seed = tf.stack([recompute_context.seed, self._recompute_seed])\n",
        "      return stateless_dropout(\n",
        "          inputs,\n",
        "          rate=self.rate,\n",
        "          seed=seed,\n",
        "          noise_shape=self._get_noise_shape(inputs))\n",
        "\n",
        "    output = smart_cond(training, dropped_inputs, lambda: tf.identity(inputs))\n",
        "    return output\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        'rate': self.rate,\n",
        "        'noise_shape': self.noise_shape,\n",
        "        'seed': self.seed,\n",
        "        'force_recomputation': self.force_recomputation,\n",
        "    }\n",
        "    base_config = super(RecomputingDropout, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqm5NJN5Go2w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHMyxIvOFfsj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8EqRYYCEaZb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8GNF8yKC4Sn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7Klc-uCxdX"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncOw3E9PCxgc"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSELPOyFGI0X"
      },
      "source": [
        "# Main "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L8bMkas3L-l"
      },
      "source": [
        "# set-up\n",
        "# https://chancoding.tistory.com/86 콘다 환경과 쥬피터 노트북 연결방법\n",
        "# 해당 환경으로 들어감(prompt)\n",
        "# pip install jupyter notebook\n",
        "# pip install ipykernel\n",
        "# python -m ipykernel install --user --name bigbird --display-name \"bigbird\"\n",
        "# web에서 해당 패키지에 대한 정보를 가져옴\n",
        "# !pip install git+https://github.com/google-research/bigbird.git -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcehgG7P3FZN",
        "outputId": "e305f1df-2eaf-46ca-b085-5af46b758dd6"
      },
      "source": [
        "!pip install git+https://github.com/google-research/bigbird.git -q\n",
        "# 해당환경에 bigbird설치함"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 37.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 53.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 57.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 191 kB 57.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 32.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 13.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 367 kB 41.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 366 kB 63.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 251 kB 63.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 191 kB 51.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 60.2 MB/s \n",
            "\u001b[?25h  Building wheel for bigbird (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq_otaSI3Gs8"
      },
      "source": [
        "from bigbird.core import flags\n",
        "from bigbird.core import modeling\n",
        "from bigbird.core import utils\n",
        "from bigbird.classifier import run_classifier\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "if not hasattr(FLAGS, \"f\"): flags.DEFINE_string(\"f\", \"\", \"\")\n",
        "FLAGS(sys.argv)\n",
        "\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbC0Q0Sx3RjX"
      },
      "source": [
        "# setup"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB66rd7Q3YXC"
      },
      "source": [
        "FLAGS.data_dir = \"tfds://imdb_reviews/plain_text\"\n",
        "FLAGS.attention_type = \"block_sparse\"\n",
        "FLAGS.max_encoder_length = 4096  # reduce for quicker demo on free colab\n",
        "FLAGS.learning_rate = 1e-5\n",
        "FLAGS.num_train_steps = 2000\n",
        "FLAGS.attention_probs_dropout_prob = 0.0\n",
        "FLAGS.hidden_dropout_prob = 0.0\n",
        "FLAGS.use_gradient_checkpointing = True\n",
        "FLAGS.vocab_model_file = \"gpt2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKymOUpO3Yxa"
      },
      "source": [
        "bert_config = flags.as_dictionary()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twj9xRO33a3O"
      },
      "source": [
        "# define classification model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR4KnViz3ejA"
      },
      "source": [
        "model = modeling.BertModel(bert_config)\n",
        "headl = run_classifier.ClassifierLossLayer(\n",
        "        bert_config[\"hidden_size\"], bert_config[\"num_labels\"],\n",
        "        bert_config[\"hidden_dropout_prob\"],\n",
        "        utils.create_initializer(bert_config[\"initializer_range\"]),\n",
        "        name=bert_config[\"scope\"]+\"/classifier\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzgSWhpn3hLV"
      },
      "source": [
        "@tf.function(experimental_compile=True)\n",
        "def fwd_bwd(features, labels):\n",
        "  with tf.GradientTape() as g:\n",
        "    _, pooled_output = model(features, training=True)\n",
        "    loss, log_probs = headl(pooled_output, labels, True)\n",
        "  grads = g.gradient(loss, model.trainable_weights+headl.trainable_weights)\n",
        "  return loss, log_probs, grads"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJl6AfMI3of1"
      },
      "source": [
        "# dataset pipe line"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "bb18f0f793cb48ef80244c57aeba516e",
            "c7d198a2b904408fa06e0b9db3d99a6a",
            "9975a618b0ae402d8dc382ed58c341cd",
            "8c43ccff5fdc4af5808310f33f38466e",
            "dbcad235943c41cd924ca702836f9375",
            "352825bfd87b479dbd38069376f8015f",
            "f36bdb5f47af42579dc3381783462361",
            "2bccaa8511d64beb8a6e8dcfec96359f",
            "78399e4d35294549a05c2ef5f9386dbb",
            "e7160e7c6f1b452fa8f722e9298ece50",
            "c767eb18ed36405ba65600843d0a3aef",
            "0126b2e655744f3e896d5f3c74111d07",
            "543121ce3b5144cab95da109c7c4100a",
            "777ece97dc7c497da6b7acd954862d32",
            "89bf6ea7fd5f4803a52465e72339cd0d",
            "7b8dc3dc9a5f47d8a4686f3d5d307f4f",
            "9b2c8ffd70794322a1360c22198c1c8c",
            "93f1d9503031416d85dfd07a223c3d6c",
            "d0142a3dab2d48db810215dfa7c4aa3a",
            "7b1f8019ae2b4f84ad962df7639c6370",
            "0f30c06513174cc1884a715cc639b0a8",
            "de8c603c35e54059b2a775e9a3b8513f",
            "009eda280c57461684fc3452d17b534b",
            "280d673e69874a3eb0cbb94127506f04",
            "cb8dfade1a354771ab29f3b4a8b53741",
            "754dfe171d2e4cb592787d46b585263c",
            "2458e3e25eec464887c3accb846e5535",
            "71a9f4b8aeba4bdbba4a272d8164e214",
            "7c89bce775f44f52bfd1cb1db74ffd5a",
            "742284d04bd64e9bba44f3c1532528cb",
            "acea966f6ed34aed979aed4fa3dd92d3",
            "e1eaff7f37fa477c83500d7731b7e299",
            "32bf82c905db47f398ba7570ce7109c1",
            "ad47a3f79dc84691b9b993197dffabfe",
            "9d1b66d5821a4e8fac5a4c8447308e33",
            "f75fcba350c94a5ca049c5e3fd39da6d",
            "f4540de7e32b450080253e0d3646f2fe",
            "41cff3038828493da46f1295d0d56fa3",
            "fca031a120874eca82dc21f650cc2759",
            "80d61c034c9148d597d3c8b80c6d9a10",
            "b330b2a21bf44df5a8f06bb0c8112a88",
            "5a48b986efe34fedb5734210be3ce239",
            "b9007640337545b38db808f457b5f87b",
            "78f0eb31cde1494690644c7013b8bed9",
            "4c1dec5f1bb0467fade6e624061d06d9",
            "08530c885f474023b458060f4d9460c5",
            "dabc9975560649308e6c65b3df7da70e",
            "9262fb87802b4d5b948047fca1058409",
            "2454e52c15754719a8321149cb7b2479",
            "c7df5e02458b4618bccf4a84b6f2da67",
            "6447d4b4a5a340d4bbba348d6576f428",
            "c16e2929caf647838d0cf7f62eaf45f3",
            "ab8dadbbc48e40dfb590602996a35ab8",
            "2faf11bd100f4cf9bb1d32b4172cfe0d",
            "2450d6706fd64d12a3a7d36f210c11d5",
            "f6db30e7736342b9b237403f7855aa5e",
            "e94a16a3ec3f46b1a7453b9a548f1a92",
            "fa9abd680f1e4b8595657934fef784f3",
            "7d3f5264b72641568d3f8b7c6f7745b9",
            "ce56210ff95f48cf9a92122232e36d46",
            "510bc1bd58434524b3374c4bb9db1834",
            "4de3dafab8644180830ea94c83377922",
            "d3d77098f5914c60a92fbb0d6c79fa2c",
            "66eb91b37b8f4810b99a2d172069df2a",
            "46a9e43884c44dafba7c875a89f4fe7e",
            "f0b52ccfc656431aa90d66108f3be3f2",
            "835e6b4676d14ff08e05ce031dc175e7",
            "cba76259ad63468091c71d51c0eebe6a",
            "99e82dc1b64d4a4e9b662cbe8819b08f",
            "0c180b17ead74979aec1b934f2af8050",
            "0cd09db387a34134941fa46fb66a5dcd",
            "b67bbd36ec16403cb969ac7c1eaafd1a",
            "b720b6aa8c4648c49f05968f8e20f47e",
            "97f7cb20fc4045249a85d6b5c3628b96",
            "bcfd723789c646b09e8356bd705af0f3",
            "66fa5757482246a198428e4302a425b8",
            "ab48a9a6fe854397822f35b059fd9bdc",
            "c6e12461ec684d7ab629c4d7e2ebab2d",
            "8ce9b871289a4c24894a296a4f90fa24",
            "75c4bd3a2dc549bfa8e38b6415f070ee",
            "d15c9c76a9124490bd840987c8af9dd9",
            "a8cf41b1e0e9448ea3acbf6b00877e51",
            "facbe82adfb04bf982bed1886d316d43",
            "6b0d1995e4d04dfda8aa9d51dbceb818",
            "103ad4ad39c24ce9ae6d373dbf7baf80",
            "a270ec492e014579a7e9ff54cc842a5a",
            "900796159eae4671948b6879030d2349",
            "5740963bb2cb40b68d32b6af340c9b62",
            "953aabb575a34891b175c3f25bcb56af",
            "a7d4340284b34e90ab546df4310b7527",
            "efa89ee311554545b9929b702b35b315",
            "e515681e40a74fae8afd839bb715a2a6",
            "3cad977e94224132afb9b339bb3e7ccd",
            "35a128956519425f98a994be38e72e83",
            "979719cdd71e4308a5b443afc6f9fa8e",
            "37ec060f648a4feab297a48db453e1da",
            "4963448793f54f00b9ef88f59ada9a8f",
            "08ced3c5d11b4fedbc1ce2fe3166cbe4",
            "1ec1a9d3d694412b8783df39f37b6c67"
          ]
        },
        "id": "ajDTM7P53wfq",
        "outputId": "52c7ae26-330b-4206-bf45-3676febb81dd"
      },
      "source": [
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=True)\n",
        "dataset = train_input_fn({'batch_size': 8})"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb18f0f793cb48ef80244c57aeba516e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0126b2e655744f3e896d5f3c74111d07",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "009eda280c57461684fc3452d17b534b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad47a3f79dc84691b9b993197dffabfe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c1dec5f1bb0467fade6e624061d06d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Shuffling imdb_reviews-train.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6db30e7736342b9b237403f7855aa5e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "835e6b4676d14ff08e05ce031dc175e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Shuffling imdb_reviews-test.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6e12461ec684d7ab629c4d7e2ebab2d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "953aabb575a34891b175c3f25bcb56af",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Shuffling imdb_reviews-unsupervised.tfrecord...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB9Itx1M3ybg"
      },
      "source": [
        "# inspect at a few examples\n",
        "for ex in dataset.take(3):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_JSI6b03osp"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsxX89uG3ovn"
      },
      "source": [
        "# (Optionally) Check outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8myKwtk3hQ8"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xcFf-_l3hT3"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Ul3get3hWM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}